# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01c_data-question-answering.ipynb (unless otherwise specified).

__all__ = []

# Cell
import ast
from functools import reduce

from ..utils import *
from .core import *

import torch
from transformers import *
from fastai2.text.all import *

# Cell
@typedispatch
def build_hf_input(task:ForQuestionAnsweringTask, tokenizer,
                   a_tok_ids, b_tok_ids=None, targets=None,
                   max_length=512, pad_to_max_length=True, truncation_strategy=None):

    if (truncation_strategy is None):
        truncation_strategy = "only_second" if tokenizer.padding_side == "right" else "only_first"

    res = tokenizer.prepare_for_model(a_tok_ids if tokenizer.padding_side == "right" else b_tok_ids,
                                      b_tok_ids if tokenizer.padding_side == "right" else a_tok_ids,
                                      max_length=max_length,
                                      pad_to_max_length=pad_to_max_length,
                                      truncation_strategy=truncation_strategy,
                                      return_special_tokens_mask=True,
                                      return_tensors='pt')

    input_ids = res['input_ids'][0]
    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else tensor([-9999])
    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else tensor([-9999])

    # cls_index: location of CLS token (used by xlnet and xlm) ... this is a list.index(value) for pytorch tensor's
    cls_index = (input_ids == tokenizer.cls_token_id).nonzero()[0]

    # p_mask: mask with 1 for token than cannot be in the answer, else 0 (used by xlnet and xlm)
    p_mask = tensor(res['special_tokens_mask']) if ('special_tokens_mask' in res) else tensor([-9999])

    return HF_BaseInput([input_ids, attention_mask, token_type_ids, cls_index, p_mask]), targets