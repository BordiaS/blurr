# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_modeling.ipynb (unless otherwise specified).

__all__ = ['hf_splitter', 'HF_BaseModelWrapper']

# Cell
from .utils import *
from .data import *

import torch
from transformers import *
from fastai2.text.all import *

# Cell
def hf_splitter(m):
    root_modules = list(m.hf_model.named_children())
    top_module_name, top_module = root_modules[0]

    groups = L([ m for m_name, m in list(top_module.named_children()) ])
    groups += L([ m for m_name, m in root_modules[1:] ])

    return groups.map(params).filter(lambda el: len(el) > 0)

# Cell
class HF_BaseModelWrapper(torch.nn.Module):
    def __init__(self, hf_model):
        super().__init__()
        self.hf_model = hf_model
        self.hf_model_fwd_args = hf_model.forward.__code__.co_varnames

    def forward(self, x):
        model_kwargs = {}
        model_kwargs['input_ids'] = x[0]
        if (self._include_arg('token_type_ids', x[1])): model_kwargs['token_type_ids'] = x[1]
        if (self._include_arg('attention_mask', x[2])): model_kwargs['attention_mask'] = x[2]

        outputs = self.hf_model(**model_kwargs)
        return outputs[0]

    def _include_arg(self, arg_name, tensor_val):
        if (tensor_val[0][0].item() == -9999 or arg_name not in self.hf_model_fwd_args):
            return False
        return True