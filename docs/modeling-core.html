---

title: modeling.core

keywords: fastai
sidebar: home_sidebar

summary: "This module contains core custom models, loss functions, and a default layer group splitter for use in applying discriminiative learning rates to your huggingface models trained via fastai"
description: "This module contains core custom models, loss functions, and a default layer group splitter for use in applying discriminiative learning rates to your huggingface models trained via fastai"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_modeling-core.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#cuda</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Using GPU #</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using GPU #1: GeForce GTX 1080 Ti
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Base-splitter,-model-wrapper,-and-model-callback">Base splitter, model wrapper, and model callback<a class="anchor-link" href="#Base-splitter,-model-wrapper,-and-model-callback"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="hf_splitter" class="doc_header"><code>hf_splitter</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/modeling/core.py#L16" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>hf_splitter</code>(<strong><code>m</code></strong>)</p>
</blockquote>
<p>Splits the huggingface model based on various model architecture conventions</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="HF_BaseModelWrapper" class="doc_header"><code>class</code> <code>HF_BaseModelWrapper</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/modeling/core.py#L28" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>HF_BaseModelWrapper</code>(<strong><code>hf_model</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that <code>HF_baseModelWrapper</code> includes some nifty code for just passing in the things your model needs, as not all transformer architectures require/use the same information.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="HF_BaseModelCallback" class="doc_header"><code>class</code> <code>HF_BaseModelCallback</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/modeling/core.py#L43" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>HF_BaseModelCallback</code>() :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We use a <code>Callback</code> for handling what is returned from the huggingface model ... "the huggingface model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them is some regularization scheme)" - from the fastai <a href="http://dev.fast.ai/tutorial.transformers">Transformer's Tutorial</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sequence-classification">Sequence classification<a class="anchor-link" href="#Sequence-classification"> </a></h2><p>Below demonstrates how to setup your <code>blurr</code> pipeline for a sequence classification task (e.g., a model that requires a single text input)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>
<span class="n">imdb_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">imdb_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed offic...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">task</span> <span class="o">=</span> <span class="n">HF_TASKS_AUTO</span><span class="o">.</span><span class="n">SequenceClassification</span>

<span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;roberta-base&quot;</span> <span class="c1"># &quot;distilbert-base-uncased&quot; &quot;bert-base-uncased&quot;</span>
<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">BLURR_MODEL_HELPER</span><span class="o">.</span><span class="n">get_hf_objects</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;]
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># single input</span>
<span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">HF_TextBlock</span><span class="p">(</span><span class="n">hf_arch</span><span class="o">=</span><span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="o">=</span><span class="n">hf_tokenizer</span><span class="p">),</span> <span class="n">CategoryBlock</span><span class="p">)</span>

<span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> 
                   <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">),</span> 
                   <span class="n">splitter</span><span class="o">=</span><span class="n">ColSplitter</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">&#39;is_valid&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">imdb_df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I don't know where to begin. This movie feels a lot like one of those cheap Saturday morning kids shows that they used to make back in the late eighties early nineties. Sort of like Captain Power or the Power Rangers. It's full of bad digital overlays and really cheesy sounding "secret agencies" and villains.&lt;br /&gt;&lt;br /&gt;The acting is so bad that it's not even funny. The direction is terrible and there is little to now continuity. It seems as if someone just threw a bunch of scenes together and forgot that there was supposed to be a plot.&lt;br /&gt;&lt;br /&gt;Perhaps one of the most ridiculous scenes in the movie comes early on, when several villains plant an explosive device in an agents car. For some reason, even though the device is clearly stated as being "remote detonated" the bad guys decide to chase her down on their motorcycles as she drives away. This chase carries on. all the while with the bad guys doing ludicrous and completely pointless bike stunts. Standing up on the bikes, doing wheelies and so on. At one point, a crash happens and one of the attackers is thrown from his bike, we see the bike (clearly cgi) thrown over the agents car but the rider has vanished. Then, a few seconds later the rider and bike return...apparently unscathed by the crash. At this point even though the car has an explosive device planted in it, the attackers choose to shoot the agent while driving past, then blow up her car. Which was also clearly done with cgi. Sound confusing? It is, and so is the rest of the movie.&lt;br /&gt;&lt;br /&gt;I might point out that when I say cgi, we aren't talking about Lord Of The Rings type cgi here. We're talking the cheap cheesy Power Rangers type cgi, actually I think it would have been done better on Power Rangers.&lt;br /&gt;&lt;br /&gt;Why Savini and Todd did this movie I will never know, I can only assume they did for money, as a favor to someone or because they were blackmailed into it...probably the last one.</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training">Training<a class="anchor-link" href="#Training"> </a></h3><p>We'll also add in custom summary methods for blurr learners/models that work with dictionary inputs</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HF_BaseModelWrapper</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> 
                <span class="n">model</span><span class="p">,</span>
                <span class="n">opt_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="n">decouple_wd</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">HF_BaseModelCallback</span><span class="p">],</span>
                <span class="n">splitter</span><span class="o">=</span><span class="n">hf_splitter</span><span class="p">)</span>

<span class="n">learn</span><span class="o">.</span><span class="n">create_opt</span><span class="p">()</span>             <span class="c1"># -&gt; will create your layer groups based on your &quot;splitter&quot; function</span>
<span class="n">learn</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>.to_fp16()</code> requires a GPU so had to remove for tests to run on github</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Module.blurr_summary" class="doc_header"><code>Module.blurr_summary</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/modeling/core.py#L47" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Module.blurr_summary</code>(<strong>*<code>xb</code></strong>)</p>
</blockquote>
<p>Print a summary of <code>self</code> using <code>xb</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.blurr_summary" class="doc_header"><code>Learner.blurr_summary</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/modeling/core.py#L71" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.blurr_summary</code>()</p>
</blockquote>
<p>Print a summary of the model, optimizer and loss function.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have to create our own <code>summary</code> methods above because fastai only works where things are represented by a <em>single tensor</em>.  But in the case of huggingface transformers, a <em>single</em> sequence is represented by <em>multiple tensors</em> (in a dictionary).</p>
<p>The change to make this work is so minor I think that the fastai library can/will hopefully be updated to support this use case.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.blurr_summary()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>4
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#slow</span>
<span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">suggestions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.019054606556892395)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hc1Xnv8e870uhuyRfJ+IptsIkxMTagkDsxaXJqKAVCmgZC0tJSXJJD0py2OSGnTUjpyUl6cmsJocRNXTc5KYQSmkACJTeIk0AaTJABGwPGXCzLFwlb15Hm+p4/ZksIybrZ2tqj0e/zPPN4Zu81s9/l0cw7a6291zJ3R0REZLBY1AGIiEjhUXIQEZFhlBxERGQYJQcRERlGyUFERIZRchARkWFKow5gourr63358uVRhyEiMq08+uijbe7eMN7yoSUHM9sCXAQcdvfXHmP/BuB7wPPBprvc/caxXnf58uVs3759MkMVESl6ZvbiRMqH2XLYCtwMfGOUMj9394tCjEFERI5DaGMO7r4NOBLW64uISHiiHpB+o5ntMLP7zOyMkQqZ2SYz225m21tbW6cyPhGRGSnK5PAbYJm7rwO+Anx3pILuvtndG929saFh3OMpIiJynCJLDu7e6e7dwf17gbiZ1UcVj4iIvCKy5GBmC8zMgvvnBrG8HFU8IiLyijBPZb0N2ADUm1kzcAMQB3D3W4HfAz5oZhmgF7jcNX+4iMgx/XjXIZbXV7Fy/qwpOV5oycHdrxhj/83kT3UVEZExfPBbj/Inbz2Fj29cPSXHi/psJRERGUMqkyOddariJVN2TCUHEZEC15vKAlBVPnUzHik5iIgUuJ5UBoDqMrUcREQkkAiSg1oOIiIyoCeZ71ZSy0FERAb0dytVlanlICIigYEBabUcRESkX0+QHKrLlRxERCSQSKpbSUREhhhoOSg5iIhIv/6WQ6XGHEREpF9PKktZSYyy0qn7ylZyEBEpcL2pzJS2GkDJQUSk4PWkslN6ARwoOYiIFLxEKjOlU2eAkoOISMHrSRZRy8HMtpjZYTN7coxyrzOzjJn9XlixiIhMZ4lUZkqvcYBwWw5bgY2jFTCzEuDvgB+GGIeIyLTWk8xO6dXREGJycPdtwJExin0Y+A5wOKw4RESmu950lsoiajmMyswWA+8C/nEcZTeZ2XYz297a2hp+cCIiBaQnmSmeMYdx+Hvg4+6eG6ugu29290Z3b2xoaJiC0ERECkcilZ3yMYepPdqrNQK3mxlAPXChmWXc/bsRxiQiUlDcnZ5UZsrHHCJLDu6+ov++mW0Fvq/EICLyan3pHO5TOyMrhJgczOw2YANQb2bNwA1AHMDdbw3ruCIixaR/FbiiaTm4+xUTKHtVWHGIiExn/avAVcZnzoC0iIiM4ZWWwww5lVVERMbWk5z69aNByUFEpKAl1HIQEZGh1HIQEZFhetNBy2GmTJ8hIiJjU8tBRESG6R9z0GI/IiIyoL/loOscRERkQCKVoTJeQknMpvS4Sg4iIgWsJzX1C/2AkoOISEHrjWC6blByEBEpaD3JzJSfqQRKDiIiBS2/0I+Sg4iIDJJf6EfdSiIiMkgiWWQtBzPbYmaHzezJEfZfYmaPm1mTmW03s7eEFYuIyHTVk8pM+dQZEG7LYSuwcZT9PwHWuft64I+Br4cYi4jItNSbylJVTKeyuvs24Mgo+7vd3YOH1YCPVFZEZKbqSWVm3qmsZvYuM9sN/IB862GkcpuCrqftra2tUxegiEiEsjmnL50rrjGH8XD3/3D31cClwN+OUm6zuze6e2NDQ8PUBSgiEqGBhX5mWsuhX9AFdYqZ1Ucdi4hIoUikgum6i2nMYSxmttLMLLh/NlAOvBxVPCIihaYnGV3LIbQjmtltwAag3syagRuAOIC73wq8G/gDM0sDvcB7Bw1Qi4jMeAMthwjGHEJLDu5+xRj7/w74u7COLyIy3b2SHGbomIOIiAzXM7AK3AwacxARkdElglXgZuzZSiIiMtxAy2GmXecgIiIjS/SfraRZWUVEpF8iHd3ZSkoOIiIFKpHMEjMoL536r2olBxGRAtU/XXdwvfCUUnIQESlQiWQ003WDkoOISMGKaqEfUHIQESlYiYgW+gElBxGRgpWIaKEfUHIQESlYiVQ2ktNYQclBRKRg9SQ15iAiIkOo5SAiIsP0JDORTJ0BSg4iIgWrKFsOZrbFzA6b2ZMj7L/SzB43syfM7CEzWxdWLCIi000qkyOT86JsOWwFNo6y/3ngbe6+FvhbYHOIsYiITCuJYLruyng0LYcwlwndZmbLR9n/0KCHvwKWhBWLiMh00xMsEVo9wy+Cuxq4b6SdZrbJzLab2fbW1tYpDEtEJBr9aznM2IvgzOx88snh4yOVcffN7t7o7o0NDQ1TF5yISESibjlEk5ICZnYm8HXgAnd/OcpYREQKyYxtOZjZycBdwAfc/Zmo4hARKUSJ/pZDRMkhtKOa2W3ABqDezJqBG4A4gLvfCnwKmAfcEixkkXH3xrDiERGZTnqCs5WimpU1zLOVrhhj/58AfxLW8UVEprP+lkPRXQQnIiLHr2emjjmIiMjI1HIQEZFhelIZykpjxEui+ZpWchARKUC9qSzVEbUaQMlBRKQg9SSzkY03gJKDiEhByq8frZaDiIgM0pPKUhXRdN2g5CAiUpASyYzGHERE5NV6UhpzEBGRIXpTmchmZAUlBxGRgqSWg4iIDJNI6mwlEREZJJdzEmldBCciIoP0ZbK4o1NZRUTkFT3J/oV+irDlYGZbzOywmT05wv7VZvawmSXN7C/DikNEZLrp6E0DUF3oLQczqzazWHD/NDO72MziYzxtK7BxlP1HgI8AXxhPDCIiM8XOlg4ATjtpVmQxjLflsA2oMLPFwA+BD5D/8h+Ru28jnwBG2n/Y3R8B0uOMQURkRmja105FPMbqBYWfHMzdE8BlwC3u/h7gjPDCEhGZuZr2tbN2cR2lEa3lABNIDmb2RuBK4AfBtikbKTGzTWa23cy2t7a2TtVhRUSmXCqTY2dLJ+uXzo40jvEmh48CnwD+w913mtkpwAPhhfVq7r7Z3RvdvbGhoWGqDisiMuV2H+wklcmxfumcSOMY11C4u/8M+BlAMDDd5u4fCTMwEZGZqGlfOwDrT54GLQcz+zczqzWzauBJYJeZfWyM59wGPAy8xsyazexqM7vWzK4N9i8ws2bgz4G/DsrUnlh1RESmt6aX2qmvKWdRXUWkcYz3JNo17t5pZlcC9wHXA48Cnx/pCe5+xWgv6O4HgSXjDVREZCZo2tfO+qWzMbNI4xjvmEM8uK7hUuBud08DHl5YIiIzT0cizd62Hs6KuEsJxp8cvga8AFQD28xsGdAZVlAiIjPRjuZgvCHiM5Vg/APSNwE3Ddr0opmdH05IIiIzU9O+dsxg7ZK6qEMZ94B0nZl9qf9aAzP7IvlWhIiITJKmfe2c2lBDbcVYsxOFb7zdSluALuD3g1sn8C9hBSUiMtO4+8BgdCEY79lKp7r7uwc9/hszawojIBGRmWjfkV6O9KQKJjmMt+XQa2Zv6X9gZm8GesMJSURk5nls31GgMAajYfwth2uBb5hZ/yjJUeAPwwlJRGTm2bGvI/KZWAcb79lKO4B1/VcwBxfEfRR4PMzgRERmiqZ9RyOfiXWwCUXh7p3u3n99w5+HEI+IyIyTyuR4sqWTdUsKo0sJTmyZ0Giv7RYRKRIDM7EWwJXR/U4kOWj6DBGRSfDsoW4A1iwsnLlHRx1zMLMujp0EDKgMJSIRkRlmf3v+5M9Fswvna3XU5ODuhTFsLiJSxJqPJpg/q5yK+JQtsDmmwhgWFxGZwZqP9rJ4TuG0GkDJQUQkcvvbe1kypyrqMF4ltORgZlvM7LCZPTnCfjOzm8xsj5k9bmZnhxWLiEihyuWclvZeFhfQeAOE23LYCmwcZf8FwKrgtgn4xxBjEREpSIe7kqSzzpKZ0q3k7tuAI6MUuQT4huf9CphtZgvDikdEpBA1H00AaMxhkMXAvkGPm4Ntw5jZpv61JFpbW6ckOBGRqdB/GutSJYeJc/fN7t7o7o0NDQ1RhyMiMmmajxbeNQ4QbXLYDywd9HhJsE1EZMZoPtrLvOoyqsrGO0n21IgyOdwN/EFw1tIbgA53PxBhPCIiU675aKLgBqNh/Os5TJiZ3QZsAOrNrBm4AYgDuPutwL3AhcAeIAH8UVixiIgUqv3tvQWzhsNgoSUHd79ijP0O/Pewji8iUujcnf1He3nH6SdFHcow02JAWkSkGLV2J0lmcgV3ARwoOYiIRGZ/cKZSIY45KDmIiESk/zTWQrsADpQcREQi038BnLqVRERkQPPRBHWVcWZVxKMOZRglBxGRiOw/2luQ4w2g5CAiEpnmo4U3VXc/JQcRkQi4e0Eu8tNPyUFEJAJHE2kSqay6lURE5BWFuo5DPyUHEZEIFPIFcKDkICISif4L4JbM1piDiIgE9rf3Mqu8lNrKwlrHoZ+Sg4hIBJqPJlg8pxIzizqUY1JyEBGJQHMBXwAHIScHM9toZk+b2R4zu/4Y+5eZ2U/M7HEze9DMloQZj4hIodhfwBfAQYjJwcxKgK8CFwBrgCvMbM2QYl8AvuHuZwI3Ap8NKx4RkULR0ZumK5kp2AvgINyWw7nAHnff6+4p4HbgkiFl1gA/De4/cIz9IiJFp/8ah5narbQY2DfocXOwbbAdwGXB/XcBs8xsXogxiYhEbn8Br+PQL+oB6b8E3mZmjwFvA/YD2aGFzGyTmW03s+2tra1THaOIyKTadaATKMx1HPqFmRz2A0sHPV4SbBvg7i3ufpm7nwX8VbCtfegLuftmd29098aGhoYQQxYRCddDe9q4+ad7OO+0BuZWl0UdzojCTA6PAKvMbIWZlQGXA3cPLmBm9WbWH8MngC0hxiMiEqk9h7u49v89yor6am5+31kFe40DhJgc3D0DXAfcDzwF3OHuO83sRjO7OCi2AXjazJ4BTgI+E1Y8IiJRautO8kdbH6GsNMaWq15HbQGu/jZYqNdtu/u9wL1Dtn1q0P07gTvDjEFEJGp96SzXfGM7hzuTfPtP38jSuYV7Cmu/wpzUQ0SkiPyvu56gaV87t7zvbNYvnR11OOMS9dlKIiJF7T8ea+aux/bzZ7+1igvWLow6nHFTchARCcmLL/fwye/u5Nzlc/nw21dFHc6EKDmIiIQgnc3xZ7c3YQZfvnw9JbHCPTPpWDTmICISgr//8TM07Wvnq+87u6AvdhuJWg4iIpPs4ede5pYHn+O9jUv5nTOnzzjDYEoOIiKT7DP37mLZ3Co+9btDJ6KePpQcREQmUV86y1MHurjozEVUl0/fnnslBxGRSfTsoW6yOWfNotqoQzkhSg4iIpNoZ0sHAGcoOYiISL9dBzqpKS9laQGv8jYeSg4iIpNoV0snpy+cRWyaXdcwlJKDiMgkyeWcpw50csaiuqhDOWFKDiIik+TFIwl6UlnWLJze4w2g5CAiMmn6B6On+5lKoOQgIjJpdrV0UhozVp1UE3UoJyzU5GBmG83saTPbY2bXH2P/yWb2gJk9ZmaPm9mFYcYjIhKmXQc6WTm/hvLSkqhDOWGhJQczKwG+ClwArAGuMLOh15L/NfnlQ88iv8b0LWHFIyIStp0txTEYDeG2HM4F9rj7XndPAbcDlwwp40B/51wd0BJiPCIioTnc1UdrV7Ioxhsg3OSwGNg36HFzsG2wTwPvN7Nm8mtNf/hYL2Rmm8xsu5ltb21tDSNWEZETsqulE6AozlSC6AekrwC2uvsS4ELgm2Y2LCZ33+zuje7e2NDQMOVBioiMZdeBIDmo5TCm/cDSQY+XBNsGuxq4A8DdHwYqgPoQYxIRCcXOlk6WzKmkrjIedSiTIszk8AiwysxWmFkZ+QHnu4eUeQn4LQAzO518clC/kYhMO0+1dE77yfYGCy05uHsGuA64H3iK/FlJO83sRjO7OCj2F8A1ZrYDuA24yt09rJhERMLQk8zw/Ms9rFlYHGcqQchrSLv7veQHmgdv+9Sg+7uAN4cZg4hI2HYf7MS9eMYbIPoBaRGRacXdadrXTl86O7Ct/0ylYupWmr5r2ImITLH2RIrrv/ME/7nzICfVlnPd21fx3sal7GzpZHZVnIV1FVGHOGmUHERExuG/9r7MR7/dRFt3kg9tOJVfP3+ET373SW598Ln8sqALazGb3ms4DKbkICIyimzO+YefPMvNP32Wk+dW8Z0Pvokzl8zG3dn2bBtf/OHTPN7cwaVnDb3Gd3pTchARGcW//folbvrJs1x29mJuvOS11JTnvzbNjLed1sB5q+r5zUvtnFYEM7EOpuQgIjKCdDbHrQ8+xznL5vDF96w7ZreRmXHOsjkRRBcuna0kIjKCu5ta2N/ey4c2nFpU4wnjoeQgInIMuZzzjz97jtULZvH21fOjDmfKKTmIiBzDD3cdYs/hbj50/soZ12oAJQcRkWHcnVse3MPyeVX8ztqFUYcTCSWHSZTLOfvbe9H0UCLT2y/2tPF4cwfXvu1USmIzr9UAOltpVPfsaGH7C0dIZnIkMzlSmRwNs8q56MyFnH3yHGLBH00mm+Oex1u45YHnePZwN6sXzOLK15/MpWctZlbFq6fv7UtnqYhP//VlRYrZLQ88x4LaCt51dnFduzARSg4j+OoDe/j8/U8zq7yUyrISyuMxykpi/GR3L1sfeoHFsyu5eP0iTppVzj//8nn2HenltJNq+It3nsb9uw7yye/t5LP37ebCtQvJufPiywlefLmHtu4UF65dwJd+f72ShEgB+PXzR/jFs6+sFNCTyvLw3pf56985nfLSmfsZVXIYwt358o+f5aafPMul6xfxhfeso7Tkld637mSGH+48yPeaWti8bS/ZnLNu6Ww+ddEZ/Nbq+cRixnVvX8mO5g6+9asX+cETB6itiLNsXhXvOP0k4iUxvvmrF2nr+jX/9AeN1FUd/8Ig2ZzT0ZumPZHCzCiNGfGSGGWlMeZUxUMZROtLZ7l/50Hu2XGA2VVxNrymgbeubDiheohEIZfLjyt88UfP4A6DPy4nz63iinNPji64AmDTrX+8sbHRt2/fHspruzufu283X9u2l/c2LuX/XLZ21P7Gtu4khzr7Rp1Txd2H7bt7Rwt/cUcTp9TXsPWPX8fCusphz8vlnPbeNEd6khzo6OOFth6eb0vwfFs3Lx1JcKQnRXtvmpHevjULa7nu7SvZeMaCge6v4+HuHOlJsbeth+817ed7TS109WVYWFdBIpWlozdNScw4++TZvO20Bt6yqoG1i+tmbD9tFHpTWQ529uHuzKqIU1tZOqFfvH3pLDtbOtl9sJPeVJZ01slkc2Ry+T+umBkxg1jMyOacdDZHKpsjnXFKS4y6yjizq+LMrixjdlWc2iCGWRVxKuIxuvoytCfyP2J6UhlWNsxi6dzKY35m+tJZSmP2qh9kE5HNOS++3MPug13sPtDJ7oNd7Gnt5vSFtby3cSlvWVlPLGZ0JNL8jzua+Onuw1yyfhGfvWwtVWXF/VvZzB5198Zxlw8zOZjZRuAfgBLg6+7+uSH7vwycHzysAua7++zRXjPM5HDjPbvY8svn+cAblvE3F59xQl+qY3loTxubvvkotRWlXHPeKRzo6GPfkQTNR3s50NHH0USKbO7V701lvIQV9dUsm1dFfU05c6rLmFsVp64qjmGksjkyWaerL823H9nH3rYeVs6v4brzV3LOsjnsO5p//eYjCQ51JjmSSHG0J8WRRIpEMktFPEZlWSmV8RilsRiHuvo40NFHKpMDoLw0xgWvXcB7GpfyxlPmkQumLn7w6VYefOYwT+7PT1tcVxnnzSvncc6yuSysq+Ck2nJOqq1gbnUZhuE47lASs2nftbbncDcP7D5Mzn2g1RYvMbr6MrR1pzjSk+Tl7hRdyQy9qSyJVP5fM2NWRWlwy3+JukPO8/83DpSV5F+rNPi3f+wrmc7Rl87S1p3/4dDRmx4WV1lpjIaacpbXV7FsXjXLg7+ZnlSWRDJDTzJDa3eKJ/a3s/tA10AiGA+zfGxlJTFS2XxMEzW3uox1S+pYu2Q2PckMz7V281xrN81HeykvjfHaRXWcuWQ265bW8foV81gwwmynbd1J7n3iALtaOnnqQCdPH+qiL52PJ2awor6aFfU1bH/xCO2JNItnV3LJ+kXc83gLBzv6+ORFa/jAG5bNiFNVCyY5mFkJ8AzwTqCZ/LKhVwQL/Byr/IeBs9z9j0d73bCSw492HeKab2znqjct54bfXTMlfyw7Wzq46l8eobUrSVlJjCVzKlkyt4pFdRXU15Qzr6aMeTXlzJ9Vzor6aubPKh93XNmcc+8TB7j5p3t4+lDXq/bFDOpryplbXcbc6jLmVJVRVVZCMpOjN52lL50dGHxfNLuShXUVLJpdyRtPnUdtxcjdR23dSX65p41fPNvGz59t42Bn35hxzq6Ks3ROFUvmVLJodiUxY+ALMJnJEgsSSEVpftynuqyEWRXxgS/V6vKSgf0V8Rjl8RJKY0ZJzAZ+gVaXlRzX+5nNOd19GdK5HNVlpVTEY5gZnX1pvr/jAP/+6D4ee6l9xOfHSyz4Py6ntiI/dlVVVkJlvBTH6erL0N2XoSuZpi+dI2Zg2ED3Rib4lZ7O5H/Fx0ti+ToG/xf1NeUsqK1gQV0FC2oriMWgqy9DZ2+arr4Mhzr7eCEY6zqaeHUCMcsn8TMW1bJuyWzWLZ3NGYtqqa2ME4/FKC3J//8BA0kr605pLDasVdiXzuZbBr0p2hPpgRg6+/L1qq0sHWhVVMRj7D7YRdNL7TTta2dPazflpTFOqa/h1Pk1nNpQTUdvmsebO9jZ0kFfOkdJzNh4xgL+6M3LOWfZHMyMlvZeNm/by+2PvERfOsecqjinL6xl9YJaVi+cxZqFtaycXzPw4yOZyfKjXYf49iP7+MWeNhbUVvDVK8/m7JOLb9qLkRRScngj8Gl3/+3g8ScA3P2zI5R/CLjB3X802uuGkRy6kxne+aWfUVsR5/sfeQvx42zSHo/eoGtm/qzyUFoquZzz4DOHOdyZZOncKpbOqWLh7IrQ69jfHXWoM8mhrj4Od/ZxpCf/BWUGRv7L70BHL/uO9NJ8NEFLez6ZlMdjVJSWUFYaI5tzkpksfcGv5Yn8wu1XGrOglZX/giorzX/BlZgRixnpbI5EMksinSGRytKTzH9p96Syr3odM6guKyWVyXerrJpfw3sal3DJ+sXMqgi2B/tmVcSprSgtmF+kHYk0RxMpqspLqCkvpTJ+fAlzsvWmspSXxo75t5/J5nj6UBd3N7Vw269forMvw9rFdaycX8M9O1oAeNdZi9l03imsnF8z7vq0diXz/wdl07vVOlETTQ5hdrItBvYNetwMvP5YBc1sGbAC+GmI8Yzoiz98moOdfdz8vrOnNDEAVJaVhPpHGosZb199UmivPxIzY15NOfNqylnD5K2Olcxk6erLBLc0PcksfZksfan8v8l0jqw72Vz+lsrk6OhNc6QnxZGeFEcTKbr6MvlfwkGZstIYlfESGmrKqSorpbo83zqpKc93+5SVxkj0d8ek8n3iF6xdyLolda/6Qqoqm7RqTrq6oPux0Iz2t19aEuOMRXWcsaiOP3vHKu76zX62PvQC9z5xgPe/YRnXnHcKi2cPH68bS8Os8hMJecYolBGYy4E73T17rJ1mtgnYBHDyyZN7BsHjze3860Mv8P7XLyvKmRWLTXlpCeU1JdTX6AM+k1SVlfL+NyzjytefTDbnxz1gLeMX5v/wfmDpoMdLgm3Hcjlw20gv5O6b3b3R3RsbGhomLcBMNsf133mC+ppyPrbxNZP2uiISDrPjP5NJJibM/+VHgFVmtsLMysgngLuHFjKz1cAc4OEQYzmmLb98nl0HOvmbi88YdaBVRGSmCS05uHsGuA64H3gKuMPdd5rZjWZ28aCilwO3+xRecJHO5vjmwy/wpR89wztOn8/G1y6YqkOLiEwLoY45uPu9wL1Dtn1qyONPhxnDkGNx35MH+fz9T/N8Ww/nrpjLZ961tiDO2hARKSSFMiAduieaO/jk956kaV9+rdctVzVy/mvmKzGIiBzDjEkOyUyWgx19/N93n8m7z1mi6R1EREYxY5JD4/K5bPuf51NWqjMdRETGMqO+KZUYRETGR9+WIiIyjJKDiIgMo+QgIiLDKDmIiMgwSg4iIjKMkoOIiAyj5CAiIsOEuoZ0GMysFXhx0KY6oGOc9+uBthM4/ODXPJ4yx9o3dNtE6gMnVqeprs/Qx/33p7I+o5VTfQr7MzQT6zN024nUZ5m7j3/NA3ef1jdg83jvA9sn61jHU+ZY+4Zum0h9TrROU12fUd6XKavPaOVUn8L+DM3E+oynDpNZn8G3YuhWumeC9yfrWMdT5lj7hm4r5voMfXzPCGWO13hfZ6Ryqk9h/83NxPoM3RZ2fQZMu26lE2Fm230CC2xPB8VWJ9WnsKk+hW0y61MMLYeJ2Bx1ACEotjqpPoVN9Slsk1afGdVyEBGR8ZlpLQcRERkHJQcRERlGyUFERIZRcgiY2VvN7FYz+7qZPRR1PCfKzGJm9hkz+4qZ/WHU8ZwoM9tgZj8P3qMNUcczGcys2sy2m9lFUccyGczs9OD9udPMPhh1PCfKzC41s38ys2+b2X+LOp4TZWanmNk/m9md4ylfFMnBzLaY2WEze3LI9o1m9rSZ7TGz60d7DXf/ubtfC3wf+Ncw4x3LZNQHuARYAqSB5rBiHY9Jqo8D3UAFxVEfgI8Dd4QT5cRM0mfoqeAz9PvAm8OMdyyTVJ/vuvs1wLXAe8OMdyyTVJ+97n71uA86WVfTRXkDzgPOBp4ctK0EeA44BSgDdgBrgLXkE8Dg2/xBz7sDmDXd6wNcD/xp8Nw7i6A+seB5JwHfKoL6vBO4HLgKuCjK+kxWnYLnXAzcB7yvGOoTPO+LwNlFVJ9xfR+UUgTcfZuZLR+y+Vxgj7vvBTCz24FL3P2zwDGb8WZ2MtDh7l0hhjumyaiPmTUDqeBhNrxoxzZZ70/gKFAeRpzjNUnvzwagmvyHudfM7nX3XJhxj2ay3iN3vxu428x+APxbeBGPbpLeIwM+B9zn7r8JN+LRTfJnaFyKIs7UrFEAAAQBSURBVDmMYDGwb9DjZuD1YzznauBfQovoxEy0PncBXzGztwLbwgzsOE2oPmZ2GfDbwGzg5nBDOy4Tqo+7/xWAmV0FtEWZGEYx0fdoA3AZ+eR9b6iRHZ+JfoY+DLwDqDOzle5+a5jBHYeJvj/zgM8AZ5nZJ4IkMqJiTg4T5u43RB3DZHH3BPlkVxTc/S7yCa+ouPvWqGOYLO7+IPBgxGFMGne/Cbgp6jgmi7u/TH78ZFyKYkB6BPuBpYMeLwm2TVeqT2ErtvpA8dVJ9ZmAYk4OjwCrzGyFmZWRH/y7O+KYToTqU9iKrT5QfHVSfSYiyhH4SRzJvw04wCunbV4dbL8QeIb8iP5fRR2n6qP6FOqt2Oqk+pz4TRPviYjIMMXcrSQiIsdJyUFERIZRchARkWGUHEREZBglBxERGUbJQUREhlFykKJgZt1TfLxJWfMjWKeiw8yazGy3mX1hHM+51MzWTMbxRUai5CByDGY26rxj7v6mSTzcz919PXAWcJGZjbUWwqXkZ3MVCY2SgxQtMzvVzP7TzB61/Cpyq4Ptv2tm/2Vmj5nZj83spGD7p83sm2b2S+CbweMtZvagme01s48Meu3u4N8Nwf47g1/+3wqmesbMLgy2PWpmN5nZ90eL1917gSbys21iZteY2SNmtsPMvmNmVWb2JvJrJnw+aG2cOlI9RU6EkoMUs83Ah939HOAvgVuC7b8A3uDuZwG3A/9z0HPWAO9w9yuCx6vJTxV+LnCDmcWPcZyzgI8Gzz0FeLOZVQBfAy4Ijt8wVrBmNgdYxStTrN/l7q9z93XAU+SnTHiI/Pw5H3P39e7+3Cj1FDlumrJbipKZ1QBvAv49+CEPrywStAT4tpktJL+C1vODnnp38Au+3w/cPQkkzeww+ZXohi5T+mt3bw6O2wQsJ7+k6V5373/t24BNI4T7VjPbQT4x/L27Hwy2v9bM/jf5NSxqgPsnWE+R46bkIMUqBrQHfflDfQX4krvfHSxQ8+lB+3qGlE0Oup/l2J+Z8ZQZzc/d/SIzWwH8yszucPcmYCtwqbvvCBYF2nCM545WT5Hjpm4lKUru3gk8b2bvgfySj2a2Lthdxyvz3v9hSCE8DZwyaGnHMReoD1oZnwM+HmyaBRwIurKuHFS0K9g3Vj1FjpuSgxSLKjNrHnT7c/JfqFcHXTY7gUuCsp8m3w3zKNAWRjBB19SHgP8MjtMFdIzjqbcC5wVJ5ZPAfwG/BHYPKnM78LFgQP1URq6nyHHTlN0iITGzGnfvDs5e+irwrLt/Oeq4RMZDLQeR8FwTDFDvJN+V9bWI4xEZN7UcRERkGLUcRERkGCUHEREZRslBRESGUXIQEZFhlBxERGQYJQcRERnm/wOupCoBILsNfQAAAABJRU5ErkJggg==
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#slow</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr_max</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.712207</td>
      <td>0.658210</td>
      <td>0.535000</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.568098</td>
      <td>0.644893</td>
      <td>0.575000</td>
      <td>00:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.450438</td>
      <td>0.486276</td>
      <td>0.805000</td>
      <td>00:19</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Showing-results">Showing results<a class="anchor-link" href="#Showing-results"> </a></h3><p>And here we creat a @typedispatched impelmentation of <code>Learner.show_results</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">show_results</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The trouble with the book, "Memoirs of a Geisha" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and misconceptions. So what is this movie if it isn't about Japan or geisha? Is it pure fantasy as so many people have said? Yes, but then why make it into an American fantasy?&lt;br /&gt;&lt;br /&gt;There were so many missed opportunities. Imagine a culture where there are no puritanical hang-ups, no connotations of sin about sex. Sex is natural and normal. How is sex handled in this movie? Right. Like it was dirty. The closest thing to a sex scene in the movie has Sayuri wrinkling up her nose and grimacing with distaste for five seconds as if the man trying to mount her had dropped a handful of cockroaches on her crotch. &lt;br /&gt;&lt;br /&gt;Does anyone actually enjoy sex in this movie? Nope. One character is said to be promiscuous but all we see is her pushing away her lover because it looks like she doesn't want to get caught doing something dirty. Such typical American puritanism has no place in a movie about Japanese geisha.&lt;br /&gt;&lt;br /&gt;Did Sayuri enjoy her first ravishing by some old codger after her cherry was auctioned off? Nope. She lies there like a cold slab of meat on a chopping block. Of course she isn't supposed to enjoy it. And that is what I mean about this movie. Why couldn't they have given her something to enjoy? Why does all the sex have to be sinful and wrong?&lt;br /&gt;&lt;br /&gt;Behind Mameha the Chairman was Sayuri's secret patron, and as such he was behind the auction of her virginity. He could have rigged the auction and won her himself. Nobu didn't even bid. So why did the Chairman let that old codger win her and, reeking of old-man stink, get his fingers all over her naked body? Would any woman ever really forgive a man for that?&lt;br /&gt;&lt;br /&gt;Let's</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.blurr_predict" class="doc_header"><code>Learner.blurr_predict</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/modeling/core.py#L100" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.blurr_predict</code>(<strong><code>item</code></strong>, <strong><code>rm_type_tfms</code></strong>=<em><code>None</code></em>, <strong><code>with_input</code></strong>=<em><code>False</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as with <code>summary</code>, we need to replace fastai's <code>Learner.predict</code> method with the one above which is able to work with inputs that are represented by multiple tensors included in a dictionary.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">blurr_predict</span><span class="p">(</span><span class="s1">&#39;I really liked the movie&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;positive&#39;, tensor(1), tensor([0.4383, 0.5617]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#slow</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr_max</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.352056</td>
      <td>0.243431</td>
      <td>0.905000</td>
      <td>00:50</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.129936</td>
      <td>0.308740</td>
      <td>0.910000</td>
      <td>00:50</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.096122</td>
      <td>0.325176</td>
      <td>0.910000</td>
      <td>00:50</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">plot_loss</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">show_results</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adrian has just gone out of the asylum, being rich and with no parents, his life seems empty. One day, he meets Gonzalo, a poor boy whom mother is prostitute. Desperate for earning some money, Gonzalo helps Adrian to search about his life and who where his parents. This is a movie from a new director, and it is perfectly clear in most of the film: scenes not correctly directed, dialogues a little forced, some incoherences in the script...Anyway, the ending is unexpectedly well done (well, just a little) and that saves a little the film. Actors are known and with great quality, nevertheless, they are not inspired enough to make the movie interesting; all of them have done better papers in other film. The film results boring and probably you will spend most of the time thinking how much time will pass until it ends. Of course there are lots of worse films, but, sure, there are many many better ones.</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">blurr_predict</span><span class="p">(</span><span class="s2">&quot;This was a really good movie&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;positive&#39;, tensor(1), tensor([0.0543, 0.9457]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">blurr_predict</span><span class="p">(</span><span class="s2">&quot;Acting was so bad it was almost funny.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;negative&#39;, tensor(0), tensor([0.9956, 0.0044]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Inference">Inference<a class="anchor-link" href="#Inference"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s1">&#39;seq_class_learn_export.pkl&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inf_learn</span> <span class="o">=</span> <span class="n">load_learner</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s1">&#39;seq_class_learn_export.pkl&#39;</span><span class="p">)</span>
<span class="n">inf_learn</span><span class="o">.</span><span class="n">blurr_predict</span><span class="p">(</span><span class="s2">&quot;This movie should not be seen by anyone!!!!&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;negative&#39;, tensor(0), tensor([0.9461, 0.0539]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inf_learn</span><span class="o">.</span><span class="n">blurr_predict</span><span class="p">(</span><span class="s2">&quot;This movie should not be seen by anyone!!!!&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;negative&#39;, tensor(0), tensor([0.9461, 0.0539]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tests">Tests<a class="anchor-link" href="#Tests"> </a></h2><p>The tests below to ensure the core training code above works for <strong>all</strong> pretrained sequence classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.</p>
<p><strong>Note</strong>: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue <em>(or a PR if you'd like to fix it yourself)</em></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BLURR_MODEL_HELPER</span><span class="o">.</span><span class="n">get_models</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;SequenceClassification&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[transformers.modeling_albert.AlbertForSequenceClassification,
 transformers.modeling_auto.AutoModelForSequenceClassification,
 transformers.modeling_bart.BartForSequenceClassification,
 transformers.modeling_bert.BertForSequenceClassification,
 transformers.modeling_camembert.CamembertForSequenceClassification,
 transformers.modeling_distilbert.DistilBertForSequenceClassification,
 transformers.modeling_electra.ElectraForSequenceClassification,
 transformers.modeling_flaubert.FlaubertForSequenceClassification,
 transformers.modeling_longformer.LongformerForSequenceClassification,
 transformers.modeling_mobilebert.MobileBertForSequenceClassification,
 transformers.modeling_roberta.RobertaForSequenceClassification,
 transformers.modeling_xlm.XLMForSequenceClassification,
 transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification,
 transformers.modeling_xlnet.XLNetForSequenceClassification]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pretrained_model_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;albert-base-v1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;facebook/bart-base&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span>
    <span class="s1">&#39;camembert-base&#39;</span><span class="p">,</span>
    <span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">,</span>
    <span class="s1">&#39;monologg/electra-small-finetuned-imdb&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flaubert/flaubert_small_cased&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;allenai/longformer-base-4096&#39;</span><span class="p">,</span>
    <span class="s1">&#39;google/mobilebert-uncased&#39;</span><span class="p">,</span>
    <span class="s1">&#39;roberta-base&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xlm-mlm-en-2048&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xlm-roberta-base&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xlnet-base-cased&#39;</span>
<span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;models&#39;</span><span class="p">)</span>
<span class="n">imdb_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#slow</span>
<span class="n">task</span> <span class="o">=</span> <span class="n">HF_TASKS_AUTO</span><span class="o">.</span><span class="n">SequenceClassification</span>

<span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">pretrained_model_names</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;=== </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1"> ===</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">BLURR_MODEL_HELPER</span><span class="o">.</span><span class="n">get_hf_objects</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                                                                   <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> 
                                                                                   <span class="n">config_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;num_labels&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;architecture:</span><span class="se">\t</span><span class="si">{</span><span class="n">hf_arch</span><span class="si">}</span><span class="se">\n</span><span class="s1">tokenizer:</span><span class="se">\t</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="se">\n</span><span class="s1">model:</span><span class="se">\t\t</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">HF_TextBlock</span><span class="p">(</span><span class="n">hf_arch</span><span class="o">=</span><span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="o">=</span><span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span> <span class="n">CategoryBlock</span><span class="p">)</span>

    <span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> 
                       <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> 
                       <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">),</span> 
                       <span class="n">splitter</span><span class="o">=</span><span class="n">ColSplitter</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">&#39;is_valid&#39;</span><span class="p">))</span>
    
    <span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">imdb_df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">HF_BaseModelWrapper</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
    <span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> 
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">opt_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="n">decouple_wd</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
                    <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">HF_BaseModelCallback</span><span class="p">],</span>
                    <span class="n">splitter</span><span class="o">=</span><span class="n">hf_splitter</span><span class="p">)</span>

    <span class="n">learn</span><span class="o">.</span><span class="n">create_opt</span><span class="p">()</span>             <span class="c1"># -&gt; will create your layer groups based on your &quot;splitter&quot; function</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
    
    <span class="n">b</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*** TESTING DataLoaders ***&#39;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]),</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">]))</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">4</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- PASSED: Batch inputs/targets ---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- FAILED: Batch inputs/targets ---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*** TESTING One pass through the model ***&#39;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- PASSED: Predictions ---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- FAILED: Predictions ---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*** TESTING Training/Results ***&#39;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">lr_max</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- PASSED: Learner.fit() ---&#39;</span><span class="p">)</span>
        <span class="n">learn</span><span class="o">.</span><span class="n">show_results</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- FAILED: Learner.fit() ---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== albert-base-v1 ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: [&#39;predictions.bias&#39;, &#39;predictions.LayerNorm.weight&#39;, &#39;predictions.LayerNorm.bias&#39;, &#39;predictions.dense.weight&#39;, &#39;predictions.dense.bias&#39;, &#39;predictions.decoder.weight&#39;, &#39;predictions.decoder.bias&#39;]
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	albert
tokenizer:	AlbertTokenizer
model:		AlbertForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.699314</td>
      <td>0.662049</td>
      <td>0.600000</td>
      <td>00:06</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen.br /br /the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall.br</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>it seems evident from this adaptation that he did not. not only did he leave the plot behind, he made up his own! the things that he chose to leave in were so ridiculously unbelievable that i was happy he chose to leave out some of the most important parts of the novel. the plot was hazy, inconsistent and choppy to say the least. i don't want to say anything mean-spirited about the actors, but they can't act! dickens is difficult, of course, but this is pathetic! micawber was nothing more than a mid-nineteenth century kramer,</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== facebook/bart-base ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForSequenceClassification: [&#39;final_logits_bias&#39;]
- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: [&#39;classification_head.dense.weight&#39;, &#39;classification_head.dense.bias&#39;, &#39;classification_head.out_proj.weight&#39;, &#39;classification_head.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	bart
tokenizer:	BartTokenizer
model:		BartForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.577024</td>
      <td>0.683855</td>
      <td>0.635000</td>
      <td>00:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The movie is great and I like the story. I prefer this movie than other movie such The cell ( sick movie ) and Highlander ( silly movie ). I just tell the truth, I like a reality hehe and also a true story :)&lt;br /&gt;&lt;br /&gt;</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== bert-base-uncased ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;]
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	bert
tokenizer:	BertTokenizer
model:		BertForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.681477</td>
      <td>0.676787</td>
      <td>0.575000</td>
      <td>00:07</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>i like movies about ufos, which is why i recently decided to rewatch eyes behind the stars after seeing it when i was a kid back in the late 1970s. and now i'm compelled to write a review about it because i'm afraid i'll start forgetting everything about it fast. you see, even though ebts ain't bad, it's very dull and nondescript. the story is sorta interesting but flat. the actors are good but their roles are boring and a little confusing. the fx are terribly amateurish but i can overlook something like that if the movie is compelling,</td>
      <td>negative</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== camembert-base ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;]
- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	camembert
tokenizer:	CamembertTokenizer
model:		CamembertForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.702532</td>
      <td>0.682081</td>
      <td>0.535000</td>
      <td>00:07</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>As a long time fan of Peter O'Donnell's greatest creation, I watched this film on DVD with no great hopes of enjoyment; indeed I expected to be reaching in disgust for the remote control within fifteen minutes. But instead I thoroughly enjoyed this production, and I especially enjoyed and appreciated how the producers and director succeeded in telling the Modesty Blaise back story. They managed to a</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== distilbert-base-uncased ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_transform.weight&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_layer_norm.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;]
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;pre_classifier.weight&#39;, &#39;pre_classifier.bias&#39;, &#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	distilbert
tokenizer:	DistilBertTokenizer
model:		DistilBertForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.668580</td>
      <td>0.671489</td>
      <td>0.565000</td>
      <td>00:04</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>this really doesn't match up to castle of cagliostro. lupin isn't as funny or wacky or as hyperactive. the scenery and music are uninspired and plot just isn't interesting. &lt; br / &gt; &lt; br / &gt; the only good thing about this'un is the nudity ( only in the uncut version ) provided by fujiko. it helped spice up some of the tedious scenes. coc had a formidable villain and set up the movie for some imaginative set - pieces. the locations in tsotg are not very vivid or engaging. &lt; br / &gt; &lt;</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== monologg/electra-small-finetuned-imdb ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at monologg/electra-small-finetuned-imdb were not used when initializing ElectraForSequenceClassification: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/electra-small-finetuned-imdb and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	electra
tokenizer:	ElectraTokenizer
model:		ElectraForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.253925</td>
      <td>0.391201</td>
      <td>0.860000</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>a grade - z horror filmmaker carl monson was one of the most prolific directors operating within the field of the low - budget gory mayhem. his movies are full of inept gore, laughable acting, boring sub - plots and woeful dialogue. a mysterious black clad figure is savagely murdering guests staying at the family mansion. unfortunately this film is almost bloodless. you don't actually see the murders except with shadows and a few blood splatters. the pace is lethargic and the plot is rather uninteresting. the acting is merely competent, but the lack of gore and mutilation</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== flaubert/flaubert_small_cased ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at flaubert/flaubert_small_cased were not used when initializing FlaubertForSequenceClassification: [&#39;pred_layer.proj.bias&#39;, &#39;pred_layer.proj.weight&#39;]
- This IS expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_small_cased and are newly initialized: [&#39;sequence_summary.summary.weight&#39;, &#39;sequence_summary.summary.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	flaubert
tokenizer:	FlaubertTokenizer
model:		FlaubertForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- FAILED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.000000</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- FAILED: Learner.fit() ---

=== allenai/longformer-base-4096 ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;]
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	longformer
tokenizer:	LongformerTokenizer
model:		LongformerForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.649169</td>
      <td>0.638569</td>
      <td>0.685000</td>
      <td>00:38</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>
      <td>positive</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>While I count myself as a fan of the Babylon 5 television series, the original movie that introduced the series was a weak start. Although many of the elements that would later mature and become much more compelling in the series are there, the pace of The Gathering is slow, the makeup somewhat inadequate, and the plot confusing. Worse, the characterization in the premiere episode is poor. Although the ratings chart shows that many fans are willing to overlook these problems, I remember The Gathering almost turned me off off what soon grew into a spectacular series.</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== google/mobilebert-uncased ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.dense.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;]
- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	mobilebert
tokenizer:	MobileBertTokenizer
model:		MobileBertForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>338351.906250</td>
      <td>167783.281250</td>
      <td>0.535000</td>
      <td>00:12</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>the coming attractions to " the order " make it seem like a decent horror mystery / thriller, but what we get is a plot that has potential to be excellent all thrown together to form a pile of garbage. &lt; br / &gt; &lt; br / &gt; first off the whole movie consists of terrible dialogue and god awful special affects. the acting was also nothing to be proud of, but keath ledger ( i think i spelled that right. ) saved the movie in this category. &lt; br / &gt; &lt; br / &gt; for heaven's sake : don't see this movie!</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== roberta-base ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;]
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	roberta
tokenizer:	RobertaTokenizer
model:		RobertaForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.708608</td>
      <td>0.670014</td>
      <td>0.610000</td>
      <td>00:07</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I got all excited when I saw the ads for this movie because I recently read the book and really enjoyed it. The movie, however, did not meet my expectations. Having read the book recently prepared me for big let down as often happens when stories are translated into movies. The characters didn't seem to fit very well with the book. The direction was weak. I had a hard time getting into the characters. There wasn't a real connection with the viewer about what was going on. The dialog didn't explain adequately what was happening. It just seemed slapped together and rushed through. All in all I was very disappointed with the</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== xlm-mlm-en-2048 ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: [&#39;pred_layer.proj.weight&#39;, &#39;pred_layer.proj.bias&#39;]
- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: [&#39;sequence_summary.summary.weight&#39;, &#39;sequence_summary.summary.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	xlm
tokenizer:	XLMTokenizer
model:		XLMForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- FAILED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.000000</td>
      <td>00:01</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- FAILED: Learner.fit() ---

=== xlm-roberta-base ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;]
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	xlm_roberta
tokenizer:	XLMRobertaTokenizer
model:		XLMRobertaForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- PASSED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.688892</td>
      <td>0.677581</td>
      <td>0.550000</td>
      <td>00:07</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- PASSED: Learner.fit() ---
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Sure this was a remake of a 70's film, but it had the suspense and action of a current film, say Breakdown. He's running, desperate to be with his hospitalized wife, the police are the least concern. The chases were very good, the part with him being&lt;br /&gt;&lt;br /&gt;cornered at a rest stop was well done, the end of the movie was a great cliffhanger. This is better than Bullitt, a boring movie with what, a muscle car chase that was filmed badly? Vigo's character</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>=== xlnet-base-cased ===

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: [&#39;lm_loss.weight&#39;, &#39;lm_loss.bias&#39;]
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: [&#39;sequence_summary.summary.weight&#39;, &#39;sequence_summary.summary.bias&#39;, &#39;logits_proj.weight&#39;, &#39;logits_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>architecture:	xlnet
tokenizer:	XLNetTokenizer
model:		XLNetForSequenceClassification

*** TESTING DataLoaders ***
--- PASSED: Batch inputs/targets ---

*** TESTING One pass through the model ***
--- FAILED: Predictions ---

*** TESTING Training/Results ***
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.000000</td>
      <td>00:01</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--- FAILED: Learner.fit() ---

</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example:-Multi-label-classification">Example: Multi-label classification<a class="anchor-link" href="#Example:-Multi-label-classification"> </a></h2><p>Below demonstrates how to setup your <code>blurr</code> pipeline for a multi-label classification task</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># creates a dataset with the first 10% of training set</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;civil_comments&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train[:2%]&#39;</span><span class="p">)</span> 
<span class="nb">len</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Using custom data configuration default
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>36097</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">toxic_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">toxic_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>toxicity</th>
      <th>severe_toxicity</th>
      <th>obscene</th>
      <th>threat</th>
      <th>insult</th>
      <th>identity_attack</th>
      <th>sexual_explicit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>haha you guys are a bunch of losers.</td>
      <td>0.893617</td>
      <td>0.021277</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.87234</td>
      <td>0.021277</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lbl_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">toxic_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:]);</span> <span class="n">lbl_cols</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;severe_toxicity&#39;,
 &#39;obscene&#39;,
 &#39;threat&#39;,
 &#39;insult&#39;,
 &#39;identity_attack&#39;,
 &#39;sexual_explicit&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">toxic_df</span> <span class="o">=</span> <span class="n">toxic_df</span><span class="o">.</span><span class="n">round</span><span class="p">({</span><span class="n">col</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">lbl_cols</span><span class="p">})</span>
<span class="n">toxic_df</span> <span class="o">=</span> <span class="n">toxic_df</span><span class="o">.</span><span class="n">convert_dtypes</span><span class="p">()</span>

<span class="n">toxic_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>toxicity</th>
      <th>severe_toxicity</th>
      <th>obscene</th>
      <th>threat</th>
      <th>insult</th>
      <th>identity_attack</th>
      <th>sexual_explicit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>
      <td>0.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>
      <td>0.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>
      <td>0.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>
      <td>0.000000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>haha you guys are a bunch of losers.</td>
      <td>0.893617</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">toxic_df</span><span class="p">[</span><span class="n">toxic_df</span><span class="o">.</span><span class="n">identity_attack</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>toxicity</th>
      <th>severe_toxicity</th>
      <th>obscene</th>
      <th>threat</th>
      <th>insult</th>
      <th>identity_attack</th>
      <th>sexual_explicit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>31</th>
      <td>Yet call out all Muslims for the acts of a few will get you pilloried.   So why is it okay to smear an entire religion over these few idiots?  Or is this because it's okay to bash Christian sects?</td>
      <td>0.912500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>534</th>
      <td>So tired of all these Portlanders moving to California and ruining the culture.  When will it stop?!?</td>
      <td>0.700000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>956</th>
      <td>NIGGER</td>
      <td>0.878788</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2263</th>
      <td>Residence in our local county hoosegow will be quite educational for Bundy and his troops.  There, they will meet lots of interesting people.  We got black gangstas, and brown gangstas, methheads and assorted other felons.</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2473</th>
      <td>It's real simple ...if you want wage rates to rise ....get rid of all of the illegal aliens currently keeping then artificially low</td>
      <td>0.700000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2520</th>
      <td>No more niqqers nips spics or Jews</td>
      <td>0.785714</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2974</th>
      <td>Every government is prone to tyranny unless those who are governed are diligently watchful over those who are given great power and are supposed to be serving them. This law subverts our ability to be watchful.

Conflating the civil disobedience of a group who has to date never hurt or killed anyone with radical Muslim extremists is nonsense, and most certainly not the same thing. Would you have withheld Officer Darren Wilson's name from the violent and misinformed rioters in Ferguson? Quite frankly, he was in far more danger. The principles involved are of such importance that releasing t...</td>
      <td>0.700000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4171</th>
      <td>The point is: Imago Dei has doctrines that are repugnant to me and other Portlanders.  We hear the all-male "Elders" arguing for 1. female submission 2. unmarried households are living in sin 3. gay sex is sin.

This is not a flawed doctrine- it is a homophobic misogynist doctrine that your evangelical church wants to spread.  You are no different from a Santorum supporter except that the Santorum supporter is more honest and will not deny their church's doctrine.</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4969</th>
      <td>The comment about enslaving white people was rhetorical and affirmative action is not discrimination but reparation--payment for a debt. 
The white power structure consists of a bunch of super rich deadbeat racist thieves and murderers. Save your pop psychology billhooper, the dispossessed  know how to make themselves "feel better."</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6342</th>
      <td>The Black one isn't a scientist, unlike her White counterparts.  #Hollywoodsowhite</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6736</th>
      <td>Progressives who support the free  exercise of Muslim religious beliefs, including mutilation, and death for homosexuals don't get a free pass.  

These are some more Muslim religious values:    "Criticizing or denying Muhammad is a prophet is punishable by death.  A man can marry an infant girl and consummate the marriage when she is 9 years old.  Testimonies of four male witnesses are required to prove rape against a woman,  A woman who has been raped cannot testify in court against her rapist(s)."

Democrats can't have it both ways.  Some religious values are too repugnant to be protect...</td>
      <td>0.500000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7377</th>
      <td>Imago Dei &amp; Pastor McKinley believe that gay sex is sinful and that gays do not deserve the protections that come with legal marriage.  That is treating them as second class citizens.  My friend Jason's love for his husband means nothing to Pastor McKinley, yet Jason's love for his husband is very much part of who Jason is.

Please source Pastor McKinley saying something in support of equality between a gay couple and a hetero married couple.

It has to be specific to homosexuality- I am not gonna accept some punt from the New Testament about loving everyone. I want McKinley saying that ga...</td>
      <td>0.500000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7516</th>
      <td>How about back when black/hispanics used to live on MLK Blvd. instead of California transplants and yuppie hipsters alongside their organic food grocery chains and microbreweries?

I don't see this gentrification stuff going away.  Pretty soon, all the young white people of the next generation will be making similar claims about the good old days when Portland was "weird" instead of the semi-Oakland/semi-SF situation we'll have in 10-20 years.

People in Portland are so clueless sometimes to the things that don't happen to them.</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8037</th>
      <td>So basically some crazy White guy can just make shit up out of thin air and the DMV and courts just go right along with it. Awesome!

#whitepriviligematters</td>
      <td>0.771429</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8272</th>
      <td>How do you know “torture results in little more than false confessions”? 

Kerry reply on ex-gitmo prisoner returning to jihad …. “Well, Senator, he’s not supposed to be doing that.” http://drrichswier.com/2016/02/28/video-kerry-on-gitmo-prisoner-who-returned-to-jihad-hes-not-supposed-to-be-doing-that/

Why do you defend these unconscionable jihadists who hate America, women and homosexuals?</td>
      <td>0.800000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8619</th>
      <td>If black people cared about black lives. Then they would spend their time and energy getting black people to stop killing each other.</td>
      <td>0.613333</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9188</th>
      <td>how shameful, a jew practising eugenics</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9210</th>
      <td>hire someone who is NOT a cis-het white man, even if that person is the most qualified (because he’s had the most opportunities)

That is Straight up racist and sexist.</td>
      <td>0.587500</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9226</th>
      <td>What does he being Jewish have to do with this???
Nice way to pigeon hole someone,and to show ignorant anti-semiticism.
Maybe he isn't that religious, or religious at all.
I know Catholics who have gotten divorces, gotten abortions,
were gay- do you see how ignorantly bigoted what you said is. 
And it doesn't send any bad message to the girls. He said he loves his daughters. He wanted a son, what's wrong with that. 
The only real issue in this article is the custody battle.
Other than that, the rest i.e being rich, being Jewish, wanting a son only,etc. is totally irrelevant. So many need t...</td>
      <td>0.700000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10299</th>
      <td>Those aren't white people. Its only cool to hate on white people</td>
      <td>0.850000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10329</th>
      <td>Damn Christians!</td>
      <td>0.982435</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11100</th>
      <td>This reminds me much of the racists who didn't want "negro's" in Oregon. It's all about crony capitalism today. The stated reasons why people are against this is as bigoted as anything racists state. Some think it's a fool who buys bottled water while others hate because it's a corporation. Nestle needs to market this as bottled organic bong water.</td>
      <td>0.700000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11656</th>
      <td>Only a left wing moron could write such a ridiculous headline.  How about " Why do virtually all terrorists turn out to be MUSLIMS?"</td>
      <td>0.830769</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12746</th>
      <td>Oh Chuckles, living with all that white guilt must be miserable.</td>
      <td>0.725000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12762</th>
      <td>Comedy; apparently women are so stupid that unless they go to a religious funded clinic they will never attain any of the free contraception offered by countless liberal clinics throughout every state.</td>
      <td>0.789474</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12803</th>
      <td>Right.  Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes.  No enchiladas for them!</td>
      <td>0.757143</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13259</th>
      <td>04/24/11--19 year-old Ashley Bashore ran down Hubert Tunuchuk and fled, leaving him to die.  She was charged with "failing to render assistance" (!) and sentenced to 18 months.  Do blondes have more fun?  They apparently get away with vehicular manslaughter of Yup'ik people!

02/06/13--17 year-old Murphy Gross got high, stole daddy's SUV and T-boned another car, killing young mother of two Catherine Cope.  He got 8 months--the Judge felt he could be 'rehabilitated'.  Translation, he was white and Cope wasn't.

07/19/14--17 year-old drunk driver Alexandra Ellis ran down 51 year-old bicyclis...</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14354</th>
      <td>Lucky for the way this turned out and thanks is differently in order for Springfield PD. The homosexual lifestyle is more often a violent lifestyle largely hidden from public view by the media and by LGBTQ agenda. I read some where in past there are nearly 3 times as many domestic abusive help centers for homosexuals in San Francisco and L.A. then heterosexual domestic abusive help centers.</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14670</th>
      <td>Bunk.  Obama got elected by black privilege and George Soros.</td>
      <td>0.600000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14761</th>
      <td>Black privilege? Sure, according to you. 

Obama earned the presidency unlike your golden calf idol, current GOP ringmaster, and admitted (women are dogs) liar.</td>
      <td>0.840000</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">task</span> <span class="o">=</span> <span class="n">HF_TASKS_AUTO</span><span class="o">.</span><span class="n">SequenceClassification</span>

<span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;roberta-base&quot;</span> <span class="c1"># &quot;distilbert-base-uncased&quot; &quot;bert-base-uncased&quot;</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lbl_cols</span><span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">BLURR_MODEL_HELPER</span><span class="o">.</span><span class="n">get_hf_objects</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> 
                                                                               <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> 
                                                                               <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;]
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note how we have to configure the <code>num_labels</code> to the number of labels we are predicting. Given that our labels are already encoded, we use a <code>MultiCategoryBlock</code> with <code>encoded=True</code> and <code>vocab</code> equal to the columns with our 1's and 0's.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># single input</span>
<span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">HF_TextBlock</span><span class="p">(</span><span class="n">hf_arch</span><span class="o">=</span><span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="o">=</span><span class="n">hf_tokenizer</span><span class="p">),</span> 
    <span class="n">MultiCategoryBlock</span><span class="p">(</span><span class="n">encoded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">lbl_cols</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> 
                   <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="n">lbl_cols</span><span class="p">),</span> 
                   <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">toxic_df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(2, torch.Size([8, 512]), torch.Size([8, 6]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>None</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The last I heard this legislation ONLY applies to businesses, not individuals.</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>Jay Hammond voted against Statehood. Why Wonder Today. His Diapering the Devil, writ, on display in another ghetto group photo shoot.\n\nJuneau; where the Bong Hits Jesus case, explains it all!</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HF_BaseModelWrapper</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> 
                <span class="n">model</span><span class="p">,</span>
                <span class="n">opt_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="n">decouple_wd</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="n">loss_func</span><span class="o">=</span><span class="n">BCEWithLogitsLossFlat</span><span class="p">(),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">partial</span><span class="p">(</span><span class="n">accuracy_multi</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)],</span>
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">HF_BaseModelCallback</span><span class="p">],</span>
                <span class="n">splitter</span><span class="o">=</span><span class="n">hf_splitter</span><span class="p">)</span>

<span class="n">learn</span><span class="o">.</span><span class="n">loss_func</span><span class="o">.</span><span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">learn</span><span class="o">.</span><span class="n">create_opt</span><span class="p">()</span>             <span class="c1"># -&gt; will create your layer groups based on your &quot;splitter&quot; function</span>
<span class="n">learn</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we're doing multi-label classification, we adjust our loss function to use binary cross-entropy and our metrics to use the multi-label friendly version of accuracy.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 6])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#slow</span>
<span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">suggestions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.0020892962347716093)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#slow</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr_max</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy_multi</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.035510</td>
      <td>0.032239</td>
      <td>0.992452</td>
      <td>10:47</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.043954</td>
      <td>0.028548</td>
      <td>0.989289</td>
      <td>10:50</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.037740</td>
      <td>0.027281</td>
      <td>0.992175</td>
      <td>10:50</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">show_results</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>None</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>He did not call all Mexicans rapists.   He did say, and it has been proven that some of the illegal aliens are indeed rapists and murders.</td>
      <td></td>
      <td>[]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I feel sorry for the other two Joseph Spindles in the area.</td>
      <td></td>
      <td>[]</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">loss_func</span><span class="o">.</span><span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.02</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">comment</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. </span>
<span class="s2">No enchiladas for them!</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">learn</span><span class="o">.</span><span class="n">blurr_predict</span><span class="p">(</span><span class="n">comment</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((#1) [&#39;insult&#39;],
 tensor([False, False, False,  True, False, False]),
 tensor([4.3196e-07, 6.4220e-03, 9.4548e-04, 6.2519e-02, 1.2812e-02, 9.2042e-04]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cleanup">Cleanup<a class="anchor-link" href="#Cleanup"> </a></h2>
</div>
</div>
</div>
</div>
 

