{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "from seqeval import metrics as seq_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_batch_tfm = HF_TokenClassBatchTransform(hf_arch, hf_tokenizer)\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, hf_batch_tfm=hf_batch_tfm, max_length=128, is_pretokenized=True,\n",
    "                 tok_kwargs={ 'return_special_tokens_mask': True }), \n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('In', 'O'), ('den', 'O'), ('Jahren', 'O'), ('1991', 'O'), ('bis', 'O'), ('1996', 'O'), ('bekleidete', 'O'), ('er', 'O'), ('die', 'O'), ('Funktion', 'O'), ('des', 'O'), ('Direktors', 'O'), ('des', 'O'), ('International', 'B-ORG'), ('Film', 'I-ORG'), ('Festival', 'I-ORG'), ('Rotterdam', 'I-ORG'), ('und', 'O'), ('des', 'O'), ('Hubert', 'B-ORG'), ('Bals', 'I-ORG'), ('Fonds', 'I-ORG'), (',', 'O'), ('mit', 'O'), ('dem', 'O'), ('Filmproduktionen', 'O'), ('in', 'O'), ('Entwicklungsländern', 'O'), ('gefördert', 'O'), ('werden', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Die', 'O'), ('nächste', 'O'), ('AchemAsia', 'B-ORG'), ('eröffnet', 'O'), ('im', 'O'), ('Frühjahr', 'O'), ('2013', 'O'), ('in', 'O'), ('Beijing', 'B-LOC'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if (metric_key == 'accuracy'): return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'precision'): return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'recall'): return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'f1'): return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "        \n",
    "    if (metric_key == 'classification_report'): return seq_metrics.classification_report(targ_toks, pred_toks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenClassCallback(HF_BaseModelCallback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self=self, names='tok_metrics, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in tok_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the target's HF_TokenizerTransform (used for rouge metrics)\n",
    "        hf_textblock_tfm = self.dls.tfms[0]\n",
    "        self.hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = self.dls.tfms[1].ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = hf_textblock_tfm.kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "    def before_fit(self): self.setup()\n",
    "    \n",
    "    \n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        if (self.training or self.learn.y is None): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self): self.results = []\n",
    "        \n",
    "    def after_validate(self):\n",
    "        if (len(self.results) < 1): return\n",
    "        \n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys(): \n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "        \n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, \n",
    "                                                                                   preds, \n",
    "                                                                                   'classification_report')\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f'Couldn\\'t calcualte classification report: {err}')\n",
    "        \n",
    "        \n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                cbs=[HF_TokenClassCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([2, 40, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([2, 40]), 2, torch.Size([2, 40]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 18]) torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0007585775572806596, lr_steep=3.0199516913853586e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV9fn/8deVHZIQIAl7hiGEDWEIqGgdoFXcSh1VUWod1Vb9tXY52n5rrbWtq0odqFVwgVuxWlQ2hCVb9hLIIHuTXL8/zo3GcLIgd+6Tk+v5eJwH59zj3O/wUK7c92eJqmKMMcZUF+J1AGOMMYHJCoQxxhi/rEAYY4zxywqEMcYYv6xAGGOM8csKhDHGGL9cKxAiEiUiy0VkrYhsEJEH/BzzCxHZKCJfichnItKjyr4KEVnjvN51K6cxxhj/xK1xECIiQIyqFohIOLAQuENVl1Y55nRgmaoWichPgYmqeoWzr0BVYxtyzcTERO3Zs2fj/RDGGBPkVq5cmamqSf72hbl1UfVVngLnY7jz0mrHzK/ycSlw9Ylcs2fPnqSlpZ3IVxhjTIsiIrtr2udqG4SIhIrIGiAd+K+qLqvl8GnAR1U+R4lImogsFZEL3cxpjDHmWK7dQQCoagUwTETaAHNFZJCqrq9+nIhcDaQCp1XZ3ENV94tIMvA/EVmnqtv9nDsdmA7QvXt3V34OY4xpiZqkF5Oq5gDzgUnV94nImcBvgAtUtbTKOfudP3cAnwPDa/juGaqaqqqpSUl+H6MZY4w5Dm72Ykpy7hwQkWjgLGBztWOGA8/gKw7pVba3FZFI530iMB7Y6FZWY4wxx3LzEVMn4EURCcVXiF5X1fdF5EEgTVXfBf4KxAJv+Do9sUdVLwAGAM+ISKVz7kOqagXCGGOakJu9mL7Cz2MhVf19lfdn1nDuYmCwW9mMMcbUrcWPpD5SUcnibZlsOZjvdRRjjAkoLb5AKHDjS2n8Z2mNXYGNMaZFavEFIjw0hFE927F0R5bXUYwxJqC0+AIBMDY5ga3pBWTkl9Z9sDHGtBBWIICTeycA2F2EMcZUYQUCGNS5NbGRYVYgjDGmCisQQFhoCKN7tWOJFQhjjPmWFQjH2OR27Mgo5FBeiddRjDEmIFiBcJycnAhYO4QxxhxlBcKR0rk1raOsHcIYY46yAuEIDRFG90pgyXYrEMYYA1Ygvmdscjt2ZRVxILe4ya+dkV/Kil2HySspb/JrG2OMP64uGNTcVB0PcdHwrq5eq7JSmbl4F/O3pLPpQD6ZBb5BeuGhwvg+iUwa2JEzUzqQGBvpag5jjKmJFYgqBnRsTZtW4SzZ7m6BKCmv4K7X1/LBugP07xjHaf2SGNApjh4JMazYdZiP1x/kV3PWEfb2es4f2pmbTkkmpXNr1/IYY4w/ViCqCAkRxrg8HiIjv5SbXkpj7b4cfnPuAG48pRfOWhgAnJXSgXsn92fTgXzeXLmP2Sv2MHf1fk7pm8jlqd3o1yGOHgmtiAoPdS2jMcYAiKq688UiUcCXQCS+QvSmqt5X7ZhI4CVgJJAFXKGqu5x99wLTgArgZ6o6r65rpqamalpa2gnlfmHRTh54byM/OS2Z8iNKfkk5eSXlHC4sI6uwjOzCMtrHRXHtuB5cPLwr0RH1+4daVUnbnc2ds9dwuLCMf1w5jHMGdqzzvNyicl5ZvpuZi3aR7swVJQKd46PpGB9F21YRtG0VTtuYCFpHhdE6Opy4qDDatIqgb/tYurSJ/l4BMsaYqkRkpaqm+t3nYoEQIEZVC0QkHFgI3KGqS6sccwswRFVvFpErgYtU9QoRSQFmAaOBzsCnQD9Vrajtmo1RIHZlFnL2P76kvKKS2IgwYqPCiIsKo11MBO1iImjbKoK1+3JYvz+Ptq3C+dGY7qR0iqekvIKSIxWUH6mkXWwkneKj6Ng6irKKSt5b+w1vr97Prqwi2sdF8tyPRzG4a3yDcpUdqWTLwXx2ZhWyM6OQnZkFpOeXcriwjJyicrKLyig9UnnMeXGRYfTrGPdtsejsvPp1iCXB2jeMafFqKxBuriinQIHzMdx5Va9GU4D7nfdvAk84hWUKMFtVS4GdIrINX7FY4lbeo3omxrD+/nMICxFCQvz/5q2qLN95mOcW7uSpz7dTV40VgZOTE7jl9D5MHtSRuKjwBueKCAthcNf4WgtL6ZEK8kuOkF9yhMyCUrYczP/29emmQ2QWlH3v+D7tYxnVsx1jerXjjAHtaX0cuYwxwcvVNghnPeqVQB/gSVVdVu2QLsBeAFU9IiK5QIKzfWmV4/Y525pERFjtvX9FhDHJCYxJTuBgbgk5xWVEhYUSFR5KWKhwuLCMg7klHMwtobSikjMHtKdTfLTruSPDQomMDSUxNpJeiTGM6tnue/tLyis4mFvCvuxi1u3PZfnOLN5f+w2zlu8hOjyUC4d34eqx3RnYuWF3N8aY4ORqgXAeCQ0TkTbAXBEZpKrrG/MaIjIdmA7QvXv3xvzqeukYH0XH+KjvbUuMjaRfh7gmz1KXqPBQeibG0DMxhgl9E/npxN5UVCpr9+Uwe/ke5q7ex6zlexjWrQ1npXTg1L5JDOzcusY7KWNMcHOtDeKYC4n8HihS1UeqbJsH3K+qS0QkDDgIJAG/AlDVP1c/rrZrNEYbREuWW1TOm6v28dbKfWw8kAdAu5gIzhnYgZ+f1Y/2cVF1fIMxprnxqpE6CShX1RwRiQY+Af6iqu9XOeZWYHCVRuqLVfVyERkIvMp3jdSfAX2bopHa+KTnl7BoWyZfbMngw3UHiQwL4Rdn9+OasT0IC7UB+MYEC68KxBDgRSAU35Qer6vqgyLyIJCmqu86XWFfBoYDh4ErVXWHc/5vgBuAI8CdqvpRXde0AuGOnZmF/P6d9SzYmklKp9b85ZIhDe6FZYwJTJ4UCC9YgXCPqvLR+oM8+N5GcorLeOaaVE7rl+R1LGPMCaqtQNizAlMvIsK5gzvx3u0TSE6M5cYXV/DBVwe8jmWMcZEVCNMgSXGRzJo+lmHd2nD7rFXMXr7H60jGGJdYgTANFh8dzks3jOGUvkn8as46Hpm3hYrK4HlUaYzxsQJhjkt0RCj/vjaVy1O78sT8bVz7/LJvpyw3xgQHKxDmuEWEhfDwpUN5+JIhpO3K5tx/LmD5zsNexzLGNBIrEOaEXT6qG3NvGU+riFCm/nspb67c53UkY0wjsAJhGkVK59a8d/sExia34+431vL8wp1eRzLGnCArEKbRxEWF8/x1ozhnYAcefH8jf//v1wTTOBtjWhorEKZRRYaF8uSPRnDpyK7887OtPPDeRiqth5MxzZItOWoaXVhoCA9fMoQ20eE8u3AnecXlPHzpEJvDyZhmxgqEcUVIiPCb8wYQHx3O3/77NfmlR3h86nBbS9uYZsR+pTOuERFu/0FfHrhgIP/deIjrX1hBQekRr2MZY+rJCoRx3Y/H9eTRy4eyfNdhfvJymrVJGNNMWIEwTeLiEV3504WDWLQti38v2OF1HGNMPViBME3milHdmDyoI498soX1+3O9jmOMqYMVCNNkRIQ/XzyYhJhIfjZ7NcVltS4QaIzxmGsFQkS6ich8EdkoIhtE5A4/x9wjImuc13oRqRCRds6+XSKyztlnqwAFiTatInj08qHszCzkTx9u9DqOMaYWbt5BHAHuUtUUYCxwq4ikVD1AVf+qqsNUdRhwL/CFqlad7e10Z7/f1Y5M8zSuTyI3nZLMf5bu4eP1tuiQMYHKtQKhqgdUdZXzPh/YBHSp5ZSpwCy38pjActfZ/RjarQ13vraGNXtzvI5jjPGjSdogRKQnMBxYVsP+VsAk4K0qmxX4RERWisj0Wr57uoikiUhaRkZG44U2rooMC+XZa1NJiotk2swV7Mkq8jqSMaYa1wuEiMTi+4f/TlXNq+Gw84FF1R4vTVDVEcBkfI+nTvV3oqrOUNVUVU1NSkpq1OzGXUlxkcy8fjQVqlz3wnKyC8u8jmSMqcLVAiEi4fiKwyuqOqeWQ6+k2uMlVd3v/JkOzAVGu5XTeKd3Uiz/vjaVfTnF3PRSGiXl1rPJmEDhZi8mAZ4DNqnqo7UcFw+cBrxTZVuMiMQdfQ+cDax3K6vx1qie7fj75cNI253Ng+9bzyZjAoWbk/WNB64B1onIGmfbr4HuAKr6tLPtIuATVS2scm4HYK6vxhAGvKqqH7uY1XjsvCGdWLe/N09/sZ1RPdty0fCuXkcypsVzrUCo6kJA6nHcTGBmtW07gKGuBDMB6+6z+7FqTza/nrOegZ3j6dchzutIxrRoNpLaBIyw0BCemDqcmMgwbnllFYU286sxnrICYQJK+9ZRPDZ1GDsyCvj13HVexzGmRbMCYQLOuN6J3HlmP95Z8w0frz/odRxjWiwrECYg/XRib/p3jOP+dzeQX1LudRxjWiQrECYghYeG8NAlQziUX8Ij87Z4HceYgLUvu4hDeSWufLcVCBOwhnVrw7Vje/DS0t2s3pPtdRxjAtKT87dz5t++oMKFlRqtQJiAdvc5J9EhLop756yjvKLS6zjGBJwl2zMZk9yO0JA6RxU0mBUIE9DiosK5/4KBbD6Yz/MLd3odx5iA8k1OMbuyiji5d6Ir328FwgS8SYM6cvpJSTz1+XYbG2FMFUu2ZwEwrneCK99vBcI0C7f/oC+5xeW8tmKv11GMCRiLt2fRtlU4J7k064AVCNMsjOjellE92/Lcwp3WFmEMoKos2Z7Jyb0TCHGh/QGsQJhm5Cen9mZ/TjEfrrNlSo3ZnVXEN7klrrU/gBUI04yc0b89fdrH8vQXO1Bt/C59xjQni11ufwArEKYZCQkRpp+SzKYDeSzclul1HGM8tWRHFh1aR5KcGOPaNaxAmGZlyvDOtI+L5JkvdngdxRjPfNv+kJyAs26OK9xcUa6biMwXkY0iskFE7vBzzEQRyRWRNc7r91X2TRKRLSKyTUR+5VZO07xEhoVyw4ReLNyWyfr9uV7HMcYTW9MLyCwoY5yL7Q/g7h3EEeAuVU0BxgK3ikiKn+MWqOow5/UggIiEAk8Ck4EUYGoN55oW6EdjuhMXFcZfPt5sbRGmRVrsPGI92cX2B3CxQKjqAVVd5bzPBzYBXep5+mhgm6ruUNUyYDYwxZ2kprlpHRXOL87qx4KtmXxk04GbFmjx9iy6tYumW7tWrl6nSdogRKQnMBxY5mf3ySKyVkQ+EpGBzrYuQNURUfuof3ExLcA1Y3uQ0qk1D763kQIbXW1akIpKZemOLMYlu/t4CZqgQIhILPAWcKeq5lXbvQrooapDgceBt4/j+6eLSJqIpGVkZJx4YNMshIWG8IcLB3Ewr4THPtvqdRxjmsymA3nklRxx/fESuFwgRCQcX3F4RVXnVN+vqnmqWuC8/xAIF5FEYD/QrcqhXZ1tx1DVGaqaqqqpSUlJjf4zmMA1skdbrkjtxvMLd/L1oXyv4xjTJDZ84+ucMaJ7W9ev5WYvJgGeAzap6qM1HNPROQ4RGe3kyQJWAH1FpJeIRABXAu+6ldU0X7+c3J/YqDB++/Z6a7A2LcKhvFIAOsRHun4tN+8gxgPXAGdU6cZ6rojcLCI3O8dcCqwXkbXAY8CV6nMEuA2Yh69x+3VV3eBiVtNMtYuJ4JeT+rN852HmbTjkdRxjXHcor4S2rcKJDAt1/Vphbn2xqi4Eah3BoapPAE/UsO9D4EMXopkgc9nIrjz1+Tae+XI75wzs4OrAIWO8lp5fSofWUU1yLRtJbZq9sNAQbpyQzOo9OaTttqVJTXBLzyuhvRUIY+rvstSutGkVblNwmKB3KK+U9nHutz+AFQgTJFpFhHHt2B58uukQ29ILvI5jjCsqKpWMglI6tLYCYUyDXDuuJ5FhITy7wO4iTHDKKiylolKtDcKYhkqMjeSSkV2Zs2o/6fklXscxptGlO11c28dZgTCmwW46JZnyykpeXLzL6yjGNLqjv/i0t0dMxjRcr8QYzk7pwMtLdlNoczSZIPPtIDl7xGTM8Zl+am/ySo7wRtreug82phk5lOe7g0iKtTsIY47LyB5tGdmjLc8t2smRikqv4xjTaNLzS0mIiSAirGn+6bYCYYLSTacks/dwsU2/YYJKUw6SAysQJkidldKBHgmtmLFgh03iZ4JGUw6SAysQJkiFhgg3TujF2r02/YYJHofySppskBxYgTBB7NKR3WjbKpwZX9rAOdP8VVQqmQVNN1EfWIEwQSw6IpSrnek3dmTY9BumecsqKKVSsTYIYxrLtSf3JDwkhH/b9BummTv07SjqIHjEJCLdRGS+iGwUkQ0icoefY64Ska9EZJ2ILBaRoVX27XK2rxGRNLdymuCWFBfJ1NHdmL1iLyt3H/Y6jjHH7egYiGB5xHQEuEtVU4CxwK0iklLtmJ3Aaao6GPgDMKPa/tNVdZiqprqY0wS5eyb1p3N8NPe88RUl5RVexzHmuKTnHx1FHQR3EKp6QFVXOe/z8S0d2qXaMYtV9WgXk6VAV7fymJYrNjKMhy8dwo7MQh6Zt8XrOMYcl0N5JYj4JqVsKvUqECISIyIhzvt+InKBiITX9yIi0hMYDiyr5bBpwEdVPivwiYisFJHp9b2WMf6M75PINWN78NyinazYZY+aTPOTnl9CQkwE4aFN13Rc3yt9CUSJSBfgE+AaYGZ9ThSRWOAt4E5VzavhmNPxFYhfVtk8QVVHAJPxPZ46tYZzp4tImoikZWRk1PPHMS3Rryb3p2vbaO55Yy3FZfaoyTQvvkFyTdf+APUvEKKqRcDFwFOqehkwsM6TfHcZbwGvqOqcGo4ZAjwLTFHVrKPbVXW/82c6MBcY7e98VZ2hqqmqmpqUlFTPH8e0RDGRYTx8yVB2ZRVx66uryC8p9zqSMfXW1IPkoAEFQkROBq4CPnC2hdZ1AvAcsElVH63hmO7AHOAaVf26yvYYEYk7+h44G1hfz6zG1Ojk3gn8YcpAvvg6g4ueWszOzEKvIxlTL+n5TTtIDupfIO4E7gXmquoGEUkG5tdxznh8j6LOcLqqrhGRc0XkZhG52Tnm90AC8FS17qwdgIUishZYDnygqh835AczpibXnNyTl28YTVZBKVOeWMjnW9K9jmRMrY5UVJJZ0LTzMIHv0VHDTvA1VsfW1J7gpdTUVE1LsyETpn72Hi7ippfS+PpQPi9cP5rT+tkjShOYDuaWMPbPn/HHCwdx9dgejfrdIrKypqEE9e3F9KqItHYe96wHNorIPY0Z0pim1q1dK+bcMo7kpFh+M3edNVybgOXFIDmo/yOmFOeO4UJ8XVF74Xt8ZEyz1ioijD9dOIh92cX887OtXscxxi8vBslB/QtEuNMj6ULgXVUtxzdOwZhmb0xyApenduXZBTvYfDDgnpwaE/B3EM8Au4AY4EsR6QHY/0kmaNw7eQCto8O5d846Kivtdx8TWNKdUdQJMRFNet16FQhVfUxVu6jqueqzGzjd5WzGNJm2MRH85twBrN6Tw6vL93gdx5jvOZRXSmJsJGFNOIoa6t9IHS8ijx4dsSwif8N3N2FM0Lh4RBdOTk7gLx9vJt25pTcmEKTnN/0gOaj/I6bngXzgcueVB7zgVihjvCAi/OmiQZQeqeSB9zd6HceYbx3KK6VDE0+zAfUvEL1V9T5V3eG8HgCS3QxmjBeSk2K57fQ+fPDVAeZvtgF0JjCk55fQPoDvIIpFZMLRDyIyHih2J5Ix3vrJacn0Torht2+vp6jsiNdxTAtXXlFJZkFZk0/UB/UvEDcDTzqrvO0CngB+4loqYzwUGRbKny8ewv6cYv7xqY2NMN7KLHCWGg3UOwhVXauqQ4EhwBBVHQ6c4WoyYzw0ulc7rhzVjecW7mTDN7lexzEt2OHCMgDatWraLq7QwBXlVDWvyhxMv3AhjzEB497JA2jbKtyWKjWeyinyTUvfJtALRDXSaCmMCUDxrcJ56OIhbDyQx/3vbvA6jmmhsot8dxBtY+q9iGejOZECYcNNTdA7M6UDt57em9kr9vLaChtAZ5petnMH0daDO4iw2naKSD7+C4EA0a4kMibA/OKsk1i7N5ffvbOBlE7xDO4a73Uk04LkOG0QbVoF2B2Eqsapams/rzhVrau4dBOR+SKyUUQ2iMgdfo4REXlMRLaJyFciMqLKvh+LyFbn9ePj/xGNOTGhIcI/rxxGYkwEN/9nJdnO/7DGNIXsonJiIkKJDKt1EU9XuDmxxxHgLlVNAcYCt4pISrVjJgN9ndd04F8AItIOuA8Yg28t6vtEpK2LWY2pVUJsJE9dPZKM/FIu+ddiVu7O9jqSaSFyiso8aaAGFwuEqh5Q1VXO+3xgE9Cl2mFTgJecCQCXAm1EpBNwDvBfVT2sqtnAf4FJbmU1pj6GdWvDzBtGUXqkksueXsyfP9pkvZuM67KLyjxpoAZ37yC+JSI9geHAsmq7ugB7q3ze52yrabsxnhrXO5GP7zyFy1O78cwXOzj/8YXsPVzkdSwTxLKLyj1poIYmKBAiEgu8BdzpxjrWIjL96CyzGRkZjf31xhwjLiqchy4ZwszrR3Ewr4SfvrLS7iSMa4LyEROAswrdW8ArqjrHzyH7gW5VPnd1ttW0/RiqOkNVU1U1NSnJFp03TWfiSe3522VDWb8/jz9+YLO/Gnf47iCC7BGTiAjwHLBJVR+t4bB3gWud3kxjgVxVPQDMA84WkbZO4/TZzjZjAsrZAzsy/dRk/rN0D++s8fs7jDHHraJSySsp9+wOotauqidoPHANsE5E1jjbfg10B1DVp4EPgXOBbUARcL2z77CI/AFY4Zz3oKoedjGrMcftnnNOYvWebO6ds46BnePp0z7W60gmSOQWl6OKZ3cQrhUIVV1IHdNxqKoCt9aw73l8CxUZE9DCQ0N4fOoIzntsAbe8spJ3b5tAVHjT91k3wefbaTaCsQ3CmJaiY3wUf79iGF8fKuChjzZ7HccEiZwi70ZRgxUIYxrNqf2SuG5cT2Yu3sXCrZlexzFBILvQu3mYwAqEMY3ql5P6k5wUwz1vriW3uNzrOKaZs0dMxgSR6IhQ/n75MNLzS22KcHPCjhaINsE8ktqYlmRotzbcfkYf5q7ez4frDngdxzRj2UXlhIUIcZFudjitmRUIY1xw6+l9GNo1nt++vf7bhkZjGso3ijoc37CypmcFwhgXhIeG8NAlQ8gtLufheVu8jmOaqexC7wbJgRUIY1wzoFNrrhvXk1nL97B6j00Pbhouu6iMdlYgjAlOd57Zl/ZxkfzunfVUVNoqvaZhcorKPRsDAVYgjHFVXFQ4v/thCuv35/HKst1exzHNTHZRmWddXMEKhDGuO29wJ07pm8hf520hPb/E6zimmVBV3x2ER11cwQqEMa4TER64YCCl5ZX8du56fFOQGVO7orIKyioq7Q7CmGCXnBTL/5t0Ep9sPMRzC3d6Hcc0A9+NorY7CGOC3rQJvThnYAce+mgzK3fb7PWmdjlFvqlarJurMS2AiPDwpUPp3CaaW19ZTVZBqdeRTADzeh4mcHdFuedFJF1E1tew/x4RWeO81otIhYi0c/btEpF1zr40tzIa09Tio8N56qoRHC4q487X1ljXV1Oj7KKjM7kG5yOmmcCkmnaq6l9VdZiqDgPuBb6otmrc6c7+VBczGtPkBnWJ54ELBrJgayZn/O1znvliO4cLbToO833frQURhHcQqvolUN8HrVOBWW5lMSbQXDmqG/+6agQdWkfx5482M/b/PuOeN9ZSUl7hdTQTII6uBeHlQDlvpgisQkRa4bvTuK3KZgU+EREFnlHVGZ6EM8YlIsLkwZ2YPLgTXx/K5z9Ld/PSkt1EhIXwp4sGex3PBIDsojLiIsMID/WuqdjzAgGcDyyq9nhpgqruF5H2wH9FZLNzR3IMEZkOTAfo3r27+2mNaWT9OsTx4JRBREeE8swXOxiTnMAFQzt7Hct4LKeozNNBchAYvZiupNrjJVXd7/yZDswFRtd0sqrOUNVUVU1NSkpyNagxbrr77JMY2aMt9771FTszC72OYzyWXVTuaQ8m8LhAiEg8cBrwTpVtMSISd/Q9cDbgtyeUMcEkPDSEx6cOJzwshFteWWXtES2cby2IIC0QIjILWAKcJCL7RGSaiNwsIjdXOewi4BNVrfrrUgdgoYisBZYDH6jqx27lNCaQdG4Tzd8uG8qmA3n84f2NXscxHvLdQXj7iMm1NghVnVqPY2bi6w5bddsOYKg7qYwJfD8Y0IHppyYz48sdjO+TyLmDO3kdyXjA65lcITDaIIwx1dx99kkM7daGX771FXsPF3kdxzSx8opK8kuOWIEwxhwrIiyEx68cDgp3zF5NeUWl15FMEzo6D1Nb68VkjPGne0Ir/u/iwazak8Pf//u113FMEwqEUdRgBcKYgHb+0M6+UddfbGfh1kyv45gmEgjzMIEVCGMC3n3nD6R3Uiw/f30NmTYDbIsQCDO5ghUIYwJedEQoT/xoOLnF5dz1+loqbQbYoPfdIya7gzDG1KF/x9b87ocpfPF1Bs8u3OF1HOOy7x4x2R2EMaYerh7TnUkDO/Lwx1tYuzfH6zjGRdlFZUSEhtAqItTTHFYgjGkmRIS/XDKEDq2juH3WanKLy72OZFySU1hOm1bhiIinOaxAGNOMxLcK57Gpw/gmp5jzH1/I6j3ZXkcyLgiEUdRgBcKYZmdkj3bMnj6Wikrl0qeX8MT/ttrSpUEmp6jc8wZqsAJhTLOU2rMdH95xCucO7sQjn3zNj/69lPwSe+QUDPYeLmLP4SK7gzDGHL/46HAeu3IYf710CGm7s7n7jbWo2p1Ec5VdWMYf39/ID/72BdlFZUwZ5v2iUYGwopwx5jiJCJeldiO3uJw/frCJf32xnVsm9vE6lmmgTzce4uevr6Gg9AiXjezKz8/qR6f4aK9jWYEwJhhMm9CLtftyeWTeFgZ1jufUfra6YnPy90+/Jik2kjduPpn+HVt7Hedbbi4Y9LyIpIuI39XgRGSiiOSKyBrn9fsq+yaJyBYR2SYiv3IrozHBwtcFdjD9OsTxs9mrbYrwZmTv4SI2fJPHlaO7BVRxAHfbIGYCk+o4ZoGqDnNeDwKISCjwJDAZSAGmikiKizmNCQqtIsJ45pqRVFYqN76YxsHcEq8juSKroJSfzf0tHiQAABIoSURBVFrNun25XkdpFPM2HATgnIEdPU5yLNcKhKp+CRw+jlNHA9tUdYeqlgGzgSmNGs6YINUjIYanrx7JvuwiLnxyERu/yfM6UqObs2o/7679hitnLGHxtuY/w+0nGw7Rv2McPRJivI5yDK97MZ0sImtF5CMRGehs6wLsrXLMPmebMaYexvVJ5I2bxwFw2dOLmb8l3eNEjeuDdQfonRRD17atuO6FFXy07oDXkY5bZkEpK3YfDsi7B/C2QKwCeqjqUOBx4O3j+RIRmS4iaSKSlpGR0agBjWmuUjq35u1bx9MjIYYbX0zjxcW7gqIL7P6cYtbszeHiEV157SdjGdw1nltfXcWry/Z4He24fLrxEKqB+XgJPCwQqpqnqgXO+w+BcBFJBPYD3aoc2tXZVtP3zFDVVFVNTUqynhvGHNUxPoo3bj6Z009K4r53N/Dz19ZQVHbE61gn5OjdwnmDO9GmVQT/mTaGU/sl8eu56/jVW19RXFbhccKG+XjDQbq1i2ZApzivo/jlWYEQkY7izEQlIqOdLFnACqCviPQSkQjgSuBdr3Ia05zFRIYx45pU7jqrH++s/YaLnlzMjowCr2Mdtw/WHWBg59b0TPQ9r4+OCOXZa1O59fTevJa2lylPLuTrQ/kep6yf/JJyFm/L4pyUjp5PylcTN7u5zgKWACeJyD4RmSYiN4vIzc4hlwLrRWQt8BhwpfocAW4D5gGbgNdVdYNbOY0JdiEhwu0/6MuL148mPb+EC55YxKJm2Lj7TU4xq/fkcO7gTt/bHhYawj3n9OfF60dzuLCMC55YyBtpe2v4lsaTW1TOvA0Hj/vR3fwtGZRVVDJpUGA+XgKQYHgueVRqaqqmpaV5HcOYgLU/p5hpM1ewO6uIl6eNJrVnO68j1duzC3bwxw828fndE7+9g6guPb+EO2evYfH2LK4f35PfnDuAsNDG/z24oPQIU2csZd3+XKaO7safLhxMSMj37wIqKpUQoca7g1tfXcWyHYdZ/usfHHNuUxKRlaqa6m+f172YjDFNqEubaF6eNoZO8VFc/8KKZjWW4MN1B0jp1LrG4gDQPi6Kl24YzQ3je/HCol1cP3MFuUWNO4lhSXkF019KY+OBPM4b0olZy/dy9xtrOVJRCUBlpfJG2l7G/N9n3PTSSkrKj20XKSmv4PPN6ZyV0sHT4lAXKxDGtDBJcZH858YxtI4O59rnlzWLZ/bf5BSzak8O5w3pVOexYaEh/P78FP5yyWCW7sjiwqcWsbWRfsYjFZXcMXs1i7dn8chlQ3jyRyO4++x+zFm9n5/NXs3qPdlc+vRi7nnzKxJiIvhs8yGuf2EFBaVHvvcdT83fRmFZRUA/XgIrEMa0SJ3bRPPqTWMIDw3hqmeXsflgYA+o+9DpvVS9/aE2V4zqzqs3jSWvuJwfPr6Ql5acWFdfVeW3b69n3oZD3Hd+ChcN7wrAbWf05bfnDeDDdQe56KnF7DlcxF8vHcJHd5zC3y8fxvJdh7n62WXkFpXz1b4cpjy5iMf+t41zBnZgXO+E487TFKwNwpgWbOuhfK5+bhmFpRX86+oRnNI3MLuKX/zUIorLK/nojlMafG56Xgn3vPkVX3ydwcSTknj40iG0j4tq8Pf8dd5mnpy/ndvP6MNdZ590zP45q/axLb2An5zWm/jo7xb7mbfhILe/upqE2AgO5ZWQGBvJAxcMZNKgwOi9ZG0Qxhi/+naIY+4t4+naNprrX1jB603Q+6ehissqWL03h7MGtD+u89u3jmLm9aN44IKBLNmexaR/LOCJ/20lPb/+c1W9uHgXT87fztTR3fnFWf38HnPxiK78v0n9v1ccwDcI7rnrUimvqOSqMT349K7TmDy4U0AUh7rYHYQxhvyScm55ZRULtmZy6+m9+fmZ/Vzp/XM8Nh/MY9I/FvDY1OFcMPTEFtHZeiif+9/bwKJtWYSFCOcM7MiPxnTn5OSEGhuLP/jqALfNWsWZAzrwr6tGHPffi6oGZFGo7Q7C1oMwxhAXFc7z143id2+v58n521m4LYtHLx9K76RYr6OxM6MQgF6NMJld3w5xvHLjWLZnFDBr2R7eWLmPD9YdoEubaC4Z0YVLRnalR0IMlZVKbnE5K3dn8/PX1jCye1senzr8hIpmIBaHutgdhDHme95b+w2/e2c9xWUV/HJSf64b19PTrphPfb6Nhz/ewrr7zyYuKrzuExqgpLyCeRsO8ubKfSzclokqJMREkFNcTkWl79/Gvu1jeePmk2kTAGtEu8HuIIwx9Xb+0M6M6dWOX81Zx4Pvb2Tx9iyeumoEEWHePHLalVlIYmxkoxcHgKjwUKYM68KUYV04kFvM3NX72Xu4iHYxEbSLiSQhJoKJJyUFbXGoixUIY8wx2reO4rkfp/LCol08+P5Gbp+1iid+NIJwD9oldmUW0SuxlevX6RQfbet5VxMYrVDGmIAjItwwoRf3n5/CvA2HuHP2mm9HCzelnVmF9AzAxXRaAruDMMbU6rrxvThSqfzxg02EhQqPXj6M0CZqk8gvKScjv5ReSVYgvGAFwhhTpxtPSaa8QvnLx5sR4G9NVCR2ZxUBjdODyTScFQhjTL38dGJvKlX567wtVCo8evlQ18dK7Mz0dXGtbYI+4x4rEMaYerv19D6EiPCXjzdToco/rhjmasP1rqMFwu4gPGEFwhjTID+d2JuwEOFPH26islL5x5XDiAwLdeVaOzML6RQfRXSEO99vaufminLPi0i6iKyvYf9VIvKViKwTkcUiMrTKvl3O9jUiYiPfjAkwN52azO9+mMJH6w9y2dNL2Hu4yJXrWA8mb7n5AHEmMKmW/TuB01R1MPAHYEa1/aer6rCaRvgZY7w1bUIvnr56JDszCjn/iYXM35Le6NfYlVlo7Q8ecq1AqOqXwOFa9i9W1Wzn41Kgq1tZjDHumDSoI+/ePoGOraO4YeYKHv1kS6ONlcgtKie7qLxJBskZ/wJloNw04KMqnxX4RERWisj02k4UkekikiYiaRkZGa6GNMYcq1diDHNvGc8lI7ry2P+28aN/L+ObnOIT/t6dWc4kfYneTxjYUnleIETkdHwF4pdVNk9Q1RHAZOBWETm1pvNVdYaqpqpqalJSYC52Ykywi44I5ZHLhvLo5UNZ/00u5z62gE82HDyh79yZWQBgdxAe8rQXk4gMAZ4FJqtq1tHtqrrf+TNdROYCo4EvvUlpjKmvi0d0ZXj3ttw+axXTX17JGf3bM653AqN6tiOlc+sGdYndmVlEiEC3dlYgvOJZgRCR7sAc4BpV/brK9hggRFXznfdnAw96FNMY00C9EmN466fjeOyzrby39gD/2+xrvI4OD+Wy1K7c8YO+JMRG1vk9uzIL6dwm2rUutKZurhUIEZkFTAQSRWQfcB8QDqCqTwO/BxKAp5yFNI44PZY6AHOdbWHAq6r6sVs5jTGNLzIslHvO6c895/TnUF4JK3Yd5vMtGbyybA9zVu3nltN7c8P4XkSF1/yP/66sQnpZDyZP2YJBxpgmsy29gIc+2synmw7ROT6KX583gPP8rM+sqgy5/xMuGtGFB6cM8ihty1DbgkGeN1IbY1qOPu1jefbHqcy6aSxtWkVw26uruerZZWw9lP+947IKy8gvPWKD5DxmBcIY0+RO7p3Ae7dP4A8XDmLDN3lM/ucC/u/DTZQd8Y2hODoHkz1i8pYVCGOMJ0JDhGvG9mD+3RO5dGRXZny5gytmLOFAbvG3s7hagfCWTdZnjPFUu5gIHrpkCKf2S+KeN9Zy/uMLGdg5nrAQoWvbaK/jtWh2B2GMCQjnDu7EO7eNJz46nC++zqBbu1aurzdhamd3EMaYgNGnfRzv3DaBP7y3kR42gtpzViCMMQElNjKMv1w6xOsYBnvEZIwxpgZWIIwxxvhlBcIYY4xfViCMMcb4ZQXCGGOMX1YgjDHG+GUFwhhjjF9WIIwxxvgVVOtBiEgGkAPkOpviq7yv+jnezzGJQOZxXLb6Neq73992f7nq896y13+/ZW+e2atu8zp7bZ+bY/Y2qprk91tVNahewAx/76t+9ncMkHai12vIfn/ba8pe13vLbtmDPXu1bZ5mr+1zc87u7xWMj5jeq+F91c+1HXMi12vIfn/ba8pVn/fHw7Ifu82y186r7Ceauz7fUd/stX1uztmPEVSPmE6EiKRpDcvuBTrL7g3L7g3L3nSC8Q7ieM3wOsAJsOzesOzesOxNxO4gjDHG+GV3EMYYY/yyAmGMMcYvKxDGGGP8sgJRDyJyiog8LSLPishir/PUl4iEiMifRORxEfmx13kaQkQmisgC5+99otd5GkpEYkQkTUR+6HWWhhCRAc7f+Zsi8lOv8zSEiFwoIv8WkddE5Gyv8zSEiCSLyHMi8qbXWaoK+gIhIs+LSLqIrK+2fZKIbBGRbSLyq9q+Q1UXqOrNwPvAi27mrZLvhHMDU4CuQDmwz62s1TVSdgUKgCiaX3aAXwKvu5PSv0b6b32T89/65cB4N/NW1UjZ31bVm4CbgSvczFtVI2XfoarT3E3acEHfi0lETsX3D81LqjrI2RYKfA2che8fnxXAVCAU+HO1r7hBVdOd814HpqlqfnPI7byyVfUZEXlTVS91O3cjZs9U1UoR6QA8qqpXNaPsQ4EEfMUtU1Xfby7ZVTVdRC4Afgq8rKqvNqfsznl/A15R1VXNMHuT/X9aH2FeB3Cbqn4pIj2rbR4NbFPVHQAiMhuYoqp/Bvw+EhCR7kBuUxQHaJzcIrIPKHM+VriX9vsa6+/ckQ1EupHTn0b6e58IxAApQLGIfKiqlW7mhsb7e1fVd4F3ReQDoEkKRCP9vQvwEPBRUxUHaPT/3gNK0BeIGnQB9lb5vA8YU8c504AXXEtUPw3NPQd4XEROAb50M1g9NCi7iFwMnAO0AZ5wN1qdGpRdVX8DICLX4dwJuZqudg39e58IXIyvKH/oarK6NfS/99uBM4F4Eemjqk+7Ga4ODf17TwD+BAwXkXudQuK5llogGkxV7/M6Q0OpahG+wtbsqOocfAWu2VLVmV5naChV/Rz43OMYx0VVHwMe8zrH8VDVLHxtJwEl6Bupa7Af6Fblc1dnW6BrrrnBsnvFsnujOWf/VkstECuAviLSS0QigCuBdz3OVB/NNTdYdq9Ydm805+zfOZ65yZvTC5gFHOC7rp7TnO3n4utlsB34jdc5gyW3Zbfslr35ZK/rFfTdXI0xxhyflvqIyRhjTB2sQBhjjPHLCoQxxhi/rEAYY4zxywqEMcYYv6xAGGOM8csKhAlqIlLQxNdrlPVCxLceRq6IrBGRzSLySD3OuVBEUhrj+saAFQhjGkREap2/TFXHNeLlFqjqMGA48EMRqWt9hgvxzSBrTKOwAmFaHBHpLSIfi8hK8a1a19/Zfr6ILBOR1SLyqbMWBSJyv4i8LCKLgJedz8+LyOciskNEflbluwucPyc6+9907gBecaajRkTOdbatFJHHRKTW9SJUtRhYg2+GUETkJhFZISJrReQtEWklIuOAC4C/OncdvWv6OY2pLysQpiWaAdyuqiOBu4GnnO0LgbGqOhyYDfy/KuekAGeq6lTnc39805GPBu4TkXA/1xkO3OmcmwyMF5Eo4BlgsnP9pLrCikhboC/fTdk+R1VHqepQYBO+qR0W45vr5x5VHaaq22v5OY2pF5vu27QoIhILjAPecH6hh+8WJOoKvCYinYAIYGeVU991fpM/6gNVLQVKRSQd6MCxS6MuV9V9znXXAD3xrTy2Q1WPfvcsYHoNcU8RkbX4isM/VPWgs32QiPwR31oZscC8Bv6cxtSLFQjT0oQAOc6z/eoex7e86bvOwjn3V9lXWO3Y0irvK/D//1J9jqnNAlX9oYj0ApaKyOuqugaYCVyoqmudRYkm+jm3tp/TmHqxR0ymRVHVPGCniFwGvmUqRWSoszue7+bs/7FLEbYAyVWWqLyirhOcu42HgF86m+KAA85jraprdec7++r6OY2pFysQJti1EpF9VV6/wPeP6jTn8c0GYIpz7P34HsmsBDLdCOM8proF+Ni5Tj6QW49TnwZOdQrL74BlwCJgc5VjZgP3OI3svan55zSmXmy6b2OamIjEqmqB06vpSWCrqv7d61zGVGd3EMY0vZucRusN+B5rPeNxHmP8sjsIY4wxftkdhDHGGL+sQBhjjPHLCoQxxhi/rEAYY4zxywqEMcYYv6xAGGOM8ev/Az1VkcV+BohXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.259039</td>\n",
       "      <td>0.139523</td>\n",
       "      <td>0.962448</td>\n",
       "      <td>0.636735</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.645963</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.108007</td>\n",
       "      <td>0.130293</td>\n",
       "      <td>0.962726</td>\n",
       "      <td>0.702041</td>\n",
       "      <td>0.625455</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053934</td>\n",
       "      <td>0.109036</td>\n",
       "      <td>0.970793</td>\n",
       "      <td>0.759184</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.739563</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       0.96      0.97      0.96        69\n",
      "      ORG       0.75      0.66      0.70        61\n",
      "      LOC       0.78      0.68      0.73        66\n",
      " LOCderiv       0.87      0.67      0.75        30\n",
      "  ORGpart       0.14      1.00      0.25         1\n",
      "      OTH       0.52      0.40      0.45        30\n",
      "  LOCpart       0.12      1.00      0.22         1\n",
      "\n",
      "micro avg       0.76      0.72      0.74       258\n",
      "macro avg       0.80      0.72      0.75       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "print(learn.token_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, learner=None, \n",
    "                 ctxs=None, max_n=6, **kwargs):    \n",
    "    # grab tokenizer\n",
    "    hf_textblock_tfm = learner.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x[0], y, samples, outs):\n",
    "        # recontstruct the string and split on space to get back your pre-tokenized list of tokens\n",
    "        toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=True)\n",
    "        pretokenized_toks =  hf_tokenizer.convert_tokens_to_string(toks).split()\n",
    "        \n",
    "        # get predictions for subtokens that aren't ignored (e.g. special toks and token parts)\n",
    "        pred_labels = [ pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != -100 ]\n",
    "        \n",
    "        trg_labels = ast.literal_eval(sample[1])\n",
    "        res.append([f'{[ (tok, trg, pred) for tok, pred, trg in zip(pretokenized_toks, pred_labels, trg_labels) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Neben', 'O', 'O'), ('einem', 'O', 'O'), ('4', 'O', 'O'), ('-', 'O', 'O'), ('in', 'O', 'O'), ('-', 'O', 'O'), ('1', 'O', 'O'), ('Kartenleser', 'O', 'O'), ('und', 'B-ORG', 'B-ORG'), ('Bluetooth', 'O', 'O'), ('2', 'O', 'O'), ('.', 'O', 'O'), ('0', 'O', 'O'), ('hat', 'O', 'O'), ('Medion', 'O', 'O'), ('einen', 'O', 'O'), ('8', 'O', 'O'), ('-', 'O', 'O'), ('fach', 'O', 'O'), ('DVD', 'O', 'O'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Das', 'O', 'O'), ('ist', 'O', 'O'), ('die', 'O', 'O'), ('Geschäftspolitik', 'O', 'O'), ('meines', 'O', 'O'), ('Vorgängers', 'O', 'O'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ich', 'O', 'O'), ('sehr', 'O', 'O'), ('schätze', 'O', 'O'), (':', 'O', 'O'), ('Jedes', 'O', 'O'), ('Jahr', 'O', 'O'), ('gibt', 'O', 'O'), ('es', 'O', 'O'), ('ein', 'O', 'O'), ('bisschen', 'O', 'O'), ('mehr', 'O', 'O'), (',', 'O', 'O'), ('ergänzte', 'O', 'O'), ('Lutz', 'B-PER', 'B-PER'), (',', 'O', 'O'), ('der', 'O', 'O'), ('seit', 'O', 'O'), ('zwei', 'O', 'O'), ('Jahren', 'O', 'O'), ('Vorstandsvorsitzender', 'O', 'O'), ('der', 'O', 'O'), ('BayWa', 'B-ORG', 'B-ORG'), ('AG', 'I-ORG', 'I-ORG'), ('ist', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict_tokens(self:Learner, inp, **kargs):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
    "    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input\n",
    "    \"\"\"\n",
    "    pred_lbls, pred_lbl_ids, probs = self.blurr_predict(inp)\n",
    "\n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_textblock_tfm.kwargs\n",
    "    \n",
    "    # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "    # return\n",
    "    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp ]\n",
    "    \n",
    "    # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "    # (e.g., [CLS], [SEP], etc...)\n",
    "    res = hf_tokenizer(inp, None, \n",
    "                       max_length=hf_textblock_tfm.max_length,\n",
    "                       padding=hf_textblock_tfm.padding,\n",
    "                       truncation=hf_textblock_tfm.truncation,\n",
    "                       is_pretokenized=hf_textblock_tfm.is_pretokenized,\n",
    "                       **tok_kwargs)\n",
    "\n",
    "    special_toks_msk = L(res['special_tokens_mask'])\n",
    "    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "    # using the indexes to the actual tokens, get that info from the results returned above\n",
    "    pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "    actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "    # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "    offset = 0\n",
    "    raw_trg_idxs = []\n",
    "    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "        raw_trg_idxs.append(idx+offset)\n",
    "        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`inp`**, **\\*\\*`kargs`**)\n",
       "\n",
       "Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
       "get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "print([(tok, lbl) for tok,lbl in zip(res[0],res[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_albert.AlbertForTokenClassification,\n",
       " transformers.modeling_auto.AutoModelForTokenClassification,\n",
       " transformers.modeling_bert.BertForTokenClassification,\n",
       " transformers.modeling_camembert.CamembertForTokenClassification,\n",
       " transformers.modeling_distilbert.DistilBertForTokenClassification,\n",
       " transformers.modeling_electra.ElectraForTokenClassification,\n",
       " transformers.modeling_longformer.LongformerForTokenClassification,\n",
       " transformers.modeling_mobilebert.MobileBertForTokenClassification,\n",
       " transformers.modeling_roberta.RobertaForTokenClassification,\n",
       " transformers.modeling_xlm.XLMForTokenClassification,\n",
       " transformers.modeling_xlm_roberta.XLMRobertaForTokenClassification,\n",
       " transformers.modeling_xlnet.XLNetForTokenClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='TokenClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    #'<electra>', # currently no pre-trained electra model works for token classification\n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-ende-1024',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForTokenClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.316232</td>\n",
       "      <td>0.379795</td>\n",
       "      <td>0.912136</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.229665</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('das', 'O', 'O'), ('cas-urteil', 'B-ORGpart', 'O'), ('nannte', 'O', 'O'), ('lehner', 'B-PER', 'O'), ('einen', 'O', 'O'), ('schlag', 'O', 'O'), ('ins', 'O', 'O'), ('gesicht', 'O', 'O'), ('fur', 'O', 'O'), ('das', 'O', 'O'), ('sportrecht', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('seit', 'O', 'O'), ('1925', 'O', 'O'), ('ubernimmt', 'O', 'O'), ('die', 'O', 'O'), ('post', 'B-ORG', 'O'), ('die', 'O', 'O'), ('personenbeforderung', 'O', 'O'), ('mit', 'O', 'O'), ('postkraftwagen', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-multilingual-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.199364</td>\n",
       "      <td>0.160735</td>\n",
       "      <td>0.957389</td>\n",
       "      <td>0.668103</td>\n",
       "      <td>0.582707</td>\n",
       "      <td>0.622490</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Die', 'O', 'O'), ('Tradition', 'O', 'O'), ('hat', 'O', 'O'), ('sie', 'O', 'O'), ('an', 'O', 'O'), ('ihre', 'O', 'O'), ('Kinder', 'O', 'O'), ('weitergegeben', 'O', 'O'), (',', 'O', 'O'), ('ihre', 'O', 'O'), ('Tochter', 'O', 'O'), ('Marica', 'B-PER', 'B-PER'), ('(', 'O', 'O'), ('8', 'O', 'O'), (')', 'O', 'O'), ('machte', 'O', 'O'), ('gestern', 'O', 'O'), ('beim', 'O', 'O'), ('Turnier', 'O', 'O'), ('mit', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('«', 'O', 'O'), ('Um', 'O', 'O'), ('etwas', 'O', 'O'), ('zu', 'O', 'O'), ('verändern', 'O', 'O'), (',', 'O', 'O'), ('müssen', 'O', 'O'), ('wir', 'O', 'O'), ('Frauen', 'O', 'O'), ('uns', 'O', 'O'), ('einsetzen', 'O', 'O'), ('»', 'O', 'O'), (',', 'O', 'O'), ('sagt', 'O', 'O'), ('sie', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.941831</td>\n",
       "      <td>0.821777</td>\n",
       "      <td>0.908688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Das', 'O', 'O'), ('Wort', 'O', 'O'), ('bezeichnete', 'O', 'O'), ('Gelndestellen', 'O', 'O'), (',', 'O', 'O'), ('an', 'O', 'O'), ('denen', 'O', 'O'), ('es', 'O', 'O'), ('ntig', 'O', 'O'), ('war', 'O', 'O'), (',', 'O', 'O'), ('wegen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Innenminister', 'O', 'O'), ('Radek', 'B-PER', 'O'), ('John', 'I-PER', 'O'), ('hat', 'O', 'O'), ('fr', 'O', 'O'), ('das', 'O', 'O'), ('kommende', 'O', 'O'), ('Jahr', 'O', 'O'), ('die', 'O', 'O'), ('Einfhrung', 'O', 'O'), ('hherer', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.347233</td>\n",
       "      <td>0.392468</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.402685</td>\n",
       "      <td>0.351906</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('der', 'O', 'O'), ('erste', 'O', 'O'), ('am', 'O', 'O'), ('dienstag', 'O', 'O'), ('vor', 'O', 'O'), ('palmsonntag', 'O', 'O'), (',', 'O', 'O'), ('der', 'O', 'O'), ('zweite', 'O', 'O'), ('am', 'O', 'O'), ('10', 'O', 'O'), ('.', 'O', 'O'), ('august', 'O', 'O'), ('und', 'O', 'O'), ('der', 'O', 'O'), ('dritte', 'O', 'O'), ('am', 'O', 'O'), ('29', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('zunachst', 'O', 'O'), ('gab', 'O', 'O'), ('es', 'O', 'O'), ('die', 'O', 'O'), ('mehr', 'O', 'O'), ('oder', 'O', 'O'), ('weniger', 'O', 'O'), ('spontanen', 'O', 'O'), (',', 'O', 'O'), ('unkontrollierten', 'O', 'O'), ('aktionen', 'O', 'O'), ('(', 'O', 'O'), ('epuration', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.342755</td>\n",
       "      <td>0.381885</td>\n",
       "      <td>0.905371</td>\n",
       "      <td>0.132597</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.194332</td>\n",
       "      <td>03:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Es', 'O', 'O'), ('erreichte', 'O', 'O'), ('in', 'O', 'O'), ('den', 'O', 'O'), ('USA', 'B-LOC', 'O'), ('und', 'O', 'O'), ('in', 'O', 'O'), ('Großbritannien', 'B-LOC', 'O'), ('jeweils', 'O', 'O'), ('Platz', 'O', 'O'), ('4', 'O', 'O'), ('der', 'O', 'O'), ('Charts', 'O', 'O'), ('und', 'O', 'O'), ('blieb', 'O', 'O'), ('15', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Rehaforscher', 'O', 'O'), ('haben', 'O', 'O'), ('das', 'O', 'O'), ('Kraft-', 'O', 'O'), (',', 'O', 'O'), ('Ausdauer-', 'O', 'O'), ('und', 'O', 'O'), ('Koordinationstraining', 'O', 'O'), (',', 'O', 'O'), ('kurz', 'O', 'O'), ('Kako', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForTokenClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.588358</td>\n",
       "      <td>0.530755</td>\n",
       "      <td>0.896880</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.062284</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('der', 'O', 'O'), ('karneval', 'O', 'O'), ('wird', 'O', 'O'), ('hier', 'O', 'O'), ('an', 'O', 'O'), ('den', 'O', 'O'), ('vier', 'O', 'O'), ('tagen', 'O', 'O'), ('vor', 'O', 'O'), ('aschermittwoch', 'O', 'O'), ('gefeiert', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('1953', 'O', 'O'), ('hinter', 'O', 'O'), ('horst', 'B-PER', 'O'), ('schade', 'I-PER', 'O'), ('(', 'O', 'O'), ('22', 'O', 'O'), ('tore', 'O', 'O'), (')', 'O', 'O'), ('und', 'O', 'O'), ('1954', 'O', 'O'), ('hinter', 'O', 'O'), ('helmut', 'B-PER', 'O'), ('preisendorfer', 'I-PER', 'O'), ('und', 'O', 'O'), ('nochmals', 'O', 'O'), ('horst', 'B-PER', 'O'), ('schade', 'I-PER', 'O'), ('mit', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.325484</td>\n",
       "      <td>0.412405</td>\n",
       "      <td>0.902539</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Eine', 'O', 'O'), ('Frage', 'O', 'O'), (',', 'O', 'O'), ('die', 'O', 'O'), ('sich', 'O', 'O'), ('physikalistische', 'O', 'O'), ('Positionen', 'O', 'O'), ('stellen', 'O', 'O'), ('müssen', 'O', 'O'), (',', 'O', 'O'), ('ist', 'O', 'O'), (',', 'O', 'O'), ('ob', 'O', 'O'), ('es', 'O', 'O'), ('eine', 'O', 'O'), ('physik', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Die', 'O', 'O'), ('meisten', 'O', 'O'), ('Lehrmittel', 'O', 'O'), (',', 'O', 'O'), ('wie', 'O', 'O'), ('Bücher', 'O', 'O'), (',', 'O', 'O'), ('werden', 'O', 'O'), ('gestellt', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-ende-1024 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-mlm-ende-1024 were not used when initializing XLMForTokenClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForTokenClassification were not initialized from the model checkpoint at xlm-mlm-ende-1024 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.556006</td>\n",
       "      <td>0.478039</td>\n",
       "      <td>0.907362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('souness', 'B-PER', 'O'), ('hatte', 'O', 'O'), ('zuvor', 'O', 'O'), ('im', 'O', 'O'), ('englischen', 'B-LOCderiv', 'O'), ('fußball', 'O', 'O'), ('große', 'O', 'O'), ('erfolge', 'O', 'O'), ('beim', 'O', 'O'), ('fc', 'B-ORG', 'O'), ('liverpool', 'I-ORG', 'O'), ('gefeiert', 'O', 'O'), ('und', 'O', 'O'), ('setzte', 'O', 'O'), ('in', 'O', 'O'), ('der', 'O', 'O'), ('transferpolitik', 'O', 'O'), ('-', 'O', 'O'), ('unterstutzt', 'O', 'O'), ('von', 'O', 'O'), ('hol', 'B-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('nahost-konflikt', 'B-LOCpart', 'O'), (':', 'O', 'O'), ('deutscher', 'B-LOCderiv', 'O'), ('militareinsatz', 'O', 'O'), ('in', 'O', 'O'), ('nahost', 'B-LOC', 'O'), ('findet', 'O', 'O'), ('mehr', 'O', 'O'), ('befurworter', 'O', 'O'), ('neben', 'O', 'O'), ('union', 'B-ORG', 'O'), ('und', 'O', 'O'), ('spd', 'B-ORG', 'O'), ('denken', 'O', 'O'), ('auch', 'O', 'O'), ('judische', 'O', 'O'), ('vertreter', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.258121</td>\n",
       "      <td>0.218065</td>\n",
       "      <td>0.944282</td>\n",
       "      <td>0.520362</td>\n",
       "      <td>0.518018</td>\n",
       "      <td>0.519187</td>\n",
       "      <td>00:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Zwischen', 'O', 'O'), ('dem', 'O', 'O'), ('Stadtzentrum', 'O', 'O'), ('von', 'O', 'O'), ('Kastoria', 'B-LOC', 'B-LOC'), ('und', 'O', 'O'), ('dem', 'O', 'O'), ('Seeufer', 'O', 'O'), ('besteht', 'O', 'O'), ('bisweilen', 'O', 'O'), ('ein', 'O', 'O'), ('starkes', 'O', 'O'), ('Gefälle', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Des', 'O', 'O'), ('Weiteren', 'O', 'O'), ('hat', 'O', 'O'), ('das', 'O', 'O'), ('privater', 'O', 'O'), ('Müllentsorgungsunternehmen', 'O', 'O'), ('Werner', 'B-ORG', 'B-ORG'), ('RC', 'I-ORG', 'I-ORG'), ('GmbH', 'I-ORG', 'I-ORG'), ('&amp;', 'I-ORG', 'O'), ('Co', 'I-ORG', 'I-ORG'), ('seinen', 'O', 'O'), ('Sitz', 'O', 'O'), ('in', 'O', 'O'), ('Goldbach', 'B-LOC', 'B-LOC'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForTokenClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.414580</td>\n",
       "      <td>0.408119</td>\n",
       "      <td>0.899368</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('Geboren', 'B-OTH', 'O'), ('um', 'I-OTH', 'O'), ('zu', 'I-OTH', 'O'), ('leben', 'I-OTH', 'O'), ('\"', 'O', 'O'), ('hat', 'O', 'O'), ('ein', 'O', 'O'), ('paar', 'O', 'O'), ('Dominosteine', 'O', 'O'), ('in', 'O', 'O'), ('Bewegung', 'O', 'O'), ('gesetz', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Einige', 'O', 'O'), ('soziale', 'O', 'O'), ('Organisationen', 'O', 'O'), ('bekundeten', 'O', 'O'), ('ihre', 'O', 'O'), ('Unzufriedenheit', 'O', 'O'), ('mit', 'O', 'O'), ('dieser', 'O', 'O'), ('Entlass', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "bsz = 2\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config=config)\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n')\n",
    "    \n",
    "    hf_batch_tfm = HF_TokenClassBatchTransform(hf_arch, hf_tokenizer)\n",
    "\n",
    "    blocks = (\n",
    "        HF_TextBlock(hf_arch, hf_tokenizer, is_pretokenized=True, max_length=32, padding='max_length',\n",
    "                     hf_batch_tfm=hf_batch_tfm,\n",
    "                     tok_kwargs={ 'return_special_tokens_mask': True }), \n",
    "        HF_TokenCategoryBlock(vocab=labels)\n",
    "    )\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('tokens'),\n",
    "                       get_y= lambda inp: [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ],\n",
    "                       splitter=RandomSplitter())\n",
    "    \n",
    "    dls = dblock.dataloaders(germ_eval_df, bs=bsz)\n",
    "\n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                cbs=[HF_TokenClassCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.unfreeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, 32]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(len(preds[0]), bsz)\n",
    "        test_eq(preds[0].shape, torch.Size([bsz, 32, len(labels)]))\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max= 3e-5, moms=(0.8,0.7,0.8))\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(learner=learn, max_n=2)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizer</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizer</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizer</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizer</td>\n",
       "      <td>DistilBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizer</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizer</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizer</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizer</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model_name', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
