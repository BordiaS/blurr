{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from blurr.utils import *\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseInput(list): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Tokenizer():\n",
    "    def __init__(self, hf_arch, hf_tokenizer, **kwargs):\n",
    "        self.hf_arch = hf_arch\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        \n",
    "    def __call__(self, items): \n",
    "        for txt in items: yield self._tokenize(txt)\n",
    "\n",
    "    def _tokenize(self, txt):\n",
    "        return self.hf_tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BatchTransform(Transform):\n",
    "    \n",
    "    def __init__(self, hf_arch, hf_tokenizer, max_seq_len=512, truncation_strategy='longest_first'):\n",
    "        \n",
    "        self.hf_arch = hf_arch\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        store_attr(self, 'max_seq_len, truncation_strategy')\n",
    "        \n",
    "    def encodes(self, samples):\n",
    "        \n",
    "        encoded_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            \n",
    "            if (isinstance(sample[0], tuple)):\n",
    "                a_tok_ids = sample[0][0].tolist()\n",
    "                b_tok_ids = sample[0][1].tolist()\n",
    "            else:\n",
    "                a_tok_ids = sample[0].tolist()\n",
    "                b_tok_ids = None\n",
    "            \n",
    "            res = self.hf_tokenizer.prepare_for_model(a_tok_ids, b_tok_ids, \n",
    "                                                      max_length=self.max_seq_len, \n",
    "                                                      pad_to_max_length=True,\n",
    "                                                      truncation_strategy=self.truncation_strategy,\n",
    "                                                      return_tensors='pt')\n",
    "            \n",
    "            input_ids = res['input_ids'][0]\n",
    "            token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "            attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "\n",
    "            inputs = [input_ids, token_type_ids, attention_mask]\n",
    "            targets = sample[1:]\n",
    "            \n",
    "            encoded_samples.append((HF_BaseInput(inputs), *targets))\n",
    "            \n",
    "        return encoded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TextBlock(TransformBlock):\n",
    "    \n",
    "    @delegates(Numericalize.__init__)\n",
    "    def __init__(self, tok_tfms, hf_arch, hf_tokenizer, \n",
    "                 hf_batch_tfm=None, vocab=None, max_seq_len=512, **kwargs):\n",
    "\n",
    "        if hf_batch_tfm is None:\n",
    "            hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, max_seq_len=max_seq_len,\n",
    "                                             truncation_strategy='longest_first')\n",
    "            \n",
    "        return super().__init__(type_tfms=[*tok_tfms, Numericalize(vocab, **kwargs)],\n",
    "                                dl_type=SortedDL, \n",
    "                                dls_kwargs={ 'before_batch': hf_batch_tfm })\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(Tokenizer.from_df, keep=True)\n",
    "    def from_df(cls, text_cols_lists, hf_arch, hf_tokenizer,\n",
    "                res_col_names=None, vocab=None, \n",
    "                hf_batch_tfm=None, max_seq_len=512, **kwargs):\n",
    "        \n",
    "        # grab hf tokenizer class to do the actual tokenization (via tok_func) and its vocab\n",
    "        tokenizer_cls = partial(HF_Tokenizer, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer)\n",
    "        if (vocab is None): vocab = list(hf_tokenizer.get_vocab())\n",
    "\n",
    "        # build the column name(s) returned after tokenization\n",
    "        if (res_col_names is None): res_col_names = [ f'text{i}' for i in range(len(text_cols_lists)) ] \n",
    "    \n",
    "        tok_tfms = [ Tokenizer.from_df(text_cols, \n",
    "                                       res_col_name=res_col_name, \n",
    "                                       tok_func=tokenizer_cls,\n",
    "                                       rules=[], **kwargs) \n",
    "                    for text_cols, res_col_name in zip(text_cols_lists, res_col_names) ]\n",
    "  \n",
    "        return cls(tok_tfms, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, \n",
    "                   hf_batch_tfm=hf_batch_tfm, vocab=vocab, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example when Huggingface model requires a single text item (e.g., sequence classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['text']], hf_arch=hf_arch, hf_tokenizer=hf_tokenizer),\n",
    "    CategoryBlock\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: x.text0,\n",
    "                   get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]),\n",
       " torch.Size([4, 1]),\n",
       " torch.Size([4, 512]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0].shape, b[0][1].shape, b[0][2].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:HF_BaseInput, y, samples, hf_tokenizer, ctxs=None, max_n=6, **kwargs):        \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "        \n",
    "    samples = samples = L((TitledStr(hf_tokenizer.decode(inp)),*s[1:]) for inp, s in zip(x[0], samples))\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;Raising Victor Vargas: A Review&lt;br /&gt;&lt;br /&gt;You know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It's warm and gooey, but you're not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn't quite feel right. Victor Vargas suffers from a certain overconfidence on the director's part. Apparently, the director thought that the ethnic backdrop of a Latino family on the lower east side, and an idyllic storyline would make the film critic proof. He was right, but it didn't fool me. Raising Victor Vargas is the story about a seventeen-year old boy called, you guessed it, Victor Vargas (Victor Rasuk) who lives his teenage years chasing more skirt than the Rolling Stones could do in all the years they've toured. The movie starts off in `Ugly Fat' Donna's bedroom where Victor is sure to seduce her, but a cry from outside disrupts his plans when his best-friend Harold (Kevin Rivera) comes-a-looking for him. Caught in the attempt by Harold and his sister, Victor Vargas runs off for damage control. Yet even with the embarrassing implication that he's been boffing the homeliest girl in the neighborhood, nothing dissuades young Victor from going off on the hunt for more fresh meat. On a hot, New York City day they make way to the local public swimming pool where Victor's eyes catch a glimpse of the lovely young nymph Judy (Judy Marte), who's not just pretty, but a strong and independent too. The relationship that develops between Victor and Judy becomes the focus of the film. The story also focuses on Victor's family that is comprised of his grandmother or abuelita (Altagracia Guzman), his brother Nino (also played by real life brother to Victor, Silvestre Rasuk) and his sister Vicky (Krystal Rodriguez). The action follows Victor between scenes with Judy and scenes with his family. Victor tries to cope with being an oversexed pimp-daddy, his feelings for Judy and his grandmother's conservative Catholic upbringing.&lt;br /&gt;&lt;br /&gt;The problems that arise from Raising Victor Vargas are a few, but glaring errors. Throughout the film you get to know certain characters like Vicky, Nino, Grandma,&lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;With its companion piece MASTERS OF HORROR, NIGHTMARES AND DREAMSCAPES can only be seen as the absolute nadir of the genre that began so auspiciously with THE TWILIGHT ZONE and THE OUTER LIMITS.&lt;br /&gt;&lt;br /&gt;Of course, part of the problem is that it does nothing to be of any interest to a comparatively adult audience, instead aiming at TEN-YEAR-OLDS, who are only able to count body-bags, and scarcely that. And so grossness is king, and King is grossness.&lt;br /&gt;&lt;br /&gt;Stephen King is simply illiterate  in general he has the aptitude for storytelling of Bart Simpson. Since he cannot read his sole inspiration is the movies.&lt;br /&gt;&lt;br /&gt;True, the cinema is not such a bad place to start, since it has generally escaped the onslaught of \"Realism\". But these films are only the rumor, not the thing, and if you want to WRITE, you have to dig deeper.&lt;br /&gt;&lt;br /&gt;Of course, only PICKMAN had monsters as close acquaintances. But even so, it should be clear to any undergraduate that vampires are not Dracula and Lugosi.&lt;br /&gt;&lt;br /&gt;At least AUTOPSY ROOM FOUR is a clear indication of what is wrong. One can almost imagine this pathetic dolt sitting as his desk trying to come up with something SCARY.&lt;br /&gt;&lt;br /&gt;Not, mind you, trying to describe accurately the horror of the system of which he is an integral part, making the stupid stupider, but trying to come up with a scary story for his little nephew. Suppose, you were paralyzed, and people thought you were dead and started to cut you open like they do at those autopsy things! Wouldn't that be gross? And that, boys and girls, is the story.&lt;br /&gt;&lt;br /&gt;What about characterization? Oh yes, he's one of these suits, who never really appreciated life, you know, and now it's too late, right? And he's shouting  well, they can't actually hear him, you know  he's saying that he's going to sue the hospital, but he's not such a big shot anymore, you see, lying there (or is it laying, I can never remember) and all. And he's thinking: Oh no please, please don't cut me and this is terrible, lying (or laying) like that &lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example when Huggingface model requires a two text item (e.g., question answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./data/task-question-answering/squad/')\n",
    "squad_df = pd.read_csv(path/'squad_cleaned_sample.csv'); len(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bermuda</td>\n",
       "      <td>The professionals soon displaced the amateur ex-Public schoolboys. Bermuda's role as the primary Royal Navy base in the Western Hemisphere, with an army garrison to match, ensured that the naval and military officers quickly introduced the newly formalised sports to Bermuda, including cricket, football, Rugby football, and even tennis and rowing (rowing did not adapt well from British rivers to the stormy Atlantic. The officers soon switched to sail racing, founding the Royal Bermuda Yacht Club). Once these sports reached Bermuda, they were eagerly adopted by Bermudians.</td>\n",
       "      <td>5ad42b72604f3c001a400930</td>\n",
       "      <td>Who founded the Bermuda Royal Yacht Club?</td>\n",
       "      <td>True</td>\n",
       "      <td>naval and military officers</td>\n",
       "      <td>189</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black_people</td>\n",
       "      <td>Additionally, there are around 60,000 non-Jewish African immigrants in Israel, some of whom have sought asylum. Most of the migrants are from communities in Sudan and Eritrea, particularly the Niger-Congo-speaking Nuba groups of the southern Nuba Mountains; some are illegal immigrants.</td>\n",
       "      <td>5706b77e0eeca41400aa0d8f</td>\n",
       "      <td>Where are most of them from?</td>\n",
       "      <td>False</td>\n",
       "      <td>Sudan and Eritrea</td>\n",
       "      <td>157</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title  \\\n",
       "0       Bermuda   \n",
       "1  Black_people   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             context  \\\n",
       "0  The professionals soon displaced the amateur ex-Public schoolboys. Bermuda's role as the primary Royal Navy base in the Western Hemisphere, with an army garrison to match, ensured that the naval and military officers quickly introduced the newly formalised sports to Bermuda, including cricket, football, Rugby football, and even tennis and rowing (rowing did not adapt well from British rivers to the stormy Atlantic. The officers soon switched to sail racing, founding the Royal Bermuda Yacht Club). Once these sports reached Bermuda, they were eagerly adopted by Bermudians.   \n",
       "1                                                                                                                                                                                                                                                                                                     Additionally, there are around 60,000 non-Jewish African immigrants in Israel, some of whom have sought asylum. Most of the migrants are from communities in Sudan and Eritrea, particularly the Niger-Congo-speaking Nuba groups of the southern Nuba Mountains; some are illegal immigrants.   \n",
       "\n",
       "                question_id                              question_text  \\\n",
       "0  5ad42b72604f3c001a400930  Who founded the Bermuda Royal Yacht Club?   \n",
       "1  5706b77e0eeca41400aa0d8f               Where are most of them from?   \n",
       "\n",
       "   is_impossible                  answer_text  answer_start  answer_end  \n",
       "0           True  naval and military officers           189         216  \n",
       "1          False            Sudan and Eritrea           157         174  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = squad_df[(squad_df.answer_end < max_seq_len) & (squad_df.is_impossible == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForQuestionAnswering\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(enumerate(range(max_seq_len)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional): override HF_BatchTransform defaults\n",
    "hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, max_seq_len=128, truncation_strategy='longest_first')\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['context'],['question_text']], \n",
    "                         hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, hf_batch_tfm=hf_batch_tfm),\n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: (x.text0, x.text1),\n",
    "                   get_y=[ColReader('answer_start'), ColReader('answer_end')],\n",
    "                   splitter=RandomSplitter(),\n",
    "                   n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(squad_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]), len(b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]),\n",
       " torch.Size([4, 1]),\n",
       " torch.Size([4, 128]),\n",
       " torch.Size([4]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0].shape, b[0][1].shape, b[0][2].shape, b[1].shape, b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;In 2013 tourism within the state from local citizens accounted for 39.9% of tourists, the second highest originating location for tourists to Tennessee is the state of Georgia, accounting for 8.4% of tourists.:17 Forty-four percent of stays in the state were \"day trips\", 25% stayed one night, 15% stayed two nights, and 11% stayed 4 or more nights. The average stay was 2.16 nights, compared to 2.03 nights for the US as a whole.:40 The average person spent $&lt;/s&gt;&lt;/s&gt;What percentage of out-of-state tourists stayed in Tennessee for four or more nights?&lt;/s&gt;</td>\n",
       "      <td>321</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;KU's Edwards Campus is in Overland Park, Kansas. Established in 1993, its goal is to provide adults with the opportunity to complete college degrees. About 2,100 students attend the Edwards Campus, with an average age of 32. Programs available at the Edwards Campus include developmental psychology, public administration, social work, systems analysis, information technology, engineering management and design.&lt;/s&gt;&lt;/s&gt;When was the Edwards Campus built?&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
