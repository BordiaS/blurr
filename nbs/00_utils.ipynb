{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Various utility functions used by the blurr package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys, inspect\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def str_to_class(classname):\n",
    "    \"converts string representation to class\"\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Singleton:\n",
    "    def __init__(self,cls):\n",
    "        self._cls, self._instance = cls, None\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._instance == None: self._instance = self._cls(*args, **kwargs)\n",
    "        return self._instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Singleton` functions as python decorator.  Use this above any class to turn that class into a singleton (see [here](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html) for more info on the singleton pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Singleton\n",
    "class TestSingleton: pass\n",
    "\n",
    "a = TestSingleton()\n",
    "b = TestSingleton()\n",
    "test_eq(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Singleton\n",
    "class ModelHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # get hf classes (tokenizers, configs, models, etc...)\n",
    "        transformer_classes = inspect.getmembers(sys.modules[__name__], \n",
    "                                                 lambda member: inspect.isclass(member)\n",
    "                                                 and member.__module__.startswith('transformers.'))\n",
    "        \n",
    "        # build a df that we can query against to get various transformers objects/info\n",
    "        self._df = pd.DataFrame(transformer_classes, columns=['class_name', 'class_location'])\n",
    "        \n",
    "        # add the module each class is included in\n",
    "        self._df['module'] = self._df.class_location.apply(lambda v: v.__module__)\n",
    "        \n",
    "        # remove class_location (don't need it anymore)\n",
    "        self._df.drop(labels=['class_location'], axis=1, inplace=True)\n",
    "        \n",
    "        # break up the module into separate cols\n",
    "        module_parts_df = self._df.module.str.split(\".\", n = -1, expand = True) \n",
    "        for i in range(len(module_parts_df.columns)):\n",
    "            self._df[f'module_part_{i}'] = module_parts_df[i]\n",
    "\n",
    "        # using module part 1, break up the functional area and arch into separate cols\n",
    "        module_part_1_df = self._df.module_part_1.str.split(\"_\", n = 1, expand = True) \n",
    "        self._df[['functional_area', 'arch']] = module_part_1_df\n",
    "        \n",
    "        # if functional area = modeling, pull out the task it is built for\n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.split('For', n=1, expand=True)\n",
    "        \n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'For' + model_type_df[1].astype(str), \n",
    "                                    model_type_df[1])\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        self._df['model_task'] = self._df['model_task'].str.replace('For', '', n=1, case=True, regex=False)\n",
    "        \n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.split('With', n=1, expand=True)\n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'With' + model_type_df[1].astype(str), \n",
    "                                    self._df[(self._df.functional_area == 'modeling')].model_task)\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        self._df['model_task'] = self._df['model_task'].str.replace('With', '', n=1, case=True, regex=False)\n",
    "        \n",
    "        # look at what we're going to remove (use to verify we're just getting rid of stuff we want too)\n",
    "        # df[~df['hf_class_type'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "        # only need these 3 functional areas for our querying purposes\n",
    "        self._df = self._df[self._df['functional_area'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "    def get_architectures(self): \n",
    "        \"\"\"Used to get all the architectures supported by your `Transformers` install\"\"\"\n",
    "        return sorted(self._df[(self._df.arch.notna()) & \n",
    "                        (self._df.arch != None) & \n",
    "                        (self._df.arch != 'utils')].arch.unique().tolist())\n",
    "    \n",
    "    def get_config(self, arch): \n",
    "        \"\"\"Used the locate the name of the configuration class for a given architecture\"\"\"\n",
    "        config = self._df[(self._df.functional_area == 'configuration') & \n",
    "                          (self._df.arch == arch)].class_name.values[0]\n",
    "        \n",
    "        return str_to_class(config)\n",
    "    \n",
    "    def get_tokenizers(self, arch): \n",
    "        \"\"\"Used to get the available huggingface tokenizers for a given architecture. Note: There may be \n",
    "        multiple tokenizers and so this returns a list.\n",
    "        \"\"\"\n",
    "        toks = sorted(self._df[(self._df.functional_area == 'tokenization') & \n",
    "                               (self._df.arch == arch)].class_name.values)\n",
    "        \n",
    "        return [str_to_class(tok_name) for tok_name in toks]\n",
    "    \n",
    "    def get_tasks(self, arch=None): \n",
    "        \"\"\"Get the type of tasks for which there is a custom model for (*optional: by architecture*). \n",
    "        There are a number of customized models built for specific tasks like token classification, \n",
    "        question/answering, LM, etc....\n",
    "        \"\"\"\n",
    "        query = ['model_task.notna()']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "\n",
    "        return sorted(self._df.query(' & '.join(query), engine='python').model_task.unique().tolist())\n",
    "    \n",
    "    def get_models(self, arch=None, task=None):\n",
    "        \"\"\"The transformer models available for use (optional: by architecture | task)\"\"\"\n",
    "        query = ['functional_area == \"modeling\"']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "        if (task): query.append(f'model_task == \"{task}\"')\n",
    "\n",
    "        models = sorted(self._df.query(' & '.join(query)).class_name.tolist())\n",
    "        return [str_to_class(model_name) for model_name in models] \n",
    "    \n",
    "    def get_classes_for_model(self, model_name_or_cls):\n",
    "        \"\"\"Get tokenizers, config, and model for a given model name / class\"\"\"\n",
    "        model_name = model_name_or_cls if isinstance(model_name_or_cls, str) else model_name_or_cls.__name__\n",
    "\n",
    "        meta = self._df[self._df.class_name == model_name]\n",
    "        tokenizers = self.get_tokenizers(meta.arch.values[0])\n",
    "        config = self.get_config(meta.arch.values[0])\n",
    "\n",
    "        return (config, tokenizers, str_to_class(model_name))\n",
    "    \n",
    "    def get_model_architecture(self, model_name_or_enum):\n",
    "        \"\"\"Get the architecture for a given model name / enum\"\"\"\n",
    "        model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "        return self._df[self._df.class_name == model_name].arch.values[0]\n",
    "    \n",
    "    def get_hf_objects(self, pretrained_model_name_or_path, task=None,\n",
    "                       config=None, tokenizer_cls=None, model_cls=None, \n",
    "                       config_kwargs={}, tokenizer_kwargs={}, model_kwargs={}, cache_dir=None):\n",
    "        \"\"\"Returns the architecture (str), config (obj), tokenizer (obj), and model (obj) given at minimum a\n",
    "        `pre-trained model name or path`. Specify a `task` to ensure the right \"AutoModelFor<task>\" is used to\n",
    "        create the model.\n",
    "        \n",
    "        Optionally, you can pass a config (obj), tokenizer (class), and/or model (class) (along with any \n",
    "        related kwargs for each) to get as specific as you want w/r/t what huggingface objects are returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        # config\n",
    "        if (config is None):\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir, **config_kwargs)\n",
    "            \n",
    "        # tokenizer\n",
    "        if (tokenizer_cls is None):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                      cache_dir=cache_dir, \n",
    "                                                      **tokenizer_kwargs)\n",
    "        else:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                      cache_dir=cache_dir, \n",
    "                                                      **tokenizer_kwargs)\n",
    "            \n",
    "        # model\n",
    "        if (model_cls is None and task is None):\n",
    "            model = AutoModel.from_pretrained(pretrained_model_name_or_path, \n",
    "                                              config=config, \n",
    "                                              cache_dir=cache_dir, \n",
    "                                              **model_kwargs)\n",
    "        else:\n",
    "            if (model_cls is None and task is not None): \n",
    "                model_cls = self.get_models(arch=\"auto\", task=task.name)[0]\n",
    "            \n",
    "            model = model_cls.from_pretrained(pretrained_model_name_or_path, \n",
    "                                              config=config, \n",
    "                                              cache_dir=cache_dir, \n",
    "                                              **model_kwargs)\n",
    "            \n",
    "        #arch\n",
    "        arch = self.get_model_architecture(type(model).__name__)\n",
    "        \n",
    "        return (arch, config, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelHelper` is a `Singleton` (there exists only one instance, and the same instance is returned upon subsequent instantiation requests).  You can get at via the `BLURR_MODEL_HELPER` constant below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = ModelHelper()\n",
    "mh2 = ModelHelper()\n",
    "test_eq(mh, mh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>module</th>\n",
       "      <th>module_part_0</th>\n",
       "      <th>module_part_1</th>\n",
       "      <th>module_part_2</th>\n",
       "      <th>module_part_3</th>\n",
       "      <th>functional_area</th>\n",
       "      <th>arch</th>\n",
       "      <th>model_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaptiveEmbedding</td>\n",
       "      <td>transformers.modeling_transfo_xl</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_transfo_xl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlbertConfig</td>\n",
       "      <td>transformers.configuration_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>configuration_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>configuration</td>\n",
       "      <td>albert</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlbertForMaskedLM</td>\n",
       "      <td>transformers.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>MaskedLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlbertForMultipleChoice</td>\n",
       "      <td>transformers.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>MultipleChoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AlbertForPreTraining</td>\n",
       "      <td>transformers.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>PreTraining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, nan, 'MaskedLM', 'MultipleChoice', 'PreTraining', 'QuestionAnswering', 'SequenceClassification', 'TokenClassification', 'CausalLM', 'Seq2SeqLM', 'LMHead', 'ConditionalGeneration', 'NextSentencePrediction', 'QuestionAnsweringSimple', 'LMHeadModel', 'Classification']\n",
      "\n",
      "['modeling', 'configuration', 'tokenization']\n",
      "\n",
      "[None]\n",
      "\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "display_df(mh._df.head())\n",
    "\n",
    "print(list(mh._df.model_task.unique()))\n",
    "print('')\n",
    "print(list(mh._df.functional_area.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_2.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_3.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide global helper constant\n",
    "\n",
    "Users of this library can simply use `BLURR_MODEL_HELPER` to access all the `ModelHelper` capabilities without having to fetch an instance themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BLURR_MODEL_HELPER = ModelHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_architectures\" class=\"doc_header\"><code>ModelHelper.get_architectures</code><a href=\"__main__.py#L53\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_architectures</code>()\n",
       "\n",
       "Used to get all the architectures supported by your `Transformers` install"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albert', 'auto', 'bart', 'bert', 'bert_japanese', 'camembert', 'ctrl', 'distilbert', 'electra', 'encoder_decoder', 'flaubert', 'gpt2', 'longformer', 'marian', 'mmbt', 'mobilebert', 'openai', 'reformer', 'retribert', 'roberta', 't5', 'transfo_xl', 'utils_base', 'utils_fast', 'xlm', 'xlm_roberta', 'xlnet']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_architectures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create an enum for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_ARCHITECTURES = Enum('HF_ARCHITECTURES', BLURR_MODEL_HELPER.get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#27) [<HF_ARCHITECTURES.albert: 1>,<HF_ARCHITECTURES.auto: 2>,<HF_ARCHITECTURES.bart: 3>,<HF_ARCHITECTURES.bert: 4>,<HF_ARCHITECTURES.bert_japanese: 5>,<HF_ARCHITECTURES.camembert: 6>,<HF_ARCHITECTURES.ctrl: 7>,<HF_ARCHITECTURES.distilbert: 8>,<HF_ARCHITECTURES.electra: 9>,<HF_ARCHITECTURES.encoder_decoder: 10>...]\n"
     ]
    }
   ],
   "source": [
    "print(L(HF_ARCHITECTURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_config\" class=\"doc_header\"><code>ModelHelper.get_config</code><a href=\"__main__.py#L59\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_config</code>(**`arch`**)\n",
       "\n",
       "Used the locate the name of the configuration class for a given architecture"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_config('bert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_tokenizers\" class=\"doc_header\"><code>ModelHelper.get_tokenizers</code><a href=\"__main__.py#L66\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_tokenizers</code>(**`arch`**)\n",
       "\n",
       "Used to get the available huggingface tokenizers for a given architecture. Note: There may be \n",
       "multiple tokenizers and so this returns a list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.tokenization_electra.ElectraTokenizer'>, <class 'transformers.tokenization_electra.ElectraTokenizerFast'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_tokenizers('electra'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_tasks\" class=\"doc_header\"><code>ModelHelper.get_tasks</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_tasks</code>(**`arch`**=*`None`*)\n",
       "\n",
       "Get the type of tasks for which there is a custom model for (*optional: by architecture*). \n",
       "There are a number of customized models built for specific tasks like token classification, \n",
       "question/answering, LM, etc...."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CausalLM', 'Classification', 'ConditionalGeneration', 'LMHead', 'LMHeadModel', 'MaskedLM', 'MultipleChoice', 'NextSentencePrediction', 'PreTraining', 'QuestionAnswering', 'QuestionAnsweringSimple', 'Seq2SeqLM', 'SequenceClassification', 'TokenClassification']\n",
      "\n",
      "['ConditionalGeneration', 'QuestionAnswering', 'SequenceClassification']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_tasks())\n",
    "print('')\n",
    "print(BLURR_MODEL_HELPER.get_tasks('bart'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an enum for tasks as well, one for all tasks and another for tasks available via huggingface's `AutoModel` capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_TASKS_ALL = Enum('HF_TASKS_ALL', BLURR_MODEL_HELPER.get_tasks())\n",
    "HF_TASKS_AUTO = Enum('HF_TASKS_AUTO', BLURR_MODEL_HELPER.get_tasks('auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- all tasks ---\n",
      "(#14) [<HF_TASKS_ALL.CausalLM: 1>,<HF_TASKS_ALL.Classification: 2>,<HF_TASKS_ALL.ConditionalGeneration: 3>,<HF_TASKS_ALL.LMHead: 4>,<HF_TASKS_ALL.LMHeadModel: 5>,<HF_TASKS_ALL.MaskedLM: 6>,<HF_TASKS_ALL.MultipleChoice: 7>,<HF_TASKS_ALL.NextSentencePrediction: 8>,<HF_TASKS_ALL.PreTraining: 9>,<HF_TASKS_ALL.QuestionAnswering: 10>...]\n",
      "\n",
      "--- auto only ---\n",
      "(#9) [<HF_TASKS_AUTO.CausalLM: 1>,<HF_TASKS_AUTO.LMHead: 2>,<HF_TASKS_AUTO.MaskedLM: 3>,<HF_TASKS_AUTO.MultipleChoice: 4>,<HF_TASKS_AUTO.PreTraining: 5>,<HF_TASKS_AUTO.QuestionAnswering: 6>,<HF_TASKS_AUTO.Seq2SeqLM: 7>,<HF_TASKS_AUTO.SequenceClassification: 8>,<HF_TASKS_AUTO.TokenClassification: 9>]\n"
     ]
    }
   ],
   "source": [
    "print('--- all tasks ---')\n",
    "print(L(HF_TASKS_ALL))\n",
    "print('\\n--- auto only ---')\n",
    "print(L(HF_TASKS_AUTO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HF_TASKS_ALL.Classification: 2>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_TASKS_ALL.Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_models\" class=\"doc_header\"><code>ModelHelper.get_models</code><a href=\"__main__.py#L85\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_models</code>(**`arch`**=*`None`*, **`task`**=*`None`*)\n",
       "\n",
       "The transformer models available for use (optional: by architecture | task)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#136) [<class 'transformers.modeling_transfo_xl.AdaptiveEmbedding'>,<class 'transformers.modeling_albert.AlbertForMaskedLM'>,<class 'transformers.modeling_albert.AlbertForMultipleChoice'>,<class 'transformers.modeling_albert.AlbertForPreTraining'>,<class 'transformers.modeling_albert.AlbertForQuestionAnswering'>,<class 'transformers.modeling_albert.AlbertForSequenceClassification'>,<class 'transformers.modeling_albert.AlbertForTokenClassification'>,<class 'transformers.modeling_albert.AlbertModel'>,<class 'transformers.modeling_albert.AlbertPreTrainedModel'>,<class 'transformers.modeling_auto.AutoModel'>...]\n"
     ]
    }
   ],
   "source": [
    "print(L(BLURR_MODEL_HELPER.get_models()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.modeling_bert.BertForMaskedLM'>, <class 'transformers.modeling_bert.BertForMultipleChoice'>, <class 'transformers.modeling_bert.BertForNextSentencePrediction'>, <class 'transformers.modeling_bert.BertForPreTraining'>, <class 'transformers.modeling_bert.BertForQuestionAnswering'>, <class 'transformers.modeling_bert.BertForSequenceClassification'>, <class 'transformers.modeling_bert.BertForTokenClassification'>, <class 'transformers.modeling_bert.BertLMHeadModel'>, <class 'transformers.modeling_bert.BertLayer'>, <class 'transformers.modeling_bert.BertModel'>, <class 'transformers.modeling_bert.BertPreTrainedModel'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_models(arch='bert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.modeling_albert.AlbertForTokenClassification'>, <class 'transformers.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.modeling_bert.BertForTokenClassification'>, <class 'transformers.modeling_camembert.CamembertForTokenClassification'>, <class 'transformers.modeling_distilbert.DistilBertForTokenClassification'>, <class 'transformers.modeling_electra.ElectraForTokenClassification'>, <class 'transformers.modeling_longformer.LongformerForTokenClassification'>, <class 'transformers.modeling_mobilebert.MobileBertForTokenClassification'>, <class 'transformers.modeling_roberta.RobertaForTokenClassification'>, <class 'transformers.modeling_xlm.XLMForTokenClassification'>, <class 'transformers.modeling_xlm_roberta.XLMRobertaForTokenClassification'>, <class 'transformers.modeling_xlnet.XLNetForTokenClassification'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_models(task='TokenClassification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.modeling_bert.BertForTokenClassification'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_models(arch='bert', task='TokenClassification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_classes_for_model\" class=\"doc_header\"><code>ModelHelper.get_classes_for_model</code><a href=\"__main__.py#L94\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_classes_for_model</code>(**`model_name_or_cls`**)\n",
       "\n",
       "Get tokenizers, config, and model for a given model name / class"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_classes_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.configuration_roberta.RobertaConfig'>\n",
      "<class 'transformers.tokenization_roberta.RobertaTokenizer'>\n",
      "<class 'transformers.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "config, tokenizers, model = BLURR_MODEL_HELPER.get_classes_for_model('RobertaForSequenceClassification')\n",
    "\n",
    "print(config)\n",
    "print(tokenizers[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.configuration_distilbert.DistilBertConfig'>\n",
      "<class 'transformers.tokenization_distilbert.DistilBertTokenizer'>\n",
      "<class 'transformers.modeling_distilbert.DistilBertModel'>\n"
     ]
    }
   ],
   "source": [
    "config, tokenizers, model = BLURR_MODEL_HELPER.get_classes_for_model(DistilBertModel)\n",
    "\n",
    "print(config)\n",
    "print(tokenizers[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_model_architecture\" class=\"doc_header\"><code>ModelHelper.get_model_architecture</code><a href=\"__main__.py#L104\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_model_architecture</code>(**`model_name_or_enum`**)\n",
       "\n",
       "Get the architecture for a given model name / enum"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_model_architecture('RobertaForSequenceClassification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for loading pre-trained (configs, tokenizer, model) hugginface classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_hf_objects\" class=\"doc_header\"><code>ModelHelper.get_hf_objects</code><a href=\"__main__.py#L109\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_hf_objects</code>(**`pretrained_model_name_or_path`**, **`task`**=*`None`*, **`config`**=*`None`*, **`tokenizer_cls`**=*`None`*, **`model_cls`**=*`None`*, **`config_kwargs`**=*`{}`*, **`tokenizer_kwargs`**=*`{}`*, **`model_kwargs`**=*`{}`*, **`cache_dir`**=*`None`*)\n",
       "\n",
       "Returns the architecture (str), config (obj), tokenizer (obj), and model (obj) given at minimum a\n",
       "`pre-trained model name or path`. Specify a `task` to ensure the right \"AutoModelFor<task>\" is used to\n",
       "create the model.\n",
       "\n",
       "Optionally, you can pass a config (obj), tokenizer (class), and/or model (class) (along with any \n",
       "related kwargs for each) to get as specific as you want w/r/t what huggingface objects are returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased-finetuned-mrpc and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.configuration_bert.BertConfig'>\n",
      "<class 'transformers.tokenization_bert.BertTokenizer'>\n",
      "<class 'transformers.modeling_bert.BertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "arch, config, tokenizer, model = BLURR_MODEL_HELPER.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                                   task=HF_TASKS_AUTO.LMHead)\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at fmikaelian/flaubert-base-uncased-squad were not used when initializing FlaubertForQuestionAnsweringSimple: ['qa_outputs.start_logits.dense.weight', 'qa_outputs.start_logits.dense.bias', 'qa_outputs.end_logits.dense_0.weight', 'qa_outputs.end_logits.dense_0.bias', 'qa_outputs.end_logits.LayerNorm.weight', 'qa_outputs.end_logits.LayerNorm.bias', 'qa_outputs.end_logits.dense_1.weight', 'qa_outputs.end_logits.dense_1.bias', 'qa_outputs.answer_class.dense_0.weight', 'qa_outputs.answer_class.dense_0.bias', 'qa_outputs.answer_class.dense_1.weight']\n",
      "- This IS expected if you are initializing FlaubertForQuestionAnsweringSimple from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertForQuestionAnsweringSimple from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaubertForQuestionAnsweringSimple were not initialized from the model checkpoint at fmikaelian/flaubert-base-uncased-squad and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flaubert\n",
      "<class 'transformers.tokenization_flaubert.FlaubertTokenizer'>\n",
      "<class 'transformers.configuration_flaubert.FlaubertConfig'>\n",
      "<class 'transformers.modeling_flaubert.FlaubertForQuestionAnsweringSimple'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = BLURR_MODEL_HELPER.get_hf_objects(\"fmikaelian/flaubert-base-uncased-squad\",\n",
    "                                                                   task=HF_TASKS_AUTO.QuestionAnswering)\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertForNextSentencePrediction: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForNextSentencePrediction were not initialized from the model checkpoint at bert-base-cased-finetuned-mrpc and are newly initialized: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.tokenization_bert.BertTokenizer'>\n",
      "<class 'transformers.configuration_bert.BertConfig'>\n",
      "<class 'transformers.modeling_bert.BertForNextSentencePrediction'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = BLURR_MODEL_HELPER.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                                   config=None,\n",
    "                                                                   tokenizer_cls=BertTokenizer, \n",
    "                                                                   model_cls=BertForNextSentencePrediction)\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-language-modeling.ipynb.\n",
      "Converted 01c_data-question-answering.ipynb.\n",
      "Converted 01d_data-token-classification.ipynb.\n",
      "Converted 01e_data-text-generation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02_training-summarization.ipynb.\n",
      "Converted 02a_modeling-language-modeling.ipynb.\n",
      "Converted 02c_modeling-question-answering.ipynb.\n",
      "Converted 02d_modeling-token-classification.ipynb.\n",
      "Converted 02e_modeling-text-generation.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
