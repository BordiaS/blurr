{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Various utility functions used by the blurr package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys, inspect\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def str_to_class(classname):\n",
    "    \"converts string representation to class\"\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Singleton:\n",
    "    def __init__(self,cls):\n",
    "        self._cls, self._instance = cls, None\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._instance == None: self._instance = self._cls(*args, **kwargs)\n",
    "        return self._instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Singleton` functions as python decorator.  Use this above any class to turn that class into a singleton (see [here](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html) for more info on the singleton pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Singleton\n",
    "class TestSingleton: pass\n",
    "\n",
    "a = TestSingleton()\n",
    "b = TestSingleton()\n",
    "test_eq(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Singleton\n",
    "class ModelHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # get hf classes (tokenizers, configs, models, etc...)\n",
    "        transformer_classes = inspect.getmembers(sys.modules[__name__], \n",
    "                                                 lambda member: inspect.isclass(member)\n",
    "                                                 and member.__module__.startswith('transformers.'))\n",
    "        \n",
    "        # build a df that we can query against to get various transformers objects/info\n",
    "        self._df = pd.DataFrame(transformer_classes, columns=['class_name', 'class_location'])\n",
    "        \n",
    "        # add the module each class is included in\n",
    "        self._df['module'] = self._df.class_location.apply(lambda v: v.__module__)\n",
    "        \n",
    "        # remove class_location (don't need it anymore)\n",
    "        self._df.drop(labels=['class_location'], axis=1, inplace=True)\n",
    "        \n",
    "        # break up the module into separate cols\n",
    "        module_parts_df = self._df.module.str.split(\".\", n = -1, expand = True) \n",
    "        for i in range(len(module_parts_df.columns)):\n",
    "            self._df[f'module_part_{i}'] = module_parts_df[i]\n",
    "\n",
    "        # using module part 1, break up the functional area and arch into separate cols\n",
    "        module_part_1_df = self._df.module_part_1.str.split(\"_\", n = 1, expand = True) \n",
    "        self._df[['functional_area', 'arch']] = module_part_1_df\n",
    "        \n",
    "        # if functional area = modeling, pull out the task it is built for\n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.split('For', n=1, expand=True)\n",
    "        \n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'For' + model_type_df[1].astype(str), \n",
    "                                    model_type_df[1])\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        \n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.split('With', n=1, expand=True)\n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'With' + model_type_df[1].astype(str), \n",
    "                                    self._df[(self._df.functional_area == 'modeling')].model_task)\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        \n",
    "        # look at what we're going to remove (use to verify we're just getting rid of stuff we want too)\n",
    "        # df[~df['hf_class_type'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "        # only need these 3 functional areas for our querying purposes\n",
    "        self._df = self._df[self._df['functional_area'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "    def get_architectures(self): \n",
    "        \"\"\"Used to get all the architectures supported by your `Transformers` install\"\"\"\n",
    "        return self._df[(self._df.arch.notna()) & (self._df.arch != None)].arch.unique().tolist()\n",
    "    \n",
    "    def get_config(self, arch): \n",
    "        \"\"\"Used the locate the name of the configuration class for a given architecture\"\"\"\n",
    "        return self._df[(self._df.functional_area == 'configuration') & (self._df.arch == arch)].class_name.values[0]\n",
    "    \n",
    "    def get_tokenizers(self, arch): \n",
    "        \"\"\"Used to get the available huggingface tokenizers for a given architecture. Note: There may be \n",
    "        multiple tokenizers and so this returns a list.\n",
    "        \"\"\"\n",
    "        return self._df[(self._df.functional_area == 'tokenization') & (self._df.arch == arch)].class_name.values\n",
    "    \n",
    "    def get_tasks(self, arch=None): \n",
    "        \"\"\"Get the type of tasks for which there is a custom model for (*optional: by architecture*). \n",
    "        There are a number of customized models built for specific tasks like token classification, \n",
    "        question/answering, LM, etc....\n",
    "        \"\"\"\n",
    "        query = ['model_task.notna()']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "\n",
    "        return self._df.query(' & '.join(query), engine='python').model_task.unique().tolist()\n",
    "    \n",
    "    def get_models(self, arch=None, task=None):\n",
    "        \"\"\"The transformer models available for use (optional: by architecture | task)\"\"\"\n",
    "        query = ['functional_area == \"modeling\"']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "        if (task): query.append(f'model_task == \"{task}\"')\n",
    "\n",
    "        return self._df.query(' & '.join(query)).class_name.tolist()\n",
    "    \n",
    "    def get_classes_for_model(self, model_name_or_enum):\n",
    "        \"\"\"Get tokenizers, config, and model for a given model name / enum\"\"\"\n",
    "        model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "\n",
    "        meta = self._df[self._df.class_name == model_name]\n",
    "        tokenizers = self.get_tokenizers(meta.arch.values[0])\n",
    "        config = self.get_config(meta.arch.values[0])\n",
    "\n",
    "        return ([str_to_class(tok) for tok in tokenizers], str_to_class(config), str_to_class(model_name))\n",
    "    \n",
    "    def get_model_architecture(self, model_name_or_enum):\n",
    "        \"\"\"Get the architecture for a given model name / enum\"\"\"\n",
    "        model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "        return self._df[self._df.class_name == model_name].arch.values[0]\n",
    "    \n",
    "    def get_auto_hf_objects(self, pretrained_model_name_or_path, task, config=None):\n",
    "        \"\"\"Returns the architecture (str), tokenizer (obj), config (obj), and model (obj) \n",
    "        given a known pre-trained model name or path and a task using Hugginface `AutoModel` capabilities.  \n",
    "        If a `config` is passed in, it will be  used when building the model, else the default configuration \n",
    "        will be used (e.g., `AutoConfig.from_pretrained(...)`)\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "        config = AutoConfig.from_pretrained(pretrained_model_name_or_path) if (config is None) else config\n",
    "\n",
    "        model = str_to_class(f'AutoModel{task.name}').from_pretrained(pretrained_model_name_or_path, \n",
    "                                                                      config=config)\n",
    "        arch = self.get_model_architecture(type(model).__name__)\n",
    "\n",
    "        return (arch, tokenizer, config, model)\n",
    "    \n",
    "    def get_hf_objects(self, pretrained_model_name_or_path, tokenizer_cls, model_cls, config=None): \n",
    "        \"\"\"Returns the architecture (str), tokenizer (class), config (class), and model (class) \n",
    "        given a known pre-trained model name or path, a tokenizer class, and model class.  If a `config` object \n",
    "        is passed in, it will be used when building the model, else the default configuration will be used.\n",
    "        \"\"\"\n",
    "        tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "        if (config is None):\n",
    "            model = str_to_class(model_cls.name).from_pretrained(pretrained_model_name_or_path)\n",
    "            config = model.config\n",
    "        else:\n",
    "            model = str_to_class(model_cls.name).from_pretrained(pretrained_model_name_or_path, config=config)\n",
    "\n",
    "        arch = self.get_model_architecture(type(model).__name__)\n",
    "\n",
    "        return (arch, tokenizer, config, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelHelper` is a `Singleton` (there exists only one instance, and the same instance is returned upon subsequent instantiation requests).  You can get at via the `BLURR_MODEL_HELPER` constant below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = ModelHelper()\n",
    "mh2 = ModelHelper()\n",
    "test_eq(mh, mh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>module</th>\n",
       "      <th>module_part_0</th>\n",
       "      <th>module_part_1</th>\n",
       "      <th>module_part_2</th>\n",
       "      <th>module_part_3</th>\n",
       "      <th>functional_area</th>\n",
       "      <th>arch</th>\n",
       "      <th>model_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaptiveEmbedding</td>\n",
       "      <td>transformers.modeling_transfo_xl</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_transfo_xl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlbertConfig</td>\n",
       "      <td>transformers.configuration_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>configuration_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>configuration</td>\n",
       "      <td>albert</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlbertForMaskedLM</td>\n",
       "      <td>transformers.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>ForMaskedLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlbertForPreTraining</td>\n",
       "      <td>transformers.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>ForPreTraining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AlbertForQuestionAnswering</td>\n",
       "      <td>transformers.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>ForQuestionAnswering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, nan, 'ForMaskedLM', 'ForPreTraining', 'ForQuestionAnswering', 'ForSequenceClassification', 'ForTokenClassification', 'ForMultipleChoice', 'WithLMHead', 'ForConditionalGeneration', 'ForNextSentencePrediction', 'ForQuestionAnsweringSimple', 'WithLMHeadModel', 'ForClassification']\n",
      "\n",
      "['modeling', 'configuration', 'tokenization']\n",
      "\n",
      "[None]\n",
      "\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "display_df(mh._df.head())\n",
    "\n",
    "print(list(mh._df.model_task.unique()))\n",
    "print('')\n",
    "print(list(mh._df.functional_area.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_2.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_3.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide global helper constant\n",
    "\n",
    "Users of this library can simply use `BLURR_MODEL_HELPER` to access all the `ModelHelper` capabilities without having to fetch an instance themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BLURR_MODEL_HELPER = ModelHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_architectures\" class=\"doc_header\"><code>ModelHelper.get_architectures</code><a href=\"__main__.py#L51\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_architectures</code>()\n",
       "\n",
       "Used to get all the architectures supported by your `Transformers` install"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transfo_xl', 'albert', 'auto', 'bart', 'bert', 'bert_japanese', 'ctrl', 'camembert', 'utils', 'distilbert', 'electra', 'encoder_decoder', 'flaubert', 'gpt2', 'mmbt', 'marian', 'openai', 'reformer', 'roberta', 't5', 'xlm', 'xlm_roberta', 'xlnet']\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_architectures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create an enum for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_ARCHITECTURES = Enum('HF_ARCHITECTURES', BLURR_MODEL_HELPER.get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#23) [<HF_ARCHITECTURES.transfo_xl: 1>,<HF_ARCHITECTURES.albert: 2>,<HF_ARCHITECTURES.auto: 3>,<HF_ARCHITECTURES.bart: 4>,<HF_ARCHITECTURES.bert: 5>,<HF_ARCHITECTURES.bert_japanese: 6>,<HF_ARCHITECTURES.ctrl: 7>,<HF_ARCHITECTURES.camembert: 8>,<HF_ARCHITECTURES.utils: 9>,<HF_ARCHITECTURES.distilbert: 10>...]\n"
     ]
    }
   ],
   "source": [
    "print(L(HF_ARCHITECTURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_config\" class=\"doc_header\"><code>ModelHelper.get_config</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_config</code>(**`arch`**)\n",
       "\n",
       "Used the locate the name of the configuration class for a given architecture"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_config('bert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_tokenizers\" class=\"doc_header\"><code>ModelHelper.get_tokenizers</code><a href=\"__main__.py#L59\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_tokenizers</code>(**`arch`**)\n",
       "\n",
       "Used to get the available huggingface tokenizers for a given architecture. Note: There may be \n",
       "multiple tokenizers and so this returns a list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ElectraTokenizer' 'ElectraTokenizerFast']\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_tokenizers('electra'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_tasks\" class=\"doc_header\"><code>ModelHelper.get_tasks</code><a href=\"__main__.py#L65\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_tasks</code>(**`arch`**=*`None`*)\n",
       "\n",
       "Get the type of tasks for which there is a custom model for (*optional: by architecture*). \n",
       "There are a number of customized models built for specific tasks like token classification, \n",
       "question/answering, LM, etc...."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ForMaskedLM', 'ForPreTraining', 'ForQuestionAnswering', 'ForSequenceClassification', 'ForTokenClassification', 'ForMultipleChoice', 'WithLMHead', 'ForConditionalGeneration', 'ForNextSentencePrediction', 'ForQuestionAnsweringSimple', 'WithLMHeadModel', 'ForClassification']\n",
      "\n",
      "['ForConditionalGeneration', 'ForSequenceClassification']\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_tasks())\n",
    "print('')\n",
    "print(mh.get_tasks('bart'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an enum for tasks as well, one for all tasks and another for tasks available via huggingface's `AutoModel` capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_TASKS_ALL = Enum('HF_TASKS_ALL', BLURR_MODEL_HELPER.get_tasks())\n",
    "HF_TASKS_AUTO = Enum('HF_TASKS_AUTO', BLURR_MODEL_HELPER.get_tasks('auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- all tasks ---\n",
      "(#12) [<HF_TASKS_ALL.ForMaskedLM: 1>,<HF_TASKS_ALL.ForPreTraining: 2>,<HF_TASKS_ALL.ForQuestionAnswering: 3>,<HF_TASKS_ALL.ForSequenceClassification: 4>,<HF_TASKS_ALL.ForTokenClassification: 5>,<HF_TASKS_ALL.ForMultipleChoice: 6>,<HF_TASKS_ALL.WithLMHead: 7>,<HF_TASKS_ALL.ForConditionalGeneration: 8>,<HF_TASKS_ALL.ForNextSentencePrediction: 9>,<HF_TASKS_ALL.ForQuestionAnsweringSimple: 10>...]\n",
      "\n",
      "--- auto only ---\n",
      "(#6) [<HF_TASKS_AUTO.ForMultipleChoice: 1>,<HF_TASKS_AUTO.ForPreTraining: 2>,<HF_TASKS_AUTO.ForQuestionAnswering: 3>,<HF_TASKS_AUTO.ForSequenceClassification: 4>,<HF_TASKS_AUTO.ForTokenClassification: 5>,<HF_TASKS_AUTO.WithLMHead: 6>]\n"
     ]
    }
   ],
   "source": [
    "print('--- all tasks ---')\n",
    "print(L(HF_TASKS_ALL))\n",
    "print('\\n--- auto only ---')\n",
    "print(L(HF_TASKS_AUTO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HF_TASKS_ALL.ForClassification: 12>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_TASKS_ALL.ForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_models\" class=\"doc_header\"><code>ModelHelper.get_models</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_models</code>(**`arch`**=*`None`*, **`task`**=*`None`*)\n",
       "\n",
       "The transformer models available for use (optional: by architecture | task)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#104) ['AdaptiveEmbedding','AlbertForMaskedLM','AlbertForPreTraining','AlbertForQuestionAnswering','AlbertForSequenceClassification','AlbertForTokenClassification','AlbertModel','AlbertPreTrainedModel','AutoModel','AutoModelForMultipleChoice'...]\n"
     ]
    }
   ],
   "source": [
    "print(L(mh.get_models()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BertForMaskedLM', 'BertForMultipleChoice', 'BertForNextSentencePrediction', 'BertForPreTraining', 'BertForQuestionAnswering', 'BertForSequenceClassification', 'BertForTokenClassification', 'BertLayer', 'BertModel', 'BertPreTrainedModel']\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_models(arch='bert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlbertForTokenClassification', 'AutoModelForTokenClassification', 'BertForTokenClassification', 'CamembertForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'RobertaForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLNetForTokenClassification']\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_models(task='ForTokenClassification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BertForTokenClassification']\n"
     ]
    }
   ],
   "source": [
    "print(mh.get_models(arch='bert', task='ForTokenClassification'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create another enum for the huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_MODELS = Enum('HF_MODELS', BLURR_MODEL_HELPER.get_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#104) [<HF_MODELS.AdaptiveEmbedding: 1>,<HF_MODELS.AlbertForMaskedLM: 2>,<HF_MODELS.AlbertForPreTraining: 3>,<HF_MODELS.AlbertForQuestionAnswering: 4>,<HF_MODELS.AlbertForSequenceClassification: 5>,<HF_MODELS.AlbertForTokenClassification: 6>,<HF_MODELS.AlbertModel: 7>,<HF_MODELS.AlbertPreTrainedModel: 8>,<HF_MODELS.AutoModel: 9>,<HF_MODELS.AutoModelForMultipleChoice: 10>...]\n"
     ]
    }
   ],
   "source": [
    "print(L(HF_MODELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_classes_for_model\" class=\"doc_header\"><code>ModelHelper.get_classes_for_model</code><a href=\"__main__.py#L83\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_classes_for_model</code>(**`model_name_or_enum`**)\n",
       "\n",
       "Get tokenizers, config, and model for a given model name / enum"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_classes_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_roberta.RobertaTokenizer'>\n",
      "<class 'transformers.configuration_roberta.RobertaConfig'>\n",
      "<class 'transformers.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "tokenizers, config, model = mh.get_classes_for_model('RobertaForSequenceClassification')\n",
    "\n",
    "print(tokenizers[0])\n",
    "print(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_distilbert.DistilBertTokenizer'>\n",
      "<class 'transformers.configuration_distilbert.DistilBertConfig'>\n",
      "<class 'transformers.modeling_distilbert.DistilBertModel'>\n"
     ]
    }
   ],
   "source": [
    "tokenizers, config, model = mh.get_classes_for_model(HF_MODELS.DistilBertModel)\n",
    "\n",
    "print(tokenizers[0])\n",
    "print(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_model_architecture\" class=\"doc_header\"><code>ModelHelper.get_model_architecture</code><a href=\"__main__.py#L93\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_model_architecture</code>(**`model_name_or_enum`**)\n",
       "\n",
       "Get the architecture for a given model name / enum"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh.get_model_architecture('RobertaForSequenceClassification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for loading pre-trained (configs, tokenizer, model) hugginface classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_auto_hf_objects\" class=\"doc_header\"><code>ModelHelper.get_auto_hf_objects</code><a href=\"__main__.py#L98\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_auto_hf_objects</code>(**`pretrained_model_name_or_path`**, **`task`**, **`config`**=*`None`*)\n",
       "\n",
       "Returns the architecture (str), tokenizer (obj), config (obj), and model (obj) \n",
       "given a known pre-trained model name or path and a task using Hugginface `AutoModel` capabilities.  \n",
       "If a `config` is passed in, it will be  used when building the model, else the default configuration \n",
       "will be used (e.g., `AutoConfig.from_pretrained(...)`)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_auto_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.tokenization_bert.BertTokenizer'>\n",
      "<class 'transformers.configuration_bert.BertConfig'>\n",
      "<class 'transformers.modeling_bert.BertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = mh.get_auto_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                        task=HF_TASKS_AUTO.WithLMHead)\n",
    "\n",
    "print(arch)\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flaubert\n",
      "<class 'transformers.tokenization_flaubert.FlaubertTokenizer'>\n",
      "<class 'transformers.configuration_flaubert.FlaubertConfig'>\n",
      "<class 'transformers.modeling_flaubert.FlaubertForQuestionAnsweringSimple'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = mh.get_auto_hf_objects(\"fmikaelian/flaubert-base-uncased-squad\",\n",
    "                                                        task=HF_TASKS_AUTO.ForQuestionAnswering)\n",
    "\n",
    "print(arch)\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ModelHelper.get_hf_objects\" class=\"doc_header\"><code>ModelHelper.get_hf_objects</code><a href=\"__main__.py#L113\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ModelHelper.get_hf_objects</code>(**`pretrained_model_name_or_path`**, **`tokenizer_cls`**, **`model_cls`**, **`config`**=*`None`*)\n",
       "\n",
       "Returns the architecture (str), tokenizer (class), config (class), and model (class) \n",
       "given a known pre-trained model name or path, a tokenizer class, and model class.  If a `config` object \n",
       "is passed in, it will be used when building the model, else the default configuration will be used."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.tokenization_bert.BertTokenizer'>\n",
      "<class 'transformers.configuration_bert.BertConfig'>\n",
      "<class 'transformers.modeling_bert.BertForNextSentencePrediction'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = mh.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                   tokenizer_cls=BertTokenizer, \n",
    "                                                   config=None,\n",
    "                                                   model_cls=HF_MODELS.BertForNextSentencePrediction)\n",
    "print(arch)\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Marker classes\n",
    "\n",
    "These classes are provided for use with the @typedispatched `build_hf_input` in the data module.  This gives you the ability to use this new feature in fastai to alter the base huggingface tokenization strategy provided in the framework, with something particular to one of these tasks (and optionally, the type of huggingface tokenizer you are using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ForMaskedLMTask: pass\n",
    "class ForQuestionAnsweringTask: pass\n",
    "class ForSequenceClassificationTask: pass\n",
    "class ForTokenClassificationTask: pass\n",
    "class ForPreTrainingTask: pass\n",
    "class WithLMHeadTask: pass\n",
    "class ForConditionalGenerationTask: pass\n",
    "class ForMultipleChoiceTask: pass\n",
    "class ForNextSentencePredictionTask: pass\n",
    "class ForQuestionAnsweringSimpleTask: pass\n",
    "class WithLMHeadModelTask: pass\n",
    "class ForClassificationTask: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-language-modeling.ipynb.\n",
      "Converted 01c_data-question-answering.ipynb.\n",
      "Converted 01d_data-token-classification.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-language-modeling.ipynb.\n",
      "Converted 02c_modeling-question-answering.ipynb.\n",
      "Converted 02d_modeling-token-classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
