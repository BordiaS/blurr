{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_batch_tfm = HF_TokenClassBatchTransform(hf_arch, hf_tokenizer)\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, hf_batch_tfm=hf_batch_tfm, max_length=128, is_pretokenized=True,\n",
    "                 tok_kwargs={ 'return_special_tokens_mask': True }), \n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Diese', 'O'), ('erhielten', 'O'), ('jedoch', 'O'), ('bereits', 'O'), ('den', 'O'), ('Ans', 'O'), ('in', 'O'), ('oz', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Es', 'O'), ('geht', 'O'), ('dar', 'O'), (',', 'O'), ('die', 'O'), ('Bauten', 'O'), ('zu', 'O'), ('s', 'O'), (',', 'O'), ('die', 'O'), ('bei', 'O'), ('für', 'O'), ('ihre', 'O'), ('Bau', 'O'), ('sind', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self, 'tok_metrics, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in tok_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the target's HF_TokenizerTransform (used for rouge metrics)\n",
    "        hf_textblock_tfm = self.dls.tfms[0]\n",
    "        self.hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = self.dls.tfms[1].ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = hf_textblock_tfm.kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "        \n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]\n",
    "    \n",
    "    \n",
    "    # --- batch begin/after phases ---\n",
    "    def begin_fit(self): self.setup()\n",
    "    def begin_epoch(self): self.results = []\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if (self.model.training): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def after_validate(self):\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        \n",
    "        if ('accuracy' in self.custom_metrics_dict.keys()): \n",
    "            self.custom_metrics_dict['accuracy'] = seq_metrics.accuracy_score(targs, preds)\n",
    "        if ('precision' in self.custom_metrics_dict.keys()): \n",
    "            self.custom_metrics_dict['precision'] = seq_metrics.precision_score(targs, preds)\n",
    "        if ('recall' in self.custom_metrics_dict.keys()): \n",
    "            self.custom_metrics_dict['recall'] = seq_metrics.recall_score(targs, preds)\n",
    "        if ('f1' in self.custom_metrics_dict.keys()): \n",
    "            self.custom_metrics_dict['f1'] = seq_metrics.f1_score(targs, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam, decouple_wd=True),\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 128, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 128]), 4, torch.Size([4, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 18]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0003019951749593019, lr_steep=1.4454397387453355e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.285252</td>\n",
       "      <td>0.200453</td>\n",
       "      <td>0.958522</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>0.584034</td>\n",
       "      <td>0.631818</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.128875</td>\n",
       "      <td>0.125619</td>\n",
       "      <td>0.965169</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.070573</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>0.969689</td>\n",
       "      <td>0.709544</td>\n",
       "      <td>0.718487</td>\n",
       "      <td>0.713987</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8), cbs=[HF_TokenClassMetricsCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, learner=None, \n",
    "                 ctxs=None, max_n=6, **kwargs):    \n",
    "    # grab tokenizer\n",
    "    hf_textblock_tfm = learner.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x[0], y, samples, outs):\n",
    "        inp_trg_preds = [ (hf_tokenizer.ids_to_tokens[tok_id.item()], lbl_id.item(), pred_lbl) \n",
    "                         for tok_id, lbl_id, pred_lbl in zip(inp, trg, ast.literal_eval(pred[0])) \n",
    "                         if (tok_id not in hf_tokenizer.all_special_ids) and lbl_id != -100 ]\n",
    "        \n",
    "        res.append([f'{[ (itp[0], lbl, itp[2]) for itp, lbl in zip(inp_trg_preds, ast.literal_eval(sample[1])) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Ich', 'O', 'O'), ('konnte', 'O', 'O'), ('über', 'O', 'O'), ('diesen', 'O', 'O'), ('Film', 'O', 'O'), ('nicht', 'O', 'O'), ('lac', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Er', 'O', 'O'), ('heir', 'O', 'O'), ('1994', 'O', 'O'), ('die', 'O', 'O'), ('Schauspielerin', 'O', 'O'), ('Marie', 'B-PER', 'B-PER'), ('Pr', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('mit', 'O', 'O'), ('der', 'O', 'O'), ('er', 'O', 'O'), ('zwei', 'O', 'O'), ('Söhne', 'O', 'O'), (',', 'O', 'O'), ('Mateo', 'B-PER', 'B-PER'), ('und', 'O', 'O'), ('Paolo', 'B-PER', 'B-PER'), (',', 'O', 'O'), ('hat', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict_tokens(self:Learner, inp, **kargs):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
    "    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input\n",
    "    \"\"\"\n",
    "    pred_lbls, pred_lbl_ids, probs = self.blurr_predict(inp)\n",
    "\n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_textblock_tfm.kwargs\n",
    "    \n",
    "    # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "    # return\n",
    "    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp ]\n",
    "    \n",
    "    # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "    # (e.g., [CLS], [SEP], etc...)\n",
    "    res = hf_tokenizer(inp, None, \n",
    "                       max_length=hf_textblock_tfm.max_length,\n",
    "                       padding=hf_textblock_tfm.padding,\n",
    "                       truncation=hf_textblock_tfm.truncation,\n",
    "                       is_pretokenized=hf_textblock_tfm.is_pretokenized,\n",
    "                       **tok_kwargs)\n",
    "\n",
    "    special_toks_msk = L(res['special_tokens_mask'])\n",
    "    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "    # using the indexes to the actual tokens, get that info from the results returned above\n",
    "    pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "    actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "    # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "    offset = 0\n",
    "    raw_trg_idxs = []\n",
    "    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "        raw_trg_idxs.append(idx+offset)\n",
    "        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`inp`**, **\\*\\*`kargs`**)\n",
       "\n",
       "Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
       "get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "print([(tok, lbl) for tok,lbl in zip(res[0],res[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-language-modeling.ipynb.\n",
      "Converted 01c_data-question-answering.ipynb.\n",
      "Converted 01d_data-token-classification.ipynb.\n",
      "Converted 01e_data-text-generation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-language-modeling.ipynb.\n",
      "Converted 02c_modeling-question-answering.ipynb.\n",
      "Converted 02d_modeling-token-classification.ipynb.\n",
      "Converted 02e_modeling-text-generation.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
