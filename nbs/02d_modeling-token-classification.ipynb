{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, task=TokenClassificationTask(), max_seq_len=128),\n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Sen', 'O'), ('Ex', 'B-ORG'), ('Mo', 'I-ORG'), ('\"', 'O'), ('buy', 'O'), ('\"', 'O'), ('Paris', 'B-LOC'), ('(', 'O'), ('akt', 'B-ORG'), ('AG', 'I-ORG'), (')', 'O'), ('-', 'O'), (':', 'O'), ('Ay', 'B-PER'), ('de', 'I-PER'), (',', 'O'), ('Ana', 'O'), ('der', 'O'), ('Société', 'B-ORG'), ('Général', 'I-ORG'), (',', 'O'), ('st', 'O'), ('die', 'O'), ('Akt', 'O'), ('des', 'O'), ('US', 'B-LOCderiv'), ('Unternehmens', 'O'), ('Ex', 'B-ORG'), ('Mo', 'I-ORG'), ('(', 'O'), ('IS', 'O'), ('US', 'O'), ('WK', 'O'), ('852', 'O'), (')', 'O'), ('mit', 'O'), ('\"', 'O'), ('buy', 'O'), ('\"', 'O'), ('ein', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Düsseldorf', 'B-LOC'), ('(', 'O'), ('akt', 'B-ORG'), ('AG', 'I-ORG'), (')', 'O'), (':', 'O'), ('-', 'O'), ('Nachdem', 'O'), ('der', 'O'), ('Sp', 'O'), ('auf', 'O'), ('ein', 'O'), ('neues', 'O'), ('All', 'O'), ('(', 'O'), ('1', 'O'), ('USD', 'B-OTH'), (')', 'O'), ('gel', 'O'), ('ist', 'O'), (',', 'O'), ('g', 'O'), ('sich', 'O'), ('der', 'O'), ('Gold', 'O'), ('derzeit', 'O'), ('eine', 'O'), ('Vers', 'O'), (',', 'O'), ('so', 'O'), ('die', 'O'), ('Ana', 'O'), ('von', 'O'), ('HS', 'B-ORG'), ('Tri', 'I-ORG'), ('Bu', 'I-ORG'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        if (not self.do_setup): return    \n",
    "\n",
    "        # one time setup code here.\n",
    "        self.hf_tokenizer = self.dls.tfms[0].hf_tokenizer\n",
    "        self.ignore_label_token_id = self.dls.tfms[1].ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        \n",
    "        self.results = []\n",
    "        self.seq_accuracy, self.seq_precision, self.seq_recall, self.seq_f1 = 0.,0.,0.,0.\n",
    "        \n",
    "        seq_metrics = L(ValueMetric(self.seq_accuracy_value, 'accuracy'), \n",
    "                        ValueMetric(self.seq_precision_value, 'precision'),\n",
    "                        ValueMetric(self.seq_recall_value, 'recall'),\n",
    "                        ValueMetric(self.seq_f1_value, 'f1'))\n",
    "        \n",
    "        self.learn.metrics = self.learn.metrics + seq_metrics\n",
    "        self.do_setup = False\n",
    "        \n",
    "    # these HAVE to be functions\n",
    "    def seq_accuracy_value(self): return self.seq_accuracy\n",
    "    def seq_precision_value(self): return self.seq_precision\n",
    "    def seq_recall_value(self): return self.seq_recall\n",
    "    def seq_f1_value(self): return self.seq_f1\n",
    "    \n",
    "    # ----callbacks ----\n",
    "    def begin_fit(self): self.setup()\n",
    "    def begin_epoch(self): self.results = []\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if (self.model.training): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "    def after_validate(self):\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        \n",
    "        accuracy = seq_metrics.accuracy_score(targs, preds)\n",
    "        precision = seq_metrics.precision_score(targs, preds)\n",
    "        recall = seq_metrics.recall_score(targs, preds)\n",
    "        f1 = seq_metrics.f1_score(targs, preds)\n",
    "        \n",
    "        self.seq_accuracy, self.seq_precision, self.seq_recall, self.seq_f1 = accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam, decouple_wd=True),\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 128, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 128]), 4, torch.Size([4, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][0].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 18]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0007585775572806596, lr_steep=2.511886486900039e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.320077</td>\n",
       "      <td>0.194982</td>\n",
       "      <td>0.943866</td>\n",
       "      <td>0.479769</td>\n",
       "      <td>0.683128</td>\n",
       "      <td>0.563667</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.112583</td>\n",
       "      <td>0.091626</td>\n",
       "      <td>0.965772</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.748971</td>\n",
       "      <td>0.706796</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084222</td>\n",
       "      <td>0.089409</td>\n",
       "      <td>0.967963</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.731429</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8), cbs=[HF_TokenClassMetricsCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, hf_tokenizer, skip_special_tokens=True,\n",
    "                 ctxs=None, max_n=6, **kwargs):        \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x[0], y, samples, outs):\n",
    "        inp_trg_preds = [ (hf_tokenizer.ids_to_tokens[tok_id.item()], lbl_id.item(), pred_lbl) \n",
    "                         for tok_id, lbl_id, pred_lbl in zip(inp, trg, ast.literal_eval(pred[0])) \n",
    "                         if (tok_id not in hf_tokenizer.all_special_ids) and lbl_id != -100 ]\n",
    "        \n",
    "        res.append([f'{[ (itp[0], lbl, itp[2]) for itp, lbl in zip(inp_trg_preds, ast.literal_eval(sample[1])) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('He', 'B-OTH', 'I-PER'), ('et', 'I-OTH', 'I-ORG'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'O'), ('1994', 'O', 'O'), (')', 'O', 'O'), ('S', 'O', 'O'), ('593', 'O', 'O'), ('Win', 'B-OTH', 'I-PER'), ('&amp;', 'I-OTH', 'O'), ('Sei', 'I-OTH', 'I-PER'), ('et', 'I-OTH', 'I-ORG'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'O'), ('1998', 'O', 'O'), (')', 'O', 'O'), ('S', 'O', 'O'), ('32', 'O', 'O'), ('In', 'O', 'O'), ('noch', 'O', 'O'), ('andere', 'O', 'O'), ('Falk', 'O', 'O'), (',', 'O', 'O'), ('wie', 'O', 'O'), ('der', 'O', 'O'), ('Afrikan', 'B-LOCderiv', 'B-LOCderiv'), ('Baum', 'O', 'O'), ('(', 'O', 'O'), ('Falco', 'O', 'B-OTH'), ('cu', 'O', 'I-OTH'), (')', 'O', 'O'), ('oder', 'O', 'O'), ('der', 'O', 'O'), ('Mala', 'O', 'B-LOCderiv'), ('(', 'O', 'O'), ('Falco', 'O', 'B-OTH'), ('server', 'O', 'I-OTH'), (')', 'O', 'O'), ('dieser', 'O', 'O'), ('Gruppe', 'O', 'O'), ('zu', 'O', 'O'), ('sind', 'O', 'O'), (',', 'O', 'O'), ('ist', 'O', 'O'), ('Gegen', 'O', 'O'), ('der', 'O', 'O'), ('Forschung', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Der', 'O', 'O'), ('28', 'O', 'O'), ('und', 'O', 'O'), ('sein', 'O', 'O'), ('Team', 'O', 'O'), (',', 'O', 'O'), ('zu', 'O', 'O'), ('dem', 'O', 'O'), ('auch', 'O', 'O'), ('Karla', 'B-PER', 'B-PER'), ('Bar', 'I-PER', 'I-PER'), ('gehört', 'O', 'O'), (',', 'O', 'O'), ('demo', 'O', 'O'), ('im', 'O', 'O'), ('ge', 'O', 'O'), ('\"', 'O', 'O'), ('La', 'B-ORG', 'B-LOC'), ('Ti', 'I-ORG', 'I-OTH'), ('Reg', 'I-ORG', 'O'), ('Zentrum', 'I-ORG', 'O'), ('\"', 'O', 'O'), ('westlich', 'O', 'O'), ('von', 'O', 'O'), ('Puerto', 'B-LOC', 'B-LOC'), ('Viejo', 'I-LOC', 'I-LOC'), ('de', 'I-LOC', 'I-LOC'), ('Sara', 'I-LOC', 'I-LOC'), (',', 'O', 'O'), ('wie', 'O', 'O'), ('die', 'O', 'O'), ('Ur', 'O', 'O'), ('vor', 'O', 'O'), ('500', 'O', 'O'), ('Jahren', 'O', 'O'), ('das', 'O', 'O'), ('Get', 'O', 'O'), ('und', 'O', 'O'), ('R', 'O', 'O'), ('produzierte', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "res = learn.predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict_tokens(self:Learner, inp, **kargs):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
    "    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input\n",
    "    \"\"\"\n",
    "    pred_lbls, pred_lbl_ids, probs = self.predict(inp)\n",
    "    \n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    add_prefix_space = hf_textblock_tfm.add_prefix_space\n",
    "    \n",
    "    # grab the HF_BatchTransform as well\n",
    "    learn_hf_batch_transform = learn.dls.before_batch.hf__batch_transform\n",
    "    \n",
    "    # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "    # return\n",
    "    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity), add_prefix_space=add_prefix_space))) \n",
    "                           for entity in inp ]\n",
    "    \n",
    "    # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "    # (e.g., [CLS], [SEP], etc...)\n",
    "    txt_toks = [ sub_toks for entity in inp \n",
    "                for sub_toks in hf_tokenizer.tokenize(entity, add_prefix_space=add_prefix_space) ]\n",
    "    \n",
    "    txt_tok_ids = hf_tokenizer.convert_tokens_to_ids(txt_toks)\n",
    "    \n",
    "    res = hf_tokenizer.prepare_for_model(txt_tok_ids, None, \n",
    "                                         max_length=learn_hf_batch_transform.max_seq_len, \n",
    "                                         pad_to_max_length=True,\n",
    "                                         truncation_strategy=None, \n",
    "                                         return_special_tokens_mask=True)\n",
    "    \n",
    "    special_toks_msk = L(res['special_tokens_mask'])\n",
    "    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "    # using the indexes to the actual tokens, get that info from the results returned above\n",
    "    pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "    actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "    # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "    offset = 0\n",
    "    raw_trg_idxs = []\n",
    "    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "        raw_trg_idxs.append(idx+offset)\n",
    "        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.predict_tokens\" class=\"doc_header\"><code>Learner.predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.predict_tokens</code>(**`inp`**, **\\*\\*`kargs`**)\n",
       "\n",
       "Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
       "get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG')]\n"
     ]
    }
   ],
   "source": [
    "res = learn.predict_tokens(txt.split())\n",
    "print([(tok, lbl) for tok,lbl in zip(res[0],res[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-language-modeling.ipynb.\n",
      "Converted 01c_data-question-answering.ipynb.\n",
      "Converted 01d_data-token-classification.ipynb.\n",
      "Converted 01e_data-text-generation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-language-modeling.ipynb.\n",
      "Converted 02c_modeling-question-answering.ipynb.\n",
      "Converted 02d_modeling-token-classification.ipynb.\n",
      "Converted 02e_modeling-text-generation.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
