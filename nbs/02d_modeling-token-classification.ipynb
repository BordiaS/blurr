{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForTokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)\n",
    "hf_arch, type(hf_tokenizer), type(hf_config), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, task=ForTokenClassificationTask(), max_seq_len=128),\n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Sen', 'O'), ('Ex', 'B-ORG'), ('Mo', 'I-ORG'), ('\"', 'O'), ('buy', 'O'), ('\"', 'O'), ('Paris', 'B-LOC'), ('(', 'O'), ('akt', 'B-ORG'), ('AG', 'I-ORG'), (')', 'O'), ('-', 'O'), (':', 'O'), ('Ay', 'B-PER'), ('de', 'I-PER'), (',', 'O'), ('Ana', 'O'), ('der', 'O'), ('Société', 'B-ORG'), ('Général', 'I-ORG'), (',', 'O'), ('st', 'O'), ('die', 'O'), ('Akt', 'O'), ('des', 'O'), ('US', 'B-LOCderiv'), ('Unternehmens', 'O'), ('Ex', 'B-ORG'), ('Mo', 'I-ORG'), ('(', 'O'), ('IS', 'O'), ('US', 'O'), ('WK', 'O'), ('852', 'O'), (')', 'O'), ('mit', 'O'), ('\"', 'O'), ('buy', 'O'), ('\"', 'O'), ('ein', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('He', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S', 'O'), ('593', 'O'), ('Win', 'B-OTH'), ('&amp;', 'I-OTH'), ('Sei', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S', 'O'), ('32', 'O'), ('In', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falk', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('Afrikan', 'B-LOCderiv'), ('Baum', 'O'), ('(', 'O'), ('Falco', 'O'), ('cu', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Mala', 'O'), ('(', 'O'), ('Falco', 'O'), ('server', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zu', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegen', 'O'), ('der', 'O'), ('Forschung', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        if (not self.do_setup): return    \n",
    "\n",
    "        # one time setup code here.\n",
    "        self.hf_tokenizer = self.dls.tfms[0].hf_tokenizer\n",
    "        self.ignore_label_token_id = self.dls.tfms[1].ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        \n",
    "        self.results = []\n",
    "        self.seq_accuracy, self.seq_precision, self.seq_recall, self.seq_f1 = 0.,0.,0.,0.\n",
    "        \n",
    "        seq_metrics = L(ValueMetric(self.seq_accuracy_value, 'accuracy'), \n",
    "                        ValueMetric(self.seq_precision_value, 'precision'),\n",
    "                        ValueMetric(self.seq_recall_value, 'recall'),\n",
    "                        ValueMetric(self.seq_f1_value, 'f1'))\n",
    "        \n",
    "        self.learn.metrics = self.learn.metrics + seq_metrics\n",
    "        self.do_setup = False\n",
    "        \n",
    "    # these HAVE to be functions\n",
    "    def seq_accuracy_value(self): return self.seq_accuracy\n",
    "    def seq_precision_value(self): return self.seq_precision\n",
    "    def seq_recall_value(self): return self.seq_recall\n",
    "    def seq_f1_value(self): return self.seq_f1\n",
    "    \n",
    "    # ----callbacks ----\n",
    "    def begin_fit(self): self.setup()\n",
    "    def begin_epoch(self): self.results = []\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if (self.model.training): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "    def after_validate(self):\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        \n",
    "        accuracy = seq_metrics.accuracy_score(targs, preds)\n",
    "        precision = seq_metrics.precision_score(targs, preds)\n",
    "        recall = seq_metrics.recall_score(targs, preds)\n",
    "        f1 = seq_metrics.f1_score(targs, preds)\n",
    "        \n",
    "        self.seq_accuracy, self.seq_precision, self.seq_recall, self.seq_f1 = accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam, decouple_wd=True),\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 128, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 128]), 4, torch.Size([4, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][0].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 18]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0006309573538601399, lr_steep=2.0892961401841603e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.286872</td>\n",
       "      <td>0.203417</td>\n",
       "      <td>0.943590</td>\n",
       "      <td>0.517986</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.105661</td>\n",
       "      <td>0.118666</td>\n",
       "      <td>0.964103</td>\n",
       "      <td>0.684028</td>\n",
       "      <td>0.711191</td>\n",
       "      <td>0.697345</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065369</td>\n",
       "      <td>0.110807</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.687719</td>\n",
       "      <td>0.707581</td>\n",
       "      <td>0.697509</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8), cbs=[HF_TokenClassMetricsCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, hf_tokenizer, skip_special_tokens=True,\n",
    "                 ctxs=None, max_n=6, **kwargs):        \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x[0], y, samples, outs):\n",
    "        inp_trg_preds = [ (hf_tokenizer.ids_to_tokens[tok_id.item()], lbl_id.item(), pred_lbl) \n",
    "                         for tok_id, lbl_id, pred_lbl in zip(inp, trg, ast.literal_eval(pred[0])) \n",
    "                         if (tok_id not in hf_tokenizer.all_special_ids) and lbl_id != -100 ]\n",
    "        \n",
    "        res.append(f'{[ (itp[0], lbl, itp[2]) for itp, lbl in zip(inp_trg_preds, ast.literal_eval(sample[1])) ]}')\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Düsseldorf', 'B-LOC', 'B-LOC'), ('(', 'O', 'O'), ('akt', 'B-ORG', 'B-ORG'), ('AG', 'I-ORG', 'I-ORG'), (')', 'O', 'O'), (':', 'O', 'O'), ('-', 'O', 'O'), ('Nachdem', 'O', 'O'), ('der', 'O', 'O'), ('Sp', 'O', 'O'), ('auf', 'O', 'O'), ('ein', 'O', 'O'), ('neues', 'O', 'O'), ('All', 'O', 'O'), ('(', 'O', 'O'), ('1', 'O', 'O'), ('USD', 'B-OTH', 'B-OTH'), (')', 'O', 'O'), ('gel', 'O', 'O'), ('ist', 'O', 'O'), (',', 'O', 'O'), ('g', 'O', 'O'), ('sich', 'O', 'O'), ('der', 'O', 'O'), ('Gold', 'O', 'O'), ('derzeit', 'O', 'O'), ('eine', 'O', 'O'), ('Vers', 'O', 'O'), (',', 'O', 'O'), ('so', 'O', 'O'), ('die', 'O', 'O'), ('Ana', 'O', 'O'), ('von', 'O', 'O'), ('HS', 'B-ORG', 'B-ORG'), ('Tri', 'I-ORG', 'B-PER'), ('Bu', 'I-ORG', 'I-PER'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Das', 'O', 'O'), ('SS', 'B-ORGpart', 'B-ORG'), ('88', 'I-ORGpart', 'I-ORG'), ('wurde', 'O', 'O'), ('im', 'O', 'O'), ('März', 'O', 'O'), ('aus', 'O', 'O'), ('einer', 'O', 'O'), ('Kampf', 'O', 'O'), ('der', 'O', 'O'), ('SS', 'B-ORGpart', 'B-ORG'), ('des', 'O', 'O'), ('Wirtschaft', 'O', 'B-ORG'), ('und', 'O', 'O'), ('Verwaltung', 'O', 'I-ORG'), ('und', 'O', 'O'), ('Teilen', 'O', 'O'), ('des', 'O', 'O'), ('I', 'B-ORGpart', 'B-ORG'), ('34', 'I-ORGpart', 'I-ORG'), ('der', 'O', 'O'), ('Ordnung', 'O', 'O'), (',', 'O', 'O'), ('Heer', 'O', 'O'), ('und', 'O', 'O'), ('Volk', 'O', 'B-ORG'), ('gebildet', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-LOC', 'I-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "res = learn.predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict_tokens(self:Learner, inp, **kargs):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
    "    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input\n",
    "    \"\"\"\n",
    "    pred_lbls, pred_lbl_ids, probs = self.predict(inp)\n",
    "    \n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    add_prefix_space = hf_textblock_tfm.add_prefix_space\n",
    "    \n",
    "    # grab the HF_BatchTransform as well\n",
    "    learn_hf_batch_transform = learn.dls.before_batch.hf__batch_transform\n",
    "    \n",
    "    # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "    # return\n",
    "    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity), add_prefix_space=add_prefix_space))) \n",
    "                           for entity in inp ]\n",
    "    \n",
    "    # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "    # (e.g., [CLS], [SEP], etc...)\n",
    "    txt_toks = [ sub_toks for entity in inp \n",
    "                for sub_toks in hf_tokenizer.tokenize(entity, add_prefix_space=add_prefix_space) ]\n",
    "    \n",
    "    txt_tok_ids = hf_tokenizer.convert_tokens_to_ids(txt_toks)\n",
    "    \n",
    "    res = hf_tokenizer.prepare_for_model(txt_tok_ids, None, \n",
    "                                         max_length=learn_hf_batch_transform.max_seq_len, \n",
    "                                         pad_to_max_length=True,\n",
    "                                         truncation_strategy=None, \n",
    "                                         return_special_tokens_mask=True)\n",
    "    \n",
    "    special_toks_msk = L(res['special_tokens_mask'])\n",
    "    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "    # using the indexes to the actual tokens, get that info from the results returned above\n",
    "    pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "    actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "    # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "    offset = 0\n",
    "    raw_trg_idxs = []\n",
    "    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "        raw_trg_idxs.append(idx+offset)\n",
    "        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.predict_tokens\" class=\"doc_header\"><code>Learner.predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.predict_tokens</code>(**`inp`**, **\\*\\*`kargs`**)\n",
       "\n",
       "Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
       "get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG')]\n"
     ]
    }
   ],
   "source": [
    "res = learn.predict_tokens(txt.split())\n",
    "print([(tok, lbl) for tok,lbl in zip(res[0],res[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-language-modeling.ipynb.\n",
      "Converted 01c_data-question-answering.ipynb.\n",
      "Converted 01d_data-token-classification.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-language-modeling.ipynb.\n",
      "Converted 02c_modeling-question-answering.ipynb.\n",
      "Converted 02d_modeling-token-classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
