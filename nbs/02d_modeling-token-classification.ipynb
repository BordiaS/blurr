{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForTokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)\n",
    "hf_arch, type(hf_tokenizer), type(hf_config), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, task=ForTokenClassificationTask(), max_seq_len=128),\n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Sen', 'O'), ('Ex', 'B-ORG'), ('Mo', 'I-ORG'), ('\"', 'O'), ('buy', 'O'), ('\"', 'O'), ('Paris', 'B-LOC'), ('(', 'O'), ('akt', 'B-ORG'), ('AG', 'I-ORG'), (')', 'O'), ('-', 'O'), (':', 'O'), ('Ay', 'B-PER'), ('de', 'I-PER'), (',', 'O'), ('Ana', 'O'), ('der', 'O'), ('Société', 'B-ORG'), ('Général', 'I-ORG'), (',', 'O'), ('st', 'O'), ('die', 'O'), ('Akt', 'O'), ('des', 'O'), ('US', 'B-LOCderiv'), ('Unternehmens', 'O'), ('Ex', 'B-ORG'), ('Mo', 'I-ORG'), ('(', 'O'), ('IS', 'O'), ('US', 'O'), ('WK', 'O'), ('852', 'O'), (')', 'O'), ('mit', 'O'), ('\"', 'O'), ('buy', 'O'), ('\"', 'O'), ('ein', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('He', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S', 'O'), ('593', 'O'), ('Win', 'B-OTH'), ('&amp;', 'I-OTH'), ('Sei', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S', 'O'), ('32', 'O'), ('In', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falk', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('Afrikan', 'B-LOCderiv'), ('Baum', 'O'), ('(', 'O'), ('Falco', 'O'), ('cu', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Mala', 'O'), ('(', 'O'), ('Falco', 'O'), ('server', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zu', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegen', 'O'), ('der', 'O'), ('Forschung', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        if (not self.do_setup): return    \n",
    "\n",
    "        # one time setup code here.\n",
    "        self.hf_tokenizer = self.dls.tfms[0].hf_tokenizer\n",
    "        self.ignore_label_token_id = self.dls.tfms[1].ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        \n",
    "        self.results = []\n",
    "        self.seq_accuracy, self.seq_precision, self.seq_recall, self.seq_f1 = 0.,0.,0.,0.\n",
    "        \n",
    "        seq_metrics = L(ValueMetric(self.seq_accuracy_value, 'accuracy'), \n",
    "                        ValueMetric(self.seq_precision_value, 'precision'),\n",
    "                        ValueMetric(self.seq_recall_value, 'recall'),\n",
    "                        ValueMetric(self.seq_f1_value, 'f1'))\n",
    "        \n",
    "        self.learn.metrics = self.learn.metrics + seq_metrics\n",
    "        self.do_setup = False\n",
    "        \n",
    "    # these HAVE to be functions\n",
    "    def seq_accuracy_value(self): return self.seq_accuracy\n",
    "    def seq_precision_value(self): return self.seq_precision\n",
    "    def seq_recall_value(self): return self.seq_recall\n",
    "    def seq_f1_value(self): return self.seq_f1\n",
    "    \n",
    "    # ----callbacks ----\n",
    "    def begin_fit(self): self.setup()\n",
    "    def begin_epoch(self): self.results = []\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if (self.model.training): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "    def after_validate(self):\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        \n",
    "        accuracy = seq_metrics.accuracy_score(targs, preds)\n",
    "        precision = seq_metrics.precision_score(targs, preds)\n",
    "        recall = seq_metrics.recall_score(targs, preds)\n",
    "        f1 = seq_metrics.f1_score(targs, preds)\n",
    "        \n",
    "        self.seq_accuracy, self.seq_precision, self.seq_recall, self.seq_f1 = accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam, decouple_wd=True),\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 128, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 128]), 4, torch.Size([4, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][0].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 18]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.001096478197723627, lr_steep=3.0199516913853586e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.271806</td>\n",
       "      <td>0.226024</td>\n",
       "      <td>0.944488</td>\n",
       "      <td>0.553648</td>\n",
       "      <td>0.513944</td>\n",
       "      <td>0.533058</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117767</td>\n",
       "      <td>0.142813</td>\n",
       "      <td>0.957319</td>\n",
       "      <td>0.640741</td>\n",
       "      <td>0.689243</td>\n",
       "      <td>0.664107</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.070310</td>\n",
       "      <td>0.153364</td>\n",
       "      <td>0.958890</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.709163</td>\n",
       "      <td>0.696673</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8), cbs=[HF_TokenClassMetricsCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, hf_tokenizer, skip_special_tokens=True,\n",
    "                 ctxs=None, max_n=6, **kwargs):        \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x[0], y, samples, outs):\n",
    "        inp_trg_preds = [ (hf_tokenizer.ids_to_tokens[tok_id.item()], lbl_id.item(), pred_lbl) \n",
    "                         for tok_id, lbl_id, pred_lbl in zip(inp, trg, ast.literal_eval(pred[0])) \n",
    "                         if (tok_id not in hf_tokenizer.all_special_ids) and lbl_id != -100 ]\n",
    "        \n",
    "        res.append(f'{[ (itp[0], lbl, itp[2]) for itp, lbl in zip(inp_trg_preds, ast.literal_eval(sample[1])) ]}')\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Boris', 'B-PER', 'B-PER'), ('He', 'I-PER', 'I-PER'), (':', 'O', 'O'), ('Image', 'B-OTH', 'O'), ('von', 'I-OTH', 'O'), ('Kaiser', 'I-OTH', 'B-PER'), ('nach', 'I-OTH', 'O'), ('ge', 'I-OTH', 'O'), ('In', 'O', 'O'), (':', 'O', 'O'), ('Wirtschaft', 'B-ORG', 'B-ORG'), ('online', 'I-ORG', 'I-ORG'), ('vom', 'O', 'O'), ('29', 'O', 'O'), ('Juni', 'O', 'O'), ('2009', 'O', 'O'), ('Der', 'O', 'O'), ('lang', 'O', 'O'), ('Be', 'O', 'O'), ('war', 'O', 'O'), ('vor', 'O', 'O'), ('worden', 'O', 'O'), (',', 'O', 'O'), ('zwei', 'O', 'O'), ('ihr', 'O', 'O'), ('nicht', 'O', 'O'), ('gehören', 'O', 'O'), ('Lee', 'O', 'O'), ('im', 'O', 'O'), ('G', 'O', 'O'), ('von', 'O', 'O'), ('1', 'O', 'O'), ('Euro', 'B-OTH', 'B-OTH'), ('ein', 'O', 'O'), ('zu', 'O', 'O'), ('haben', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('Es', 'O', 'O'), ('ist', 'O', 'O'), ('be', 'O', 'O'), (',', 'O', 'O'), ('die', 'O', 'O'), ('Akt', 'O', 'O'), ('im', 'O', 'O'), ('ersten', 'O', 'O'), ('Hal', 'O', 'O'), ('2007', 'O', 'O'), ('von', 'O', 'O'), ('der', 'O', 'O'), ('De', 'B-ORG', 'B-ORG'), ('auf', 'O', 'O'), ('eine', 'O', 'O'), ('von', 'O', 'O'), ('der', 'O', 'O'), ('Spa', 'B-ORG', 'B-ORG'), ('zu', 'O', 'O'), ('errichten', 'O', 'O'), ('Gesellschaft', 'O', 'O'), ('zu', 'O', 'O'), ('übertragen', 'O', 'O'), ('\"', 'O', 'O'), (',', 'O', 'O'), ('teil', 'O', 'O'), ('das', 'O', 'O'), ('Frankfurter', 'B-LOCderiv', 'B-ORG'), ('Institut', 'O', 'I-ORG'), ('und', 'O', 'O'), ('der', 'O', 'O'), ('Deutsche', 'B-ORG', 'B-ORG'), ('Spa', 'I-ORG', 'I-ORG'), ('Giro', 'I-ORG', 'I-ORG'), ('(', 'O', 'O'), ('DS', 'B-ORG', 'B-ORG'), (')', 'O', 'O'), ('mit', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "res = learn.predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict_tokens(self:Learner, inp, **kargs):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
    "    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input\n",
    "    \"\"\"\n",
    "    pred_lbls, pred_lbl_ids, probs = self.predict(inp)\n",
    "    \n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    add_prefix_space = hf_textblock_tfm.add_prefix_space\n",
    "    \n",
    "    # grab the HF_BatchTransform as well\n",
    "    learn_hf_batch_transform = learn.dls.before_batch.hf__batch_transform\n",
    "    \n",
    "    # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "    # return\n",
    "    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity), add_prefix_space=add_prefix_space))) \n",
    "                           for entity in inp ]\n",
    "    \n",
    "    # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "    # (e.g., [CLS], [SEP], etc...)\n",
    "    txt_toks = [ sub_toks for entity in inp \n",
    "                for sub_toks in hf_tokenizer.tokenize(entity, add_prefix_space=add_prefix_space) ]\n",
    "    \n",
    "    txt_tok_ids = hf_tokenizer.convert_tokens_to_ids(txt_toks)\n",
    "    \n",
    "    res = hf_tokenizer.prepare_for_model(txt_tok_ids, None, \n",
    "                                         max_length=learn_hf_batch_transform.max_seq_len, \n",
    "                                         pad_to_max_length=True,\n",
    "                                         truncation_strategy=None, \n",
    "                                         return_special_tokens_mask=True)\n",
    "    \n",
    "    special_toks_msk = L(res['special_tokens_mask'])\n",
    "    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "    # using the indexes to the actual tokens, get that info from the results returned above\n",
    "    pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "    actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "    # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "    offset = 0\n",
    "    raw_trg_idxs = []\n",
    "    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "        raw_trg_idxs.append(idx+offset)\n",
    "        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.predict_tokens\" class=\"doc_header\"><code>Learner.predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.predict_tokens</code>(**`inp`**, **\\*\\*`kargs`**)\n",
       "\n",
       "Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
       "get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG')]\n"
     ]
    }
   ],
   "source": [
    "res = learn.predict_tokens(txt.split())\n",
    "print([(tok, lbl) for tok,lbl in zip(res[0],res[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-language-modeling.ipynb.\n",
      "Converted 01c_data-question-answering.ipynb.\n",
      "Converted 01d_data-token-classification.ipynb.\n",
      "Converted 01e_data-text-generation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-language-modeling.ipynb.\n",
      "Converted 02c_modeling-question-answering.ipynb.\n",
      "Converted 02d_modeling-token-classification.ipynb.\n",
      "Converted 02e_modeling-text-generation.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
