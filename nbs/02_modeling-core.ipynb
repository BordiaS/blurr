{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.core\n",
    "\n",
    "> This module contains core custom models, loss functions, and a default layer group splitter for use in applying discriminiative learning rates to your huggingface models trained via fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, nlp\n",
    "from transformers import *\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.hook import _print_shapes\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base splitter, model wrapper, and model callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hf_splitter(m):\n",
    "    \"\"\"Splits the huggingface model based on various model architecture conventions\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, 'hf_model')) else m\n",
    "    root_modules = list(model.named_children())\n",
    "    top_module_name, top_module = root_modules[0]\n",
    "    \n",
    "    groups = L([ m for m_name, m in list(top_module.named_children()) ])\n",
    "    groups += L([ m for m_name, m in root_modules[1:] ])\n",
    "\n",
    "    return groups.map(params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"hf_splitter\" class=\"doc_header\"><code>hf_splitter</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>hf_splitter</code>(**`m`**)\n",
       "\n",
       "Splits the huggingface model based on various model architecture conventions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(hf_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelWrapper(Module):\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model\n",
    "        \n",
    "        n_fwd_args = self.hf_model.forward.__code__.co_argcount\n",
    "        self.hf_model_fwd_args = self.hf_model.forward.__code__.co_varnames[:n_fwd_args][1:]\n",
    "        \n",
    "    def forward(self, x): \n",
    "        for k in list(x): \n",
    "            if k not in self.hf_model_fwd_args: del x[k]         \n",
    "                \n",
    "        return self.hf_model(**x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `HF_baseModelWrapper` includes some nifty code for just passing in the things your model needs, as not all transformer architectures require/use the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelCallback(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Callback` for handling what is returned from the huggingface model ... \"the huggingface model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them is some regularization scheme)\" - from the fastai [Transformer's Tutorial](http://dev.fast.ai/tutorial.transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a sequence classification task (e.g., a model that requires a single text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, padding='max_length'), CategoryBlock)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Being from a small town in Illinois myself, I can instantly relate to this movie. Considering the era it was made in, the townsfolk look uncomfortably like a lot of people I grew up with. Yes the plot is so-so. And yes, the Acting is not going to get nominated for an Oscar anytime soon. But that isn't the point. The point is to suspend reality and just have FUN. And this movie has Fun aplenty. From the greedy,uncaring banker to the well meaning,but dimwitted deputy, this movie was made to poke fun at the SciFi genre and small town living at it's best. Who can't smile at the sight of the Enforcer Drone or the Vern Droid? and I LOVED the FarmZoid. Wish I had one when I was growing up. Overall, considering the technology they had available at the time, this is a pleasant romp into one's childhood, when you could sit back on a Saturday afternoon, Popcorn in hand, and laugh at the foibles of small town living. This is a movie I would watch again and again, if for no other reason than to poke fun at myself and my small town ways.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We'll also add in custom summary methods for blurr learners/models that work with dictionary inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                metrics=[accuracy],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.to_fp16()` requires a GPU so had to remove for tests to run on github.  Let's check that we can get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1398, 0.0416],\n",
       "         [0.1352, 0.0231],\n",
       "         [0.1426, 0.0384],\n",
       "         [0.1496, 0.0332]], device='cuda:1', grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def blurr_module_summary(learn, *xb):\n",
    "    \"Print a summary of `model` using `xb`\"\n",
    "    infos = layer_info(learn, *xb)\n",
    "    n,bs = 64,find_bs(xb)\n",
    "    inp_sz = _print_shapes(apply(lambda x:x.shape,  xb[0]['input_ids']), bs)\n",
    "    res = f\"{learn.model.__class__.__name__} (Input shape: {inp_sz})\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    res += f\"{'Layer (type)':<20} {'Output Shape':<20} {'Param #':<10} {'Trainable':<10}\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    ps,trn_ps = 0,0\n",
    "    infos = [o for o in infos if o is not None] #see comment in previous cell\n",
    "    for typ,np,trn,sz in infos:\n",
    "        if sz is None: continue\n",
    "        ps += np\n",
    "        if trn: trn_ps += np\n",
    "        res += f\"{typ:<20} {_print_shapes(sz, bs)[:19]:<20} {np:<10,} {str(trn):<10}\\n\"\n",
    "        res += \"_\" * n + \"\\n\"\n",
    "    res += f\"\\nTotal params: {ps:,}\\n\"\n",
    "    res += f\"Total trainable params: {trn_ps:,}\\n\"\n",
    "    res += f\"Total non-trainable params: {ps - trn_ps:,}\\n\\n\"\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summary(self:Learner):\n",
    "    \"Print a summary of the model, optimizer and loss function.\"\n",
    "    xb = self.dls.train.one_batch()[:self.dls.train.n_inp]\n",
    "    res = blurr_module_summary(self, *xb)\n",
    "    res += f\"Optimizer used: {self.opt_func}\\nLoss function: {self.loss_func}\\n\\n\"\n",
    "    if self.opt is not None:\n",
    "        res += f\"Model \" + (\"unfrozen\\n\\n\" if self.opt.frozen_idx==0 else f\"frozen up to parameter group #{self.opt.frozen_idx}\\n\\n\")\n",
    "    res += \"Callbacks:\\n\" + '\\n'.join(f\"  - {cb}\" for cb in sort_by_run(self.cbs))\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create our own `summary` methods above because fastai only works where things are represented by a *single tensor*.  But in the case of huggingface transformers, a *single* sequence is represented by *multiple tensors* (in a dictionary).  \n",
    "\n",
    "The change to make this work is so minor I think that the fastai library can/will hopefully be updated to support this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.00036307806149125097, lr_steep=0.02754228748381138)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.469650</td>\n",
       "      <td>0.510889</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.211285</td>\n",
       "      <td>0.331401</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.177134</td>\n",
       "      <td>0.294831</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "And here we creat a @typedispatched impelmentation of `Learner.show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_BaseInput, y, samples, outs, learner=None, ctxs=None, max_n=6, **kwargs):        \n",
    "    \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    \n",
    "    n_preds_per_input = len(outs[0])\n",
    "    if (n_preds_per_input == 1): \n",
    "        for i,ctx in enumerate(ctxs): ctx['target'] = outs[i][0]\n",
    "    else:\n",
    "        for pred_idx in range(n_preds_per_input):\n",
    "            for i,ctx in enumerate(ctxs):  ctx[f'target{pred_idx+1}'] = outs[i][pred_idx]\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before Lost everything shown on TV was predictable. You could predict who was gonna die or who will find something, but in Lost you could predict NOTHING. Every thing was so surprisingly stunning and it really was a mystery not because it has so many secrets but because there was nothing like it before everything was so great. I literally became addicted to it. LOST is a classic work of art. It gives you something to look forward to every week. It is genius. The surrounding is brilliant it is calm and warm at the beach and so scary in the jungle. The characters are a work of genius every one of them especially the ones already on the island. The castaways are so dramatic yet we can never predict their deaths because they have so much more to do and so much more to say and they have secrets affecting the other castaways that die with them.</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n",
    "    dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    \n",
    "    # this is where we have to change things up since a blurr \"input\" is represented by a dictionary of\n",
    "    # tensors (input_ids, attention_mask, token_type_ids, etc...) and not a single tensor (which fastai assumes\n",
    "    # in a number of places)\n",
    "    b = dl.one_batch()\n",
    "    inp = b[0]\n",
    "    preds, _, dec_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n",
    "    \n",
    "    i = getattr(self.dls, 'n_inp', -1)\n",
    "    inp = (inp,) if i==1 else tuplify(inp)\n",
    "    dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
    "    dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "    res = dec_targ,dec_preds[0],preds[0]\n",
    "    if with_input: res = (dec_inp,) + res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with `summary`, we need to replace fastai's `Learner.predict` method with the one above which is able to work with inputs that are represented by multiple tensors included in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.0686, 0.9314]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict('I really liked the movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.196894</td>\n",
       "      <td>0.506864</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.114689</td>\n",
       "      <td>0.319240</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.144111</td>\n",
       "      <td>0.386641</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=slice(1e-6, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One doesn't get to enjoy this gem, the 1936 Invisible Ray, often. But no can forget it. The story is elegant. Karloff, austere and embittered in his Carpathian mountain retreat, is Janos Rukh, genius science who reads ancient beams of light to ascertain events in the great geological past",
       "particularly the crash of a potent radioactive meteor in Africa. Joining him is the ever-elegant Lugosi (as a rare hero), who studies \"astro-chemistry.\" Frances Drake is the lovely, underused young wife; Frank Lawton the romantic temptation; and the divine Violet Kemble Cooper is Mother Rukh, in a performance worthy of Maria Ospenskya.&lt;br /&gt;&lt;br /&gt;The story moves swiftly in bold episodes, with special effects that are still handsome. It also contains some wonderful lines. One Rukh restores his mother's sight, he asks, \"Mother, can you see, can you see?\" \"Yes, I can see",
       "more clearly than ever. And what I see frightens me.\" Even better when mother Rukh says, \"He broke the first law of science.\" I am not alone among my acquaintance in having puzzled for many many years exactly what this first law of science is.&lt;br /&gt;&lt;br /&gt;This movie is definitely desert island material.</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.0199, 0.9801]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"This was a really good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9955, 0.0045]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"Acting was so bad it was almost funny.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname='seq_class_learn_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9915, 0.0085]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn = load_learner(fname='seq_class_learn_export.pkl')\n",
    "inf_learn.blurr_predict(\"This movie should not be seen by anyone!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core training code above works for **all** pretrained sequence classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_albert.AlbertForSequenceClassification,\n",
       " transformers.modeling_auto.AutoModelForSequenceClassification,\n",
       " transformers.modeling_bart.BartForSequenceClassification,\n",
       " transformers.modeling_bert.BertForSequenceClassification,\n",
       " transformers.modeling_camembert.CamembertForSequenceClassification,\n",
       " transformers.modeling_distilbert.DistilBertForSequenceClassification,\n",
       " transformers.modeling_electra.ElectraForSequenceClassification,\n",
       " transformers.modeling_flaubert.FlaubertForSequenceClassification,\n",
       " transformers.modeling_longformer.LongformerForSequenceClassification,\n",
       " transformers.modeling_mobilebert.MobileBertForSequenceClassification,\n",
       " transformers.modeling_roberta.RobertaForSequenceClassification,\n",
       " transformers.modeling_xlm.XLMForSequenceClassification,\n",
       " transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification,\n",
       " transformers.modeling_xlnet.XLNetForSequenceClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='SequenceClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'facebook/bart-base',\n",
    "    'bert-base-uncased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'monologg/electra-small-finetuned-imdb',\n",
    "    'flaubert/flaubert_small_cased', \n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizer\n",
      "model:\t\tAlbertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.621941</td>\n",
       "      <td>0.617930</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen.br /br /the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall.br</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i simply could not finish this movie. i tuned out after what i would say is my nomination for the most wretched attempt at sexual suggestion award: a scene in which pia zadora, at a picnic, stands between two boys who want her. one (the good boy) pleads for her to see the error of her ways. the other (the bad boy) simply asks if she'd like a hot dog, which he then holds out for her. at crotch level. i hope i'm not spoiling anything to say she turns, and takes the hot dog, with a smile. just pathetic</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== facebook/bart-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizer\n",
      "model:\t\tBartForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.312136</td>\n",
       "      <td>0.446751</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is one of my all-time favorite films, and while it may move too slowly for some, it's well worth seeing. A corporate lawyer (Richard Chamberlain) is dragged into a case involving \"city\" Aborigines, and this is no ordinary case. OK, a man has died but it wasn't exactly a normal killing. There has also been a greater than average amount of rain lately, and the atmosphere of most of the film is somewhat claustrophobic &amp; oppressive. The Aborigines are harboring a secret and refuse to spill the beans. This has a lot to do with white men making assumptions</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizer\n",
      "model:\t\tBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.692009</td>\n",
       "      <td>0.682852</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i had the tv on for a white noise companion and heard \" $ 400 for a fully furnished apartment \" so i ran into the tv room expecting another 70's flick and got much more. luckily, i could rewind to the beginning ( dvr buffer ) and hit the record button to watch it entirely. ( cinemax uncut and in hd no less! ) aside from some holes in the story and intermittent improbable dialog / events, this is an effective thriller worthy of your time to watch. pretty creepy and progressive at times : beverly d'angelo's character masturbates in front of</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizer\n",
      "model:\t\tCamembertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.689356</td>\n",
       "      <td>0.691092</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once again the same familiar story about a man (writer here) who sell his soul to the devil in order to have his most desired ambition in life: success. Unfunny script (we should \"go home and write better\"), ridiculous lines in order to understand the \"strong\" \"Christmanish\" message (our only aspiration in life is to find love, respect and a good friendship) and a very long trial scene at the end</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizer\n",
      "model:\t\tDistilBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.516972</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>victor mclaglen's performance is one of the finest in film history. &lt; br / &gt; &lt; br / &gt; i think we can all feel for \" gypo \" because we've all struggled with what is right and what isn't and been wrong. this was one of the first art - house pictures to be released by a major american movie studio ( rko radio pictures ). &lt; br / &gt; &lt; br / &gt; joseph h. august's cinematography is at its very best here. however, august's stunning portion was mostly overlooked ; he didn't receive the oscar nomination he rightly deserved</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== monologg/electra-small-finetuned-imdb ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/electra-small-finetuned-imdb were not used when initializing ElectraForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/electra-small-finetuned-imdb and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizer\n",
      "model:\t\tElectraForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.184581</td>\n",
       "      <td>0.360627</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are many different versions of this one floating around, so make sure you can locate one of the unrated copies, otherwise some gore and one scene of nudity might be missing. some versions also omit most of the opening sequence and other bits here and there. the cut i saw has the on - screen title witchcraft : evil encounters and was released by shriek show, who maintain the original us release title witchery for the dvd release. it's a nice - looking print and seems to have all of the footage, but has some cropping / aspect ratio issues. in italy, it was released as la casa 4</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_small_cased were not used when initializing FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_small_cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "model:\t\tFlaubertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.699877</td>\n",
       "      <td>0.684506</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since for</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like Tarzan the Ape Man ( 1932 ), only more so. There' s more of everything, more animals, more varied African tribes, and scenes in which the thought must be, if this was good with three or four lions, forty would be better. Tarzan wrestles with crocodilesthe the crocodile machine spins in the water like a rolling pin, around and around, jaws flapping. Tarzan can kill it with his ubiquitous knife if the blasted saurian would hold still. Tarzan kills lions</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizer\n",
      "model:\t\tLongformerForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.421597</td>\n",
       "      <td>0.408879</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had the TV on for a white noise companion and heard\" $400 for a fully furnished apartment\" So I ran into the TV room expecting another 70's flick and got much more. Luckily, I could rewind to the beginning (DVR buffer) and hit the record button to watch it entirely.(Cinemax uncut and in HD no less!) Aside from some holes in the story and intermittent improbable dialog/events, this is an effective thriller worthy of your time to watch. Pretty creepy and progressive at times: Beverly D'Angelo's character masturbates in front of Alison Parker, played adroitly by Crist</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizer\n",
      "model:\t\tMobileBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>216683.484375</td>\n",
       "      <td>39822.539062</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horrible acting, horrible cast and cheap props. would've been a lot better if was set as an action parody style movie. what a waste. starting from the name of the movie. &lt; br / &gt; &lt; br / &gt; \" the enemy \" naming it \" action movie \" would've made it better. ( contributing to the parody effect ). the cop looking like a 60 year old player, the blond girl just having the same blank boring look on her face at all times. towards the end of the movie him and her are working together to take down the bad guys and every time they exchange words it just feels like</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizer\n",
      "model:\t\tRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.434884</td>\n",
       "      <td>0.425127</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Three part \"horror\" film with some guy in a boarded up house imploring the viewer not to go \"out there\" and (unfortunately) gives us three tales to prove why.&lt;br /&gt;&lt;br /&gt;The first story involves a young couple in a car accident who meet up with two psychos. It leads up to two totally predictable twists. Still, it's quick (about 15 minutes), violent, well-acted and well-done. Predictable but enjoyable.&lt;br /&gt;&lt;br /&gt;The second involves a man on the run after stealing a large amount of money. His car breaks down, he's attacked</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "model:\t\tXLMForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.006645</td>\n",
       "      <td>0.690878</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt; br /</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wow! stacy peralta has followed up dogtown and z-boys with an equally stunning documentary about the history of the big-wave surfing culture in america. piecing together insider archival footage along with interviews from surfing legends, we are transported into the daring and free-spirited life of the early pioneers whose sheer passion for the sport spawned an industry that today touches the lives of millions. &lt; br / &gt; &lt; br / &gt; it's getting to know these icons and their stories that gives the film its warmth. you can feel the respect peralta has for this group as we hear accounts of greg noll striding from a pack</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizer\n",
      "model:\t\tXLMRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.695163</td>\n",
       "      <td>0.690389</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A Cry in the Dark\" is a masterful piece of cinema, haunting, and incredibly though provoking. The true story of Lindy Chamberland, who, in 1980, witnessed a horrific sight, seeing her 3-month-old baby being brutally taken from their family's tent, while camping on the Austrailian outback. Azaria (the baby) was never seen again, and the result of her horrendous disappearance caused a true life frenzy all around the world. Meryl Streep does immaculate justice to the role of Lindy, as</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizer\n",
      "model:\t\tXLNetForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.394672</td>\n",
       "      <td>0.431214</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Following the disasterous Revolution, this film was pretty much the final nail in the coffin of Goldcrest and thus the British Film Industry. The film is absolute pants, it's full of music from the attempted mid-80's jazz revival and based on a book &amp; author that was briefly popular at that time and has deservedly sank back into obscurity. Temple searched for ages trying to find Suzette and came up with 8th Wonders Patsy Kensett another person who was briefly popular at the time. By the time the film came out of post production the Jazz revival was over, as was Kens</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "bsz = 2\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config_kwargs={'num_labels': 2})\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\nmodel:\\t\\t{type(hf_model).__name__}\\n')\n",
    "\n",
    "    blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, max_length=128, padding='max_length'), \n",
    "              CategoryBlock)\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('text'), \n",
    "                       get_y=ColReader('label'), \n",
    "                       splitter=ColSplitter(col='is_valid'))\n",
    "    \n",
    "    dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "    \n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                    model,\n",
    "                    opt_func=partial(Adam),\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=[accuracy],\n",
    "                    cbs=[HF_BaseModelCallback],\n",
    "                    splitter=hf_splitter)\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), bsz)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, 128]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(len(preds[0]), bsz)\n",
    "        test_eq(preds[0].shape, torch.Size([bsz, 2]))\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max=1e-3)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(max_n=2)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizer</td>\n",
       "      <td>AlbertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizer</td>\n",
       "      <td>BartForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizer</td>\n",
       "      <td>BertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizer</td>\n",
       "      <td>CamembertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizer</td>\n",
       "      <td>DistilBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizer</td>\n",
       "      <td>ElectraForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizer</td>\n",
       "      <td>LongformerForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizer</td>\n",
       "      <td>MobileBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>RobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizer</td>\n",
       "      <td>XLMRobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizer</td>\n",
       "      <td>XLNetForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Multi-label classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a multi-label classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18049"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a dataset with the first 10% of training set\n",
    "raw_data = nlp.load_dataset('civil_comments', split='train[:1%]') \n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat   insult  identity_attack  \\\n",
       "0  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "1  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "2  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "3  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "4  0.893617         0.021277      0.0     0.0  0.87234         0.021277   \n",
       "\n",
       "   sexual_explicit  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame(raw_data, columns=list(raw_data.features.keys()))\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['severe_toxicity',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_attack',\n",
       " 'sexual_explicit']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_cols = list(toxic_df.columns[2:]); lbl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0  0.000000                0        0       0       0                0   \n",
       "1  0.000000                0        0       0       0                0   \n",
       "2  0.000000                0        0       0       0                0   \n",
       "3  0.000000                0        0       0       0                0   \n",
       "4  0.893617                0        0       0       1                0   \n",
       "\n",
       "   sexual_explicit  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = toxic_df.round({col: 0 for col in lbl_cols})\n",
    "toxic_df = toxic_df.convert_dtypes()\n",
    "\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have to configure the `num_labels` to the number of labels we are predicting. Given that our labels are already encoded, we use a `MultiCategoryBlock` with `encoded=True` and `vocab` equal to the columns with our 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), \n",
    "    MultiCategoryBlock(encoded=True, vocab=lbl_cols)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader(lbl_cols), \n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(toxic_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 184]), torch.Size([4, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An excellent position on this bill from a person who clearly has serious credentials on energy issues. Thank you Mr. Hamilton for weighing in with your experienced view. I'm more convinced than ever that this is a huge win for Oregonians. I sure hope the legislature moves this bill through!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quit funding the Ambler Mine road first.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=BCEWithLogitsLossFlat(),\n",
    "                metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.loss_func.thresh = 0.2\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing multi-label classification, we adjust our loss function to use binary cross-entropy and our metrics to use the multi-label friendly version of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(b[0])\n",
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.19054607152938843, lr_steep=0.0014454397605732083)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.030272</td>\n",
       "      <td>0.036244</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>03:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.035246</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>03:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>0.036048</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>03:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOL at #10. I've had Uber drivers get lost so many times in inner SE. Buckman. It's a GRID, y'all, and I don't feel like paying for the time you wasted driving around in circles.</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hmm. New comment service! Hey this article was great. The fire was almost certainly karma</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func.thresh = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((#1) ['severe_toxicity'],\n",
       " tensor([False, False, False,  True, False, False]),\n",
       " tensor([1.4656e-06, 4.0468e-03, 4.2132e-04, 2.4139e-02, 1.1726e-03, 1.2957e-03]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = \"\"\"\n",
    "Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. \n",
    "No enchiladas for them!\n",
    "\"\"\"\n",
    "learn.blurr_predict(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
