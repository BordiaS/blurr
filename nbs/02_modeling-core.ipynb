{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.core\n",
    "\n",
    "> This module contains core custom models, loss functions, and a default layer group splitter for use in applying discriminiative learning rates to your huggingface models trained via fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, nlp\n",
    "from transformers import *\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.hook import _print_shapes\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base splitter, model wrapper, and model callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hf_splitter(m):\n",
    "    \"\"\"Splits the huggingface model based on various model architecture conventions\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, 'hf_model')) else m\n",
    "    root_modules = list(model.named_children())\n",
    "    top_module_name, top_module = root_modules[0]\n",
    "    \n",
    "    groups = L([ m for m_name, m in list(top_module.named_children()) ])\n",
    "    groups += L([ m for m_name, m in root_modules[1:] ])\n",
    "\n",
    "    return groups.map(params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"hf_splitter\" class=\"doc_header\"><code>hf_splitter</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>hf_splitter</code>(**`m`**)\n",
       "\n",
       "Splits the huggingface model based on various model architecture conventions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(hf_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelWrapper(Module):\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model\n",
    "        \n",
    "        n_fwd_args = self.hf_model.forward.__code__.co_argcount\n",
    "        self.hf_model_fwd_args = self.hf_model.forward.__code__.co_varnames[:n_fwd_args][1:]\n",
    "        \n",
    "    def forward(self, x): \n",
    "        for k in list(x): \n",
    "            if k not in self.hf_model_fwd_args: del x[k]         \n",
    "                \n",
    "        return self.hf_model(**x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `HF_baseModelWrapper` includes some nifty code for just passing in the things your model needs, as not all transformer architectures require/use the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelCallback(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Callback` for handling what is returned from the huggingface model ... \"the huggingface model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them is some regularization scheme)\" - from the fastai [Transformer's Tutorial](http://dev.fast.ai/tutorial.transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a sequence classification task (e.g., a model that requires a single text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, padding='max_length'), CategoryBlock)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't know what the rest of you guys watch Steven Seagal movies for, but I watch them because, as silly as they are, they're at least always good for a laugh. Why would you rate this movie a 1 out of 10 based on the dubbing, when that kind of thing is exactly what makes a movie like this into a cult favorite that you can laugh at the silliness of?&lt;br /&gt;&lt;br /&gt;Attack Force is by no means a great movie, but I felt it was as worthy a Steven Seagal vehicle as many of his other movies; in fact I didn't think it was one of his worst by a long-shot. It had, most of the time, a half-way coherent plot line, and it was, most of the time, fundamentally exciting. The ending really sucked, but even that had some enjoyably trashy elements. In the end the story itself did not deliver what it promised, but I actually thought that the acting, characterization (if I may use such a big word) and the rest of the production values delivered exactly what a true Steven Seagal fan would expect. Seagal himself in particular was exactly the stone-faced, no-nonsense man's man that we've come to expect, and the rest of the cast backed him up pretty well, without ever up-staging him. This, people, is what a Steven Seagal movie does. Deal with it. Or even better: laugh at it.&lt;br /&gt;&lt;br /&gt;4 out of 10.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We'll also add in custom summary methods for blurr learners/models that work with dictionary inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                metrics=[accuracy],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.to_fp16()` requires a GPU so had to remove for tests to run on github.  Let's check that we can get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0276, -0.0008],\n",
       "         [ 0.0102,  0.0116],\n",
       "         [ 0.0121, -0.0039],\n",
       "         [ 0.0196,  0.0015]], device='cuda:1', grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summary(self:nn.Module, *xb):\n",
    "    \"Print a summary of `self` using `xb`\"\n",
    "    sample_inputs,infos = layer_info(self, *xb)\n",
    "    n,bs = 64,find_bs(xb)\n",
    "    inp_sz = _print_shapes(apply(lambda x:x.shape, xb[0]['input_ids']), bs)\n",
    "    res = f\"{self.__class__.__name__} (Input shape: {inp_sz})\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    res += f\"{'Layer (type)':<20} {'Output Shape':<20} {'Param #':<10} {'Trainable':<10}\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    ps,trn_ps = 0,0\n",
    "    infos = [o for o in infos if o is not None] #see comment in previous cell\n",
    "    for typ,np,trn,sz in infos:\n",
    "        if sz is None: continue\n",
    "        ps += np\n",
    "        if trn: trn_ps += np\n",
    "        res += f\"{typ:<20} {_print_shapes(sz, bs)[:19]:<20} {np:<10,} {str(trn):<10}\\n\"\n",
    "        res += \"_\" * n + \"\\n\"\n",
    "    res += f\"\\nTotal params: {ps:,}\\n\"\n",
    "    res += f\"Total trainable params: {trn_ps:,}\\n\"\n",
    "    res += f\"Total non-trainable params: {ps - trn_ps:,}\\n\\n\"\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summary(self:Learner):\n",
    "    \"Print a summary of the model, optimizer and loss function.\"\n",
    "    xb = self.dls.train.one_batch()[:self.dls.train.n_inp]\n",
    "    res = self.model.blurr_summary(*xb)\n",
    "    res += f\"Optimizer used: {self.opt_func}\\nLoss function: {self.loss_func}\\n\\n\"\n",
    "    if self.opt is not None:\n",
    "        res += f\"Model \" + (\"unfrozen\\n\\n\" if self.opt.frozen_idx==0 else f\"frozen up to parameter group number {self.opt.frozen_idx}\\n\\n\")\n",
    "    res += \"Callbacks:\\n\" + '\\n'.join(f\"  - {cb}\" for cb in sort_by_run(self.cbs))\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create our own `summary` methods above because fastai only works where things are represented by a *single tensor*.  But in the case of huggingface transformers, a *single* sequence is represented by *multiple tensors* (in a dictionary).  \n",
    "\n",
    "The change to make this work is so minor I think that the fastai library can/will hopefully be updated to support this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=9.12010818865383e-08, lr_steep=0.010964781977236271)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEKCAYAAADw2zkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfK0lEQVR4nO3de5xcZZ3n8c+vqvre6aQ73bmQEJJAuMndcBU0IiogCjLrBRiUkZFxR7zsqKO+3Fmd3Zmd2ZerOwMrcnEQZRUGhZ1FB2/jyEUwSEcDhHuuJoGQTro73V3VXdff/lHVnSZ0d6rTferU5ft+vfqVrlOn6vyeVFLfes5z6nnM3RERkdoWCbsAEREJn8JAREQUBiIiojAQEREUBiIigsJARESAWNgFTFdnZ6cvX7487DJERCrKunXr9rh712T3V1wYLF++nO7u7rDLEBGpKGa2bar7dZpIREQUBiIiojAQEREUBiIigsJARERQGIiICAoDEZGK8ItnX2Xj7sHAnl9hICJSAf78e+u493c7A3t+hYGISJlLZXKks05rQ3DfE1YYiIiUuXgyA0BzfTSwYwQWBmZ2u5ntNrMNU+yzxszWm9kzZvZQULWIiFSyoUIYtFRoz+AO4MLJ7jSzecBNwHvc/Q3A+wKsRUSkYsVT+TCoyNNE7v4w0DvFLlcC97n7Hwr77w6qFhGRShav8J7BwRwNtJvZg2a2zsw+NNmOZnadmXWbWXdPT08JSxQRCV88mQWgtaECxwyKEAPeCLwLeCfwV2Z29EQ7uvut7r7a3Vd3dU06HbeISFXaP4AcXM8gzPUMdgB73T0OxM3sYeBk4MUQaxIRKTujA8gVOWZQhP8HnGtmMTNrBs4EnguxHhGRslSKMYPAntnM7gLWAJ1mtgP4MlAH4O43u/tzZvZT4CkgB3zL3Se9DFVEpFbFU/kxg5YAxwwCCwN3v6KIfb4KfDWoGkREqkE8mSEWMeqjwZ3M0TeQRUTKXDyZoaUhhpkFdgyFgYhImRtKZgMdPAaFgYhI2cv3DIIbLwCFgYhI2YunMoFeSQQKAxGRshdPZmgJ8AtnoDAQESl78WRWp4lERGrdUFKniUREal48ldHVRCIitS6RzAY6SR0oDEREyloqkyOVzQU6fTUoDEREylopJqkDhYGISFkrxfrHoDAQESlrpVj/GBQGIiJlbXTJy+Z6jRmIiNSseAlWOQOFgYhIWdMAsoiIlGT9Y1AYiIiUtURKYwYiIjVPl5aKiMjY+scNsWDfrhUGIiJlrBTrH4PCQESkrA0ls7QEPF4ACgMRkbKWKMGSl6AwEBEpa6VY2AYUBiIiZS2eDH5hG1AYiIiUtVKsfwwKAxGRsjaUzNAS8CpnoDAQESlrGkAWEZHCaSKFgYhIzSrV+segMBARKVuj01c3a8xARKR2lWrJS1AYiIiUrdElLyt6zMDMbjez3Wa24SD7nW5mGTP7D0HVIiJSifZPX13ZYwZ3ABdOtYOZRYH/Afw8wDpERCpSqdY/hgDDwN0fBnoPstsngHuB3UHVISJSqWpiANnMlgDvBb4ZVg0iIuUsXljysqJ7BkX4B+Dz7p472I5mdp2ZdZtZd09PTwlKExEJX7yEYwbBx83kVgN3F1bv6QQuNrOMu//LgTu6+63ArQCrV6/2klYpIhKSUq1/DCGGgbuvGP3dzO4AfjxREIiI1Kp4MkO0BOsfQ4BhYGZ3AWuATjPbAXwZqANw95uDOq6ISLVIpPJLXga9/jEEGAbufsU09r0mqDpERCrVUIkWtgF9A1lEpGzFS7TkJSgMRETK1lAyQ7PCQESktuXXPw7+slJQGIiIlK38ALJ6BiIiNU0DyCIiogFkERHJr2fQrDEDEZHaNbb+scYMRERqVyJVunmJQGEgIlKWhkq4sA0oDEREytLo+scaMxARqWGlnL4aFAYiImVpdMxAp4lERGrY2CpnuppIRKR2DSVLt/4xKAxERMrSaM9AA8giIjVMl5aKiAiJVOnWPwaFgYhIWYonS7f+MSgMRETK0lAJZywFhYGISFkq5fTVoDAQESlL6hmIiAiJVLZk6x+DwkBEpCzFk5mSffsYFAYiImVJp4lERKQwgKzTRCIiNS2eyqpnICJSy9LZHKlM6dY/BoWBiEjZ6YunAGhWz0BEpHbd/cR2AM5eOb9kx1QYiIiUkXgyw+2PbuFtxy7g+MPaSnZchYGISBn5/uN/oD+R5uPnH1XS4yoMRETKxEg6y22PbObslfM5bVl7SY8dWBiY2e1mttvMNkxy/1Vm9pSZPW1mj5nZyUHVIiJSCX64bge7B5NcX+JeAQTbM7gDuHCK+7cAb3H3E4H/BtwaYC0iImUtnc1x80ObOOXweZxzZOkGjkcFFgbu/jDQO8X9j7l7X+HmWmBpULWIiJS7+9e/zI6+Ya5/61ElW9BmvHIZM7gW+EnYRYiIhCGXc256cCPHLprD245bEEoNRYWBmbWYWaTw+9Fm9h4zq5uNAszsreTD4PNT7HOdmXWbWXdPT89sHFZEpGw8+8oAm3riXHvuilB6BVB8z+BhoNHMlgA/B64mPyYwI2Z2EvAt4FJ33zvZfu5+q7uvdvfVXV1dMz2siEhZ2TOUBGBlV2toNRQbBubuCeBy4CZ3fx/whpkc2MyWAfcBV7v7izN5LhGRStaXyE8/0dFSH1oNxU58YWZ2NnAV+VM6AFPOrWpmdwFrgE4z2wF8GagDcPebgf8CzAduKnSLMu6+eroNEBGpdL3xNAAdzeUfBp8Gvgj8X3d/xsxWAr+a6gHufsVB7v9T4E+LPL6ISNXqi6eIGMxpLN3EdAcq6sju/hDwEEBhIHmPu38yyMJERGpFXyJFe3M9kUg4g8dQ/NVE3zezNjNrATYAz5rZ54ItTUSkNvQlUrSHOF4AxQ8gH+/uA8Bl5L8PsIL8FUUiIjJDvfEU7c2zcrX+ISs2DOoK3yu4DLjf3dOAB1eWiEjt6E+kaQ9x8BiKD4NbgK1AC/CwmR0BDARVlIhILemNp0K9rBSKH0C+Abhh3KZthW8Oi4jIDLh75YwZmNlcM/v66JQQZvY18r0EERGZgaFkhnTWK2bM4HZgEHh/4WcA+HZQRYmI1Ir+RP4LZ2GPGRT7DYcj3f2Pxt3+azNbH0RBIiK1pDce/lQUUHzPYNjMzh29YWZvAoaDKUlEpHb0FuYlmlchPYOPAd81s7mF233Ah4MpSUSkdvSVSc+g2KuJngRONrO2wu0BM/s08FSQxYmIVLu+RPiT1ME0Vzpz94HCN5EB/iKAekREakpfPEU0YqFOUgczW/YyvBmVRESqRG8ixbymulAnqYOZhYGmoxARmaG+ePhfOIODjBmY2SATv+kb0BRIRSIiNaQvkQp9vAAOEgbuPqdUhYiI1KK+eJoj5jeHXcaMThOJiMgM9SbCn6QOFAYiIqFxd/rLYJI6UBiIiIRmdJK6chgzUBiIiISkL57/wtm8kGcsBYWBiEhoRucl0piBiEgN6yuEgcYMRERq2OgkdWGvZQAKAxGR0IytZaAwEBGpXX2J8pikDhQGIiKh6UukaW8Of5I6UBiIiISmL54KfYWzUQoDEZGQ9MbLY5I6UBiIiISmL5GivSX8L5yBwkBEJDT5MQP1DEREapa7l83CNqAwEBEJxWAyQyZXHpPUgcJARCQU/YVJ6qq+Z2Bmt5vZbjPbMMn9ZmY3mNlGM3vKzE4LqhYRkXIzOkldexnMWArB9gzuAC6c4v6LgFWFn+uAbwZYi4hIWRmbl6jaewbu/jDQO8UulwLf9by1wDwzWxxUPSIi5aSc5iWCcMcMlgDbx93eUdj2OmZ2nZl1m1l3T09PSYoTEQnS2PTVCoPiufut7r7a3Vd3dXWFXY6IyIyV0yR1EG4Y7AQOH3d7aWGbiEjV642XzyR1EG4Y3A98qHBV0VnAPnd/JcR6RERKpi+eKptTRACB9U/M7C5gDdBpZjuALwN1AO5+M/AAcDGwEUgAfxJULSIi5aYvUSNh4O5XHOR+Bz4e1PFFRMpZXyLFis6WsMsYUxEDyCIi1aY3nqajTL5jAAoDEZGSc3f6E+WzsA0oDERESq7cJqkDhYGISMmV21QUoDAQESm5sakoymSVM1AYiIiU3KsDSQCNGYiI1LJ7f7eDjpZ6jl/cFnYpYxQGIiIltHVPnH977lWuOnMZjXXRsMsZozAQESmhOx7bSixiXH3WEWGX8hoKAxGREhkYSfOD7u28+6TDWNDWGHY5r6EwEBEpkXue2E48leUj564Iu5TXURiIiJRAJpvj249u5YwVHZywZG7Y5byOwkBEpAR+8eyr7Owf5iNvKr9eASgMRERK4p9+vYXDO5p4+/ELwy5lQgoDEZGAPfJSD93b+rjmnBVEy2RlswOVx+KbIiJVaCSd5R9/+RK3PLSJpe1NvH/10rBLmpTCQEQkAOu39/O5HzzJS7uH+MDqw/nSJccxp7F85iI6kMJARGSaXh0YIZHKTrhS2Z6hJDf+8iXuXLuNhW2NfOcjZ/CWo7tCqHJ6FAYiItP0Z3euY/32fs5Y3sFVZy3jwhMWkc4633pkM7c9vJmRTI4rz1zGX154LG1l3BsYT2EgIjIN+4bTPLWjnzNXdLBrYIRP3b2e9uY6Imbsjae4+MRFfPYdx7CyqzXsUqdFYSAiMg3dW3vJOXzqglWctWI+j27aw12//QPJdI7rzz+KU5e1h13iIVEYiIhMw+NbeqmPRjhtWTuRiHHeqi7OW1X+YwIHo+8ZiIhMw9rNeznl8HllNf30bFAYiIgUaXAkzYad+zhrZUfYpcw6hYGISJG6t/aRczhz5fywS5l1CgMRkSKt3byXuqhxWoUOEk9FYSAiUqS1W3o55fB5NNVX13gBKAxERIoylMywYec+zlxRfaeIQGEgIlKU7q29ZHPOWVU4XgAKAxGRoqzd3EssYpx2xLywSwmEwkBEpAiPb9nLyYfPo7m+Or+rqzAQETmIeDLDUzuq8/sFoxQGIiIHsW5bH9mcV+3gMQQcBmZ2oZm9YGYbzewLE9y/zMx+ZWa/N7OnzOziIOsRETkUazfvJRYx3nhE9X2/YFRgYWBmUeAbwEXA8cAVZnb8Abv9Z+Aedz8V+CBwU1D1iIgcikQqw78/v5sTl86lpaE6xwsg2J7BGcBGd9/s7ingbuDSA/ZxoK3w+1zg5QDrERGZlm1741x+02O88OogV591RNjlBCrImFsCbB93ewdw5gH7fAX4uZl9AmgBLpjoiczsOuA6gGXLls16oSIiB/rV87v51N2/JxIxvvMnZ/DmCli6cibCHkC+ArjD3ZcCFwN3mtnranL3W919tbuv7uqq7hdERMLl7tz4y5f4yHeeYGl7Mz+6/tyqDwIItmewEzh83O2lhW3jXQtcCODuvzGzRqAT2B1gXSJSxbbuifO9x7exenkHbzqqk9ZpnOdPZXJ84d6nuO/3O3nvqUv47+89sSrnIZpIkGHwBLDKzFaQD4EPAlcesM8fgLcBd5jZcUAj0BNgTSJS5W55eBN3/XY7tz2yhbqocfryDk5f3kE8meHVwSS7B0YYHMnwrpMWc/XZR4wtWL9vOM3H7lzHbzbv5TNvP5rrzz8KMwu5NaUTWBi4e8bMrgd+BkSB2939GTP7r0C3u98PfAa4zcz+E/nB5Gvc3YOqSUSqWyab4+fPvMpFJyziQ2cv58EXdvPgCz384y9forEuwsK2RhbMaaC5PspXf/YCNz+4iT8++wguOmERn/3Bk2zZE+fr7z+Zy09bGnZTSs4q7b139erV3t3dHXYZIlKGHtu0hytve5ybrjqNi09cPLZ9JJ2lIRZ5zSf9DTv38c2HNvHA06/gDnMaY9zyx2/knKM6wyg9cGa2zt1XT3Z/9V40KyI156cbdtFYF2HNMa8d8J1oveITlszlG1eexpY9ce5dt4NLTzmMVQvnlKrUsqMwEJGqkMs5P92wi7cc3TWtyeRWdLbw2XceE2BllSHsS0tFRGbF77f3sXswyUUnLD74zvI6CgMRqQoPPL2L+miE849bEHYpFUlhICIVzz1/iujcVZ1jl4rK9CgMRKTiPb1zHzv7h7nwhEVhl1KxFAYiUvF+smEX0Yjx9uMWhl1KxaqZMHj25QE+fPtvGRhJh12KiMwid+cnT7/C2Svn095SH3Y5FatmwmAomeHRjXv4zD1PkstV1hftRGRyz+8aZOveBBedqFNEM1Ez3zM4Y0UHX3rXcfz1j57lpgc3cv35q8IuSUQmkM7meOblAbq39tK9tY/ubX3sjSeJmBE1IxIBw8i540A255jBO45XGMxEzYQBwDXnLOfJ7f187RcvcsKSuaw5RpegiZSLeDLD/1m7jdse2cyeoRQAyzqaefOqTpa0N5FzJ5vLnxbKuRMxA8sHw9ELW+ma0xByCypbTYWBmfF3l5/E87sG+dTd6/nR9eeybH5z2GWJVI2BkTRrN+2lvaWeRW2NLGhroCE29RTQQ8kM3/3NVr71yBZ64ynOW9XJB04/nNOXd7CwrbE0hUtthQFAU32UW65+I+++8dd89LvdnLWyg+19w+zoS9AzmOSyU5fwmXccM6050EVqXV88xbcf3cK3H9vK4EjmNfd1tjawsrOFIxe0cmRXC0fMb+Hl/mGee2WA514Z4PldgyQzOd5ydBeffNuqql50vpzV7Kylv3phNx//3u+ImrG0o5ml7U3URyM8sOEVFrc18jfvPYHzj9VlaiKTyeac53cNcP/6l7lz7TYSqSwXvmERHzrnCDJZZ9e+EV7ZN8LO/gSbe+Js7BmiP7H/ar725jqOW9zGcYvbuOSkxZy6TCEQpIPNWlqzYQD5uc+jEXvNtLbrtvXyxfue5sVXh3jXSYv5o9OW0NnaQNecBua3NFAfm/wCrMGRNEPJDIZROJ1JfSzCnMY6opHXLpKRyzn9w2mGRjIsmts45fOWk1Qmx87+YaJmtDbGmNMYoy5aGbXLfsOpLI9u3MODL+5mcCRDc32UproYLQ1RWhtitDXVMbfwE4sYw+ksI+ksiVSWnX3DPLGtj99t62MomSFi8O6TD+Pjbz2Kow8y6+feoSR/6E2weG4TC9saamrxmLApDA5BKpPjloc2ceO/bySVzb3mvjmNMdqb62lvrmNecz2ZXI5d+0bYtW+EeCo74fOZQVtjHfOa62iMRelNpOiNp8gWLnGNRozD25tY2dXKys4WjlrQOvYzr/nQrpvOZHPsHkzycv8wPYNJ9g2nGRhJMzCcYSiZIZ3Nkck66VwOCnO5z22qo62pjpaGGKlMjpF0luF0lqGRDFv3JtjcM8S23sRY3aMaYhFaGmI01UVpqo/SVBclYpDOOtmck8nlcIdIxIgYRCwfwAZjV4ZEI0ZTXTT/plSf/7OtsW6spramGK0NdbQ0RJnTUEdTfZShZIb+RIp9w2n6E2lSmRyZnJPN5fIDjeTrzB8JohGIRiJjf0YMcp4fkHTPvw7N9VGaG2K01EeJRSMMjWTGQj6ezJJ1HxvAhP2v67zmeuY11dFQFyUWMeqiEWJRI53NkUzn/y6TmRyZQm3ZXP7vJld4LnfGBkUbYhHqYxEaYlEikfzfYzqTyz9XJkc8lSFeqGcknSUz+jw5J+eMfRAxMyJmheeK0FAXIWJG99Zefr1xDyPpHK0NMea31pNIZRlOZUmkMhRz5fUxC+ewenk7py/v4MyVHSye23RI/06ldBQGM9AbT7Ftb5w9Qyl6BpP0DCbpS6QKP2n6EymiEWNRWyOL5jayqK2RtqY63PNvRO6QzOQKb1Yp+hNphtNZ5rfU09naQGdrPc31Mbb35bvRm3qG2LInTjKzP4Dmt9Qzv7WeOY11zGmMMaexDncnlcmRyubyb4DZ/BtuJudkss6+4TS7BkZe96YNEDFoaYhRX3izGv1UPziSYWAkzUT/HBpiEZZ1NHPUglaO7GplRWdL4TH5N8nBkQyJVP5TY/7TYwYHYhEjFokQjebfjkff8EavCoH9b4KZnDOcyofPcCpLPJVhYDjDcHrigA3TaKCNXtYYpoZYhFjECkGbD1vYH3I5Z+zfyagl85q44LgFXHD8Qs5cMf81vVJ3J5HKjn142JdIk8n5WMg31UXpaK3X/D8VSIvbzEBHSz0dJf5GYy7n7OwfZuPuIV7aPcjmnjh9iRSDI5lCOCXGTj/VxyLURyPURSM01MWIRYxoJMKxi+awpL2JxXObOGxeIwvmNDK3uY62xhitDbFJu+a5nDOYzH/qrI9FaKqL0lgXfd0prlJKZXIMjqQZGMnXNTiS79kkUplCb6aeec35HkT+jTFCNJLvaUSMsXBzKISQF3oP+U/SETMskv8knc15IdTyn7ozuRytDXVjp8Na6mOv+bsYfePsKwR9fyJNKpslnfWxgK6L5j+VN9ZF8/VFI0TNxmqMRvZ/go9YvoZUoTeRKvTexl7nWD68WxtiYz2xYl+b3OjzZnK0NU7+b8DMaCk8/2Ho034tUc9ARKQGHKxnoJE/ERFRGIiIiMJARERQGIiICAoDERFBYSAiIigMREQEhYGIiFCBXzozsx5g27hNc4F9Rf7eCeyZweHHP+d095lo+4Hbpro9+vv4bTNpz0zaMtl9xdQ/2e96bQ5eZ7H76LV5/etR6W2Z7PfptOcId++a9F4vTLxVqT/ArcX+DnTP1rGmu89E2w/cNtXtcW0Yv+2Q2zOTthxKe/Ta6LUp9WtTTW0Jqj3jf6rhNNGPpvn7bB1ruvtMtP3AbVPd/tEk+xyqmbRlsvuKqX+q32dCr83U99Xia1NNbZnq91lRcaeJZsLMun2KuTkqTTW1p5raAtXVHrWlfM1me6qhZzAdt4ZdwCyrpvZUU1ugutqjtpSvWWtPTfUMRERkYrXWMxARkQkoDERERGEgIiIKgzFmdp6Z3Wxm3zKzx8KuZybMLGJmf2tmN5rZh8OuZ6bMbI2ZPVJ4fdaEXc9MmVmLmXWb2SVh1zJTZnZc4XX5oZn9x7DrmQkzu8zMbjOzfzazd4Rdz0yZ2Uoz+ycz+2Ex+1dFGJjZ7Wa228w2HLD9QjN7wcw2mtkXpnoOd3/E3T8G/Bj4TpD1TmU22gJcCiwF0sCOoGotxiy1x4EhoJEQ2zNLbQH4PHBPMFUWb5b+3zxX+H/zfuBNQdY7lVlqy7+4+0eBjwEfCLLeg5ml9mx292uLPuhsfXstzB/gzcBpwIZx26LAJmAlUA88CRwPnEj+DX/8z4Jxj7sHmFPJbQG+APxZ4bE/rPTXBogUHrcQ+F6Ft+XtwAeBa4BLKv21KTzmPcBPgCsrvS2Fx30NOK0aXpvC44p6D4hRBdz9YTNbfsDmM4CN7r4ZwMzuBi51978DJuyem9kyYJ+7DwZY7pRmoy1mtgNIFW5mg6v24GbrtSnoAxqCqLMYs/TarAFayP8nHjazB9w9F2Tdk5mt18bd7wfuN7N/Bb4fXMWTm6XXxoC/B37i7r8LtuKpzfL/m6JURRhMYgmwfdztHcCZB3nMtcC3A6vo0E23LfcBN5rZecDDQRZ2iKbVHjO7HHgnMA/438GWNm3Taou7fwnAzK4B9oQVBFOY7muzBricfEg/EGhl0zfd/zefAC4A5prZUe5+c5DFHYLpvjbzgb8FTjWzLxZCY1LVHAbT5u5fDruG2eDuCfLBVhXc/T7yAVc13P2OsGuYDe7+IPBgyGXMCne/Abgh7Dpmi7vvJT/+UZSqGECexE7g8HG3lxa2VaJqagtUV3uqqS1QXe2pprZAwO2p5jB4AlhlZivMrJ78oN39Idd0qKqpLVBd7ammtkB1taea2gJBtyfMEfNZHHm/C3iF/ZdSXlvYfjHwIvkR+C+FXWettaXa2lNNbam29lRTW8JqjyaqExGRqj5NJCIiRVIYiIiIwkBERBQGIiKCwkBERFAYiIgICgOpEmY2VOLjzcqaF4W1GvaZ2Xoze97M/mcRj7nMzI6fjeOLjFIYiEzAzKact8vdz5nFwz3i7qcApwKXmNnB1gW4jPyspyKzRmEgVcvMjjSzn5rZOsuvlHZsYfu7zexxM/u9mf2bmS0sbP+Kmd1pZo8CdxZu325mD5rZZjP75LjnHir8uaZw/w8Ln+y/V5gKGTO7uLBtnZndYGY/nqpedx8G1pOfnRIz+6iZPWFmT5rZvWbWbGbnkF8/4KuF3sSRk7VTZDoUBlLNbgU+4e5vBD4L3FTY/mvgLHc/Fbgb+MtxjzkeuMDdryjcPpb89NlnAF82s7oJjnMq8OnCY1cCbzKzRuAW4KLC8bsOVqyZtQOr2D/t+H3ufrq7nww8R35KgsfIz0fzOXc/xd03TdFOkaJpCmupSmbWCpwD/KDwQR32L4yzFPhnM1tMfsWoLeMeen/hE/qof3X3JJA0s93kV1s7cOnN37r7jsJx1wPLyS/TudndR5/7LuC6Sco9z8yeJB8E/+DuuwrbTzCzvyG/jkMr8LNptlOkaAoDqVYRoL9wLv5ANwJfd/f7C4uzfGXcffED9k2O+z3LxP9nitlnKo+4+yVmtgJYa2b3uPt64A7gMnd/srAYzpoJHjtVO0WKptNEUpXcfQDYYmbvg/yShmZ2cuHuueyfB/7DAZXwArBy3NKFB11gvdCL+Hvg84VNc4BXCqemrhq362DhvoO1U6RoCgOpFs1mtmPcz1+QfwO9tnAK5hng0sK+XyF/WmUdsCeIYgqnmv4c+GnhOIPAviIeejPw5kKI/BXwOPAo8Py4fe4GPlcYAD+SydspUjRNYS0SEDNrdfehwtVF3wBecvf/FXZdIhNRz0AkOB8tDCg/Q/7U1C0h1yMyKfUMREREPQMREVEYiIgICgMREUFhICIiKAxERASFgYiIAP8fpeyG/9QV+80AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.524018</td>\n",
       "      <td>0.290019</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.175835</td>\n",
       "      <td>0.345148</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.147354</td>\n",
       "      <td>0.248825</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "And here we creat a @typedispatched impelmentation of `Learner.show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_BaseInput, y, samples, outs, learner=None, ctxs=None, max_n=6, **kwargs):        \n",
    "    \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    \n",
    "    n_preds_per_input = len(outs[0])\n",
    "    if (n_preds_per_input == 1): \n",
    "        for i,ctx in enumerate(ctxs): ctx['target'] = outs[i][0]\n",
    "    else:\n",
    "        for pred_idx in range(n_preds_per_input):\n",
    "            for i,ctx in enumerate(ctxs):  ctx[f'target{pred_idx+1}'] = outs[i][pred_idx]\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I caught Evening in the cinema with a lady friend. Evening is a chick flick with no apologies for being such, but I can say with some relief that it's not so infused with estrogen that it's painful for a red-blooded male to watch. Except for a single instance at the very end of the movie, I watched with interest and did not have to turn away or roll my eyes at any self-indulgent melodrama. Ladies, for their part, will absolutely love this movie.&lt;br /&gt;&lt;br /&gt;Ann Lord is elderly, bed-ridden and spending her last few days on Earth as comfortably as possible in her own home with her two grown daughters at her side. Discomfited by the memories of her past, Ann suddenly calls out a man's name her daughters have never heard before: Harris. While both of her daughters silently contemplate the significance of their mother's strong urge to recall and redress her ill-fated affair with this mysterious man at this of all times, Ann lapses back in her head to the fateful day she met Harris - and in doing so, lost the youthful optimism for the future that we all inevitably part ways with.&lt;br /&gt;&lt;br /&gt;Both Ann and her two daughters - one married with children, one a serial \"commitophobe\" - struggle with the central question of whether true love really exists, and perhaps more importantly, if true love can endure the test of time. Are we all one day fated to realize that love never lasts forever? Will we all realize that settling for the imperfect is the only realistic outcome? The subtle fact that the aged Ann is still wrestling with an answer to these questions on her deathbed is not lost on her two daughters.&lt;br /&gt;&lt;br /&gt;The cinematography for Evening is interesting - most of the film is spent in Ann's mind as she recalls the past, and for that reason I think the film was shot as if it was all deliberately overexposed, to give everyone an ethereal glow (and thus make it very obvious that all of this is not real, but occurred in the past). Claire Danes is beautiful (appearing to be really, really tall, though just 5' 5\" in reality), and is absolutely captivating in one climactic scene where her singing talents are finally put to the test.&lt;br /&gt;&lt;br /&gt;You can't really talk trash about the cast, which leads off with Claire Danes and doesn't let up from there: Vanessa Redgrave, Patrick Wilson, Meryl</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n",
    "    dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    \n",
    "    # this is where we have to change things up since a blurr \"input\" is represented by a dictionary of\n",
    "    # tensors (input_ids, attention_mask, token_type_ids, etc...) and not a single tensor (which fastai assumes\n",
    "    # in a number of places)\n",
    "    b = dl.one_batch()\n",
    "    inp = b[0]\n",
    "    preds, _, dec_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n",
    "    \n",
    "    i = getattr(self.dls, 'n_inp', -1)\n",
    "    inp = (inp,) if i==1 else tuplify(inp)\n",
    "    dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
    "    dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "    res = dec_targ,dec_preds[0],preds[0]\n",
    "    if with_input: res = (dec_inp,) + res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with `summary`, we need to replace fastai's `Learner.predict` method with the one above which is able to work with inputs that are represented by multiple tensors included in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.0383, 0.9617]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict('I really liked the movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.266495</td>\n",
       "      <td>0.324612</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.137106</td>\n",
       "      <td>0.381334</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094272</td>\n",
       "      <td>0.356802</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=slice(1e-6, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An excellent performance by Alix Elias highlights an otherwise mis-directed and confused pile of dreck. I have seen this movie, perhaps 12 times, and with each run through, I find less and less pleasure. Why are Munchies so lustful? Is that ever explained? Are they a reflection of our wanton, boorish 'animal selves?' If they are, why not make it more obvious? Why not peal back just a touch of the subtlety that plagues this movie, and make that connection explicit? Another part of this movie that bothers me to no end - motorcycles. The jacket the little monster wears on the front cover seems to suggest'street-wise' traveler. The sun glasses say 'pretty cool dude.' With all this I'm ready for Easy Rider meets the Muppets. All I get is Munchies (1987). What gives? Stick to the Gremlins series if you're a fan of diminutive, wise-cracking, reptile puppets - it'll give you the treatment you deserve.</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.0082, 0.9918]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"This was a really good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9967, 0.0033]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"Acting was so bad it was almost funny.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname='seq_class_learn_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9778, 0.0222]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn = load_learner(fname='seq_class_learn_export.pkl')\n",
    "inf_learn.blurr_predict(\"This movie should not be seen by anyone!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core training code above works for **all** pretrained sequence classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_albert.AlbertForSequenceClassification,\n",
       " transformers.modeling_auto.AutoModelForSequenceClassification,\n",
       " transformers.modeling_bart.BartForSequenceClassification,\n",
       " transformers.modeling_bert.BertForSequenceClassification,\n",
       " transformers.modeling_camembert.CamembertForSequenceClassification,\n",
       " transformers.modeling_distilbert.DistilBertForSequenceClassification,\n",
       " transformers.modeling_electra.ElectraForSequenceClassification,\n",
       " transformers.modeling_flaubert.FlaubertForSequenceClassification,\n",
       " transformers.modeling_longformer.LongformerForSequenceClassification,\n",
       " transformers.modeling_mobilebert.MobileBertForSequenceClassification,\n",
       " transformers.modeling_roberta.RobertaForSequenceClassification,\n",
       " transformers.modeling_xlm.XLMForSequenceClassification,\n",
       " transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification,\n",
       " transformers.modeling_xlnet.XLNetForSequenceClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='SequenceClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'facebook/bart-base',\n",
    "    'bert-base-uncased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'monologg/electra-small-finetuned-imdb',\n",
    "    'flaubert/flaubert_small_cased', \n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizer\n",
      "model:\t\tAlbertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.671151</td>\n",
       "      <td>0.676016</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen.br /br /the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall.br</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in order to hold the public's attention for three hours, we were treated not so much to a family's romp through four generations and 120 years of hungarian history, as to sexual liaisons with a sister, a sister-in-law and other adulteries. oh yes, there was also a totally gratuitous rape. having said all this, the first story of the relationship among the children of the patriarch was fresh and sensual - thanks to jennifer ehle.</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== facebook/bart-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizer\n",
      "model:\t\tBartForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.386570</td>\n",
       "      <td>0.441389</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We saw the silent version of this film, and it is quite simply shimmeringly beautiful. It's quite hard to see how a sound version could have been created, since it is shot with pure silent technique, long wordless sweeps of narrative without a single intertitle -- save for a few disconcerting sequences where Louise Brooks, playing a French typist, is quite visibly speaking in English... The only section that obviously cries out for sound is the final scene, where Brooks is watching the rushes for her test 'for a sound film': footage which plays constantly in the background as the action unfolds, with her mouth moving in ce</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizer\n",
      "model:\t\tBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.586870</td>\n",
       "      <td>0.582811</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there is no relation at all between fortier and profiler but the fact that both are police series about violent crimes. profiler looks crispy, fortier looks classic. profiler plots are quite simple. fortier's plot are far more complicated... fortier looks more like prime suspect, if we have to spot similarities... the main character is weak and weirdo, but have \" clairvoyance \". people like to compare, to judge, to evaluate. how about just enjoying? funny thing too, people writing fortier looks american but, on the other hand, arguing they prefer american</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizer\n",
      "model:\t\tCamembertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.717043</td>\n",
       "      <td>0.710489</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm so confused. I've been a huge Seagal fan for 25 years. I've seen all of his films, and many of those dozens of times. I can only describe this film as \"bizarre.\" Steven Seagal shares screenplay writing and producing credits on this film, but I have a really tough time believing he would choose to dub over his own voice for so many of his lines, with a thin, whiny imposter</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizer\n",
      "model:\t\tDistilBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.565935</td>\n",
       "      <td>0.518732</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>... that seem to be fooling people into seeing qualities in this film that are just not there. &lt; br / &gt; &lt; br / &gt; near dark covered the same territory but with much more class, and action. &lt; br / &gt; &lt; br / &gt; why the script kept their'big secret'so long was a total mystery to me - i guessed it at the breakfast scene at the start of the film. by the time it was revealed to the viewer it was just a case of'big deal, tell me something i don't know.'&lt; br / &gt; &lt; br / &gt; i found this to</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== monologg/electra-small-finetuned-imdb ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/electra-small-finetuned-imdb were not used when initializing ElectraForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/electra-small-finetuned-imdb and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizer\n",
      "model:\t\tElectraForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.220239</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this movie surprised me in a good way. from the box i got the impression that it was an action thriller but it was too funny to be a thriller, even though it was somewhat exciting. &lt; br / &gt; &lt; br / &gt; there's a lot of nice one - liners and funny situations in this movie and james belushi was born to do bill manucci, he does a great job. the rest of the cast ain't half - bad either and especially timothy dalton is a treat. &lt; br / &gt; &lt; br / &gt; the story can get pretty confusing at times as new characters shows up during the</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_small_cased were not used when initializing FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_small_cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "model:\t\tFlaubertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.729813</td>\n",
       "      <td>0.781687</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since for</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tears of Kali is an original yet flawed horror film that delves into the doings of a cult group in India comprised of German psychologists who have learned how to control their wills and their bodies to the point that they can cause others to be \" healed \" through radical techniques ( that can trigger nightmarish hallucinations and physical pain and torture ) to release the pent-up demons inside them. &lt; br / &gt; &lt; br / &gt; The film is shown as a series of vignettes about</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizer\n",
      "model:\t\tLongformerForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.360690</td>\n",
       "      <td>0.422340</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So, Todd Sheets once stated that he considers his 1993, shot-on-video Z-epic, Zombie Bloodbath to be his first feature film. Anyone who's ever seen a little beauty called Zombie Rampage knows exactly how untrue that statement is. I mean, what makes this one that much more superior? Well, then again, Zombie Rampage doesn't include that mullet guy, now does it? &lt;br /&gt;&lt;br /&gt;For one to comprehend exactly why Zombie Bloodbath is actually considered worth a damn, one must remember what the 90's were like for lovers of bad horror. A decade that all but said</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizer\n",
      "model:\t\tMobileBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>172742.921875</td>\n",
       "      <td>5038.297363</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as a community theater actor who works hard at it but doesn't take acting too seriously, i'm always amused by those who treat it as great art. this movie skewers the \" actor's craft \" mercilessly while dishing up a lot of good laughs. &lt; br / &gt; &lt; br / &gt; a ham actor on location for a movie bears a resemblance to the dictator. when the dictator dies of a heart attack from too much drink and food, the actor is kidnapped and forced to play \" the part of a lifetime \" by the neo - nazi head of the secret service. he plays it to</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizer\n",
      "model:\t\tRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.364680</td>\n",
       "      <td>0.497880</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sure this was a remake of a 70's film, but it had the suspense and action of a current film, say Breakdown. He's running, desperate to be with his hospitalized wife, the police are the least concern. The chases were very good, the part with him being&lt;br /&gt;&lt;br /&gt;cornered at a rest stop was well done, the end of the movie was a great cliffhanger. This is better than Bullitt, a boring movie with what, a muscle car chase that was filmed badly? Vigo's character knew what he had to do to escape Johnny Law, few movies</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "model:\t\tXLMForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.816266</td>\n",
       "      <td>0.773217</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt; br /</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salvage is the worst so called horror film i've ever seen. there is nothing remotely horrific about it. it doesn 't deserve to be in a genre so fine. first of all i don 't see how so many people can think this piece of crap such a great movie. if i wrote something as boring and utterly ridiculous as this i would be laughed at and too embarrassed to subject others to the stupidity of it. second : the acting is terrible and the lead actress is excruciatingly ugly. third : the story sucks, its been used before, and the excuse that its a cheap movie is no excuse. read the</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizer\n",
      "model:\t\tXLMRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.656283</td>\n",
       "      <td>0.635383</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I *loved* the original Scary Movie. I'm a huge fan of parody- it is my favorite form of humor. It is sometimes regarded as the most intelligent form of humor. The Wayans boys seemed to grasp that concept perfectly in the original film, then temporarily forgot it when making the sequel. I think the Wayans' are a family of comical geniuses. Alas, even geniuses make mistakes.&lt;br /&gt;&lt;br /&gt;The movie begins with promise. I liked \"The Exorcist\" parody, especially the \"come on out</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizer\n",
      "model:\t\tXLNetForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.549178</td>\n",
       "      <td>0.505658</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Loulou' delights in the same way an expensive, high quality French wine does. It leaves you with a very fine aftertaste.&lt;br /&gt;&lt;br /&gt;'Loulou's theme isn't new. The film doesn't carry an original plot either. Its colored picturing shows fine, but not extraordinary. Its setting is serious. Its elegant styling never and nowhere puts any weight on your mind.&lt;br /&gt;&lt;br /&gt;Whatever one further may say about 'Loulou', it's beyond doubt that this</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "bsz = 2\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    del learn; torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config_kwargs={'num_labels': 2})\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\nmodel:\\t\\t{type(hf_model).__name__}\\n')\n",
    "\n",
    "    blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, max_length=128, padding='max_length'), \n",
    "              CategoryBlock)\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('text'), \n",
    "                       get_y=ColReader('label'), \n",
    "                       splitter=ColSplitter(col='is_valid'))\n",
    "    \n",
    "    dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "    \n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                    model,\n",
    "                    opt_func=partial(Adam),\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=[accuracy],\n",
    "                    cbs=[HF_BaseModelCallback],\n",
    "                    splitter=hf_splitter)\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), bsz)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, 128]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(len(preds[0]), bsz)\n",
    "        test_eq(preds[0].shape, torch.Size([bsz, 2]))\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max=1e-3)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(max_n=2)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "        \n",
    "# cleanup\n",
    "del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizer</td>\n",
       "      <td>AlbertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizer</td>\n",
       "      <td>BartForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizer</td>\n",
       "      <td>BertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizer</td>\n",
       "      <td>CamembertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizer</td>\n",
       "      <td>DistilBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizer</td>\n",
       "      <td>ElectraForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizer</td>\n",
       "      <td>LongformerForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizer</td>\n",
       "      <td>MobileBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>RobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizer</td>\n",
       "      <td>XLMRobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizer</td>\n",
       "      <td>XLNetForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Multi-label classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a multi-label classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18049"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a dataset with the first 10% of training set\n",
    "raw_data = nlp.load_dataset('civil_comments', split='train[:1%]') \n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat   insult  identity_attack  \\\n",
       "0  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "1  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "2  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "3  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "4  0.893617         0.021277      0.0     0.0  0.87234         0.021277   \n",
       "\n",
       "   sexual_explicit  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame(raw_data, columns=list(raw_data.features.keys()))\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['severe_toxicity',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_attack',\n",
       " 'sexual_explicit']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_cols = list(toxic_df.columns[2:]); lbl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0  0.000000                0        0       0       0                0   \n",
       "1  0.000000                0        0       0       0                0   \n",
       "2  0.000000                0        0       0       0                0   \n",
       "3  0.000000                0        0       0       0                0   \n",
       "4  0.893617                0        0       0       1                0   \n",
       "\n",
       "   sexual_explicit  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = toxic_df.round({col: 0 for col in lbl_cols})\n",
    "toxic_df = toxic_df.convert_dtypes()\n",
    "\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have to configure the `num_labels` to the number of labels we are predicting. Given that our labels are already encoded, we use a `MultiCategoryBlock` with `encoded=True` and `vocab` equal to the columns with our 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), \n",
    "    MultiCategoryBlock(encoded=True, vocab=lbl_cols)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader(lbl_cols), \n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(toxic_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 68]), torch.Size([4, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indeed, we opposed this measure in 2014 and still oppose it. We already have problems with guns on campus and until those issues are resolved we definitely don't need more guns on campus. It's going to cost the campus the equivalent of at least 15 full professors, sometimes I have to wonder why this legislature values guns over people.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics is often a very dirty business. Kitzhauber was guilty of no offense that rendered him unable to fulfill the duties of his office. This appears to again a bit of journalistic shot of getting the facts.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=BCEWithLogitsLossFlat(),\n",
    "                metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.loss_func.thresh = 0.2\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing multi-label classification, we adjust our loss function to use binary cross-entropy and our metrics to use the multi-label friendly version of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(b[0])\n",
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.19054607152938843, lr_steep=0.0010000000474974513)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.064981</td>\n",
       "      <td>0.993026</td>\n",
       "      <td>03:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027629</td>\n",
       "      <td>0.025849</td>\n",
       "      <td>0.991687</td>\n",
       "      <td>03:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042109</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.989885</td>\n",
       "      <td>03:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Wake up and vote for a fair economy, living wages and affordable health care. This year, vote Democratic.\"\\n\\nThat's what you said in 2008.  And 2012.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Really, it's not about taxes, government debt, budget inadequacies, etc etc. It's more about our system. How we do things. \\nWe voted. It's obvious what we collectively think. Now, is our system going to fail our voices? Will the \"Superdelegate\" thing really be what counts? More than individual votes?</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func.thresh = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((#1) ['severe_toxicity'],\n",
       " tensor([False, False, False,  True, False, False]),\n",
       " tensor([1.2476e-05, 1.0716e-02, 1.5172e-03, 9.6402e-02, 3.9685e-03, 3.5775e-03]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = \"\"\"\n",
    "Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. \n",
    "No enchiladas for them!\n",
    "\"\"\"\n",
    "learn.blurr_predict(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
