{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.summarization\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "from rouge_score import rouge_scorer, scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "The objective of summarization is to generate a concise and accurate representation of a much larger body of text.  For example, we may want to summarize an article in a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./')\n",
    "cnndm_df = pd.read_csv(path/'cnndm_sample.csv'); len(cnndm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN)  -- Globalization washes like a flood over the world's cultures and economies. Floods can be destructive; however, they can also bring blessings, as the annual floods of the Nile did for ancient Egypt. The world's great universities can be crucial instruments in shaping, in a positive way, humankind's reaction to globalization and the development of humankind itself. Traditionally, universities have been defined and limited by location, creating an academic community and drawing students and scholars to that place. Eventually, some universities began to encourage students to study el...</td>\n",
       "      <td>John Sexton: Traditionally, universities have been defined and limited by location .\\nGlobal campuses form a network of thought, innovation, he writes .\\nFaculty can teach, Sexton says, students can team up in many cities at once .\\nSexton: Research, scholarship can be shared and cultural ties made in \"century of knowledge\"</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Armenian President Robert Kocharian declared a state of emergency Saturday night after a day of clashes between police and protesters, a spokeswoman for the Armenian Foreign Ministry said. Opposition supporters wave an Armenian flag during a protest rally in Yerevan, Armenia, on Saturday. The protesters claim last month's presidential election was rigged. The state of emergency will \"hopefully bring some order\" to the capital, Yerevan, said Salpi Ghazarian, assistant to the Armenian foreign minister, who spoke to CNN early Sunday. The state of emergency could last until March 20, ...</td>\n",
       "      <td>NEW: Protest moves after crackdown at Freedom Square .\\nOrder sought after protests over last month's election turn violent .\\nDemonstrators say the election was fraudulent .\\nState of emergency could last until March 20, official says .</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   article  \\\n",
       "0  (CNN)  -- Globalization washes like a flood over the world's cultures and economies. Floods can be destructive; however, they can also bring blessings, as the annual floods of the Nile did for ancient Egypt. The world's great universities can be crucial instruments in shaping, in a positive way, humankind's reaction to globalization and the development of humankind itself. Traditionally, universities have been defined and limited by location, creating an academic community and drawing students and scholars to that place. Eventually, some universities began to encourage students to study el...   \n",
       "1  (CNN) -- Armenian President Robert Kocharian declared a state of emergency Saturday night after a day of clashes between police and protesters, a spokeswoman for the Armenian Foreign Ministry said. Opposition supporters wave an Armenian flag during a protest rally in Yerevan, Armenia, on Saturday. The protesters claim last month's presidential election was rigged. The state of emergency will \"hopefully bring some order\" to the capital, Yerevan, said Salpi Ghazarian, assistant to the Armenian foreign minister, who spoke to CNN early Sunday. The state of emergency could last until March 20, ...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                              highlights  \\\n",
       "0  John Sexton: Traditionally, universities have been defined and limited by location .\\nGlobal campuses form a network of thought, innovation, he writes .\\nFaculty can teach, Sexton says, students can team up in many cities at once .\\nSexton: Research, scholarship can be shared and cultural ties made in \"century of knowledge\"   \n",
       "1                                                                                          NEW: Protest moves after crackdown at Freedom Square .\\nOrder sought after protests over last month's election turn violent .\\nDemonstrators say the election was fraudulent .\\nState of emergency could last until March 20, official says .   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnndm_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['final_logits_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bart',\n",
       " transformers.configuration_bart.BartConfig,\n",
       " transformers.tokenization_bart.BartTokenizer,\n",
       " transformers.modeling_bart.BartForConditionalGeneration)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               model_cls=BartForConditionalGeneration)\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_batch_tfm = HF_SummarizationBatchTransform(hf_arch, hf_tokenizer)\n",
    "\n",
    "blocks = ( \n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, max_length=512), \n",
    "    HF_TextBlock(hf_arch, hf_tokenizer, hf_batch_tfm=hf_batch_tfm, max_length=150, hf_input_idxs=[0,1])\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('article'), \n",
    "                   get_y=ColReader('highlights'), \n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(cnndm_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([2, 512]), torch.Size([2, 61]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (CNN) -- A U.S. professor claims he has identified the parts of the brain that help to make someone a good leader. Pierre Balthazard is using EEG to find out what parts of the brain are involved in leadership. Pierre Balthazard, an associate professor at the Carey School of Business at Arizona State University, also says he can use neuroscientific techniques to help people improve the skills that play a part in leadership. Balthazard uses electroencephalography (EEG) to produce a \"brain map\" of his subjects. By attaching electrodes to their heads, he says he can measure electrical activity generated by neurons in their brain. Much of his work has focused on calibrating the EEG data with standard psychometric tests, and now Balthazard says that just by looking at someone's brain map he can predict their capacity for certain traits linked to leadership. \"From someone's brain map I can tell if someone would rank high, medium or low on a psychometric assessment of their transformational leadership, and just that is an earth-shattering finding,\" he told CNN. He has been working with the U.S. military to produce a model that will allow them to scan soldiers' brains for complexity. The idea is that more complex brains produce better situational awareness and adaptive thinking -- essential skills for the modern soldier, who must be able to transition from front-line combat to nation building. He refers to traits like complexity and transformational leadership as antecedents to leadership itself. But for Balthazard, the ability to assess these skills is only half the story. What really excites him is the possibility of brain training and improving leadership skills. \"If you could only assess and not develop then it's only an exercise in social engineering, and that's of no interest to me,\" he said. Balthazard explained that brains can be trained using positive and negative reinforcement, in the same way that disorders like ADD are treated. A subject is wired to software programmed to recognize \"correct\" functioning of a specific part of the brain. If the brain isn't performing correctly, there is a negative reinforcement, such as a noise emitted from a speaker at an unpleasant frequency. \"The brain is amazing at adjusting so it doesn't get the negative feedback,\" he told CNN. But others think it may prove difficult to develop something as intangible as leadership. Dr Bob Kentridge, a member of the Cognitive Neuroscience Research Unit at Durham University, in England, told CNN, \"Even if you find differences</td>\n",
       "      <td>Researcher says he can spot a good leader just by scanning their brain.\\nPierre Balthazard is a business professor who uses neuroscientific methods.\\nThere is no correlation between intelligence and leadership, he says.\\nHe hopes to improve people's leadership by working on brain function.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON (CNN)  -- A government report released Thursday paints an alarming picture of an unstable future for international relations defined by waning American influence, a fragmentation of political power and intensifying struggles for increasingly scarce natural resources. The report aims to better inform policymakers, starting with the administration of President-elect Barack Obama. The report, \"Global Trends 2025: A Transformed World,\" was drafted by the National Intelligence Council to better inform U.S. policymakers -- starting with the incoming administration of President-elect Barack Obama -- about the factors most likely to shape major international trends and conflicts through the year 2025. \"Although the United States is likely to remain the single most powerful actor, the United States' relative strength -- even in the military realm -- will decline and U.S. leverage will become more constrained,\" says the report, which is the fourth in a series from the Intelligence Council. The report argues that the \"international system -- as constructed following the second World War -- will be almost unrecognizable by 2025 owing to the rise of emerging powers, a globalizing economy, an historic transfer of relative wealth and economic power from West to East, and the growing influence of nonstate actors.\" It argues that the world is in the midst of an unprecedented \"transfer of global wealth and power\" -- from West to East -- that is being fueled by long-term \"increases in oil and commodity prices\" along with a gradual shift of manufacturing and certain service industries to Asia. And yet, while American power and influence are projected to decline, America's burdens are not. \"Despite the recent rise in anti-Americanism, the U.S. probably will continue to be seen as a much-needed regional balancer in the Middle East and Asia,\" the report notes. The American military will continue to be expected to play a leading role in the war against global terrorism, though the United States as a whole will be less able to \"call the shots without the support of strong partnerships.\" America's biggest rival by 2025, the reports says, will be China. \"China is poised to have more impact on the world over the next 20 years than any other country,\" it notes. The report projects that China will have the world's second largest economy by 2025 and will be a leading military power. Equally problematic for U.S. policymakers is the fact that China is expected to become the world's biggest polluter and largest importer of natural resources. China will not be alone, however, in terms of its desire to provide a consumption-</td>\n",
       "      <td>Report says China will have growing impact, second largest economy by 2025.\\nThere will be an unprecedented global transfer of power because of oil, report says.\\nIndonesia, Iran, Turkey, will likely see power, desire for natural resources increase.\\n\"Unprecedented\" growth means demand for basic resources will outweigh supply.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for summarization tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_rouge(predicted_txts, reference_txts, rouge_keys=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True):\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "    for ref_text, pred_txt in zip(reference_txts, predicted_txts):\n",
    "        scores = scorer.score(ref_text, pred_txt)\n",
    "        aggregator.add_scores(scores)\n",
    "\n",
    "    result = aggregator.aggregate()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here we create a summarization specific subclass of `HF_BaseModelCallback` in order to include custom, summarization specific, metrics, and also handle the pre-calculated loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_SummarizationModelCallback(HF_BaseModelCallback):  \n",
    "    def __init__(self, rouge_metrics=[\"rouge1\", \"rouge2\", \"rougeL\"], text_gen_kwargs={}, **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self=self, names='rouge_metrics, text_gen_kwargs, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in rouge_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the target's HF_TokenizerTransform (used for rouge metrics)\n",
    "        hf_textblock_tfm = self.learn.dls.tfms[-1]\n",
    "        self.hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "        self.tok_kwargs = hf_textblock_tfm.kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "    def before_fit(self): self.setup()\n",
    "        \n",
    "        \n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self): self.hf_loss = None\n",
    "        \n",
    "    def after_pred(self): \n",
    "        # the \"labels\" key will only be included in the input dictionary *IF* we are training with target labels, \n",
    "        # in which case the first output of the model will be the loss\n",
    "        if ('labels' in self.xb[0]):\n",
    "            self.hf_loss, self.learn.pred = self.pred[0], self.pred[1]\n",
    "        else:\n",
    "            self.learn.pred = self.pred[0]\n",
    "            \n",
    "    def after_loss(self): \n",
    "        # if we already have the loss from the model, update the Learner's loss to be it\n",
    "        if (self.hf_loss is not None): self.learn.loss = self.hf_loss\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if (self.training or self.learn.y is None): return\n",
    "        \n",
    "        # grab predicted and reference ids for any metrics that need them\n",
    "        input_ids, attention_mask = self.xb[0]['input_ids'], self.xb[0]['attention_mask']\n",
    "        gen_ids = self.learn.model.hf_model.generate(input_ids=input_ids, \n",
    "                                                     attention_mask=attention_mask, \n",
    "                                                     use_cache=True,\n",
    "                                                     **self.text_gen_kwargs)\n",
    "        \n",
    "        self.generated_ids += gen_ids.tolist()\n",
    "        self.refernce_ids += self.yb[0].tolist()\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self): self.generated_ids, self.refernce_ids = [], []\n",
    "        \n",
    "    def after_validate(self):\n",
    "        # are there rouge metrics to calculate?\n",
    "        if (self.rouge_metrics is not None and len(self.rouge_metrics) > 0):\n",
    "            gen_texts = self.hf_tokenizer.batch_decode(self.generated_ids, \n",
    "                                                       skip_special_tokens=True, \n",
    "                                                       clean_up_tokenization_spaces=True)\n",
    "\n",
    "            ref_texts = self.hf_tokenizer.batch_decode(self.refernce_ids, \n",
    "                                                       skip_special_tokens=True, \n",
    "                                                       clean_up_tokenization_spaces=True)\n",
    "\n",
    "            rouge_results = calculate_rouge(gen_texts, ref_texts, rouge_keys=self.rouge_metrics)\n",
    "            \n",
    "            for rouge_key, scores in rouge_results.items(): \n",
    "                self.custom_metrics_dict[rouge_key] = scores.mid.fmeasure\n",
    "                \n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a custom param splitter to give us a bit more depth in applying discriminative learning rates for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarization_splitter(m, arch):\n",
    "    \"\"\"Custom param splitter for summarization models\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, 'hf_model')) else m\n",
    "    \n",
    "    if arch in ['bart', 'pegasus']:     \n",
    "        embeds = nn.Sequential(\n",
    "            model.model.shared, \n",
    "            model.model.encoder.embed_positions, \n",
    "            model.model.encoder.embed_tokens,\n",
    "            model.model.decoder.embed_positions, \n",
    "            model.model.decoder.embed_tokens\n",
    "        )\n",
    "        \n",
    "        groups = L(embeds, model.model.encoder, model.model.decoder)\n",
    "        return groups.map(params).filter(lambda el: len(el) > 0)\n",
    "    \n",
    "    raise ValueError('Invalid architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"summarization_splitter\" class=\"doc_header\"><code>summarization_splitter</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>summarization_splitter</code>(**`m`**, **`arch`**)\n",
       "\n",
       "Custom param splitter for summarization models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(summarization_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we don't really need a loss function, we have to provide a custom loss class/function for fastai to function properly (e.g. one with a `decodes` and `activation` methods).  Why?  Because these methods will get called in methods like `show_results` to get the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_MaskedLMLoss():\n",
    "    def __call__(self, inp, targ, **kwargs): return\n",
    "    def decodes(self, x): return x.argmax(dim=-1)\n",
    "    def activation(self, x): return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': True,\n",
       " 'length_penalty': 2.0,\n",
       " 'max_length': 130,\n",
       " 'min_length': 30,\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'num_beams': 4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_kwargs = { **hf_config.task_specific_params['summarization'], **{'max_length': 130, 'min_length': 30} }\n",
    "text_gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "model_cb = HF_SummarizationModelCallback(text_gen_kwargs=text_gen_kwargs)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=ranger,\n",
    "                loss_func=HF_MaskedLMLoss(),\n",
    "                cbs=[model_cb],\n",
    "                splitter=partial(summarization_splitter, arch=hf_arch))#.to_fp16()\n",
    "\n",
    "learn.create_opt() \n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " tensor(3.5636, device='cuda:1', grad_fn=<NllLossBackward>),\n",
       " torch.Size([2, 58, 50264]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0], preds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([2, 512]), 2, torch.Size([2, 59]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.00020892962347716094, lr_steep=8.31763736641733e-06)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.477953</td>\n",
       "      <td>1.530667</td>\n",
       "      <td>0.384321</td>\n",
       "      <td>0.169405</td>\n",
       "      <td>0.257252</td>\n",
       "      <td>04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.121308</td>\n",
       "      <td>1.541027</td>\n",
       "      <td>0.386247</td>\n",
       "      <td>0.170724</td>\n",
       "      <td>0.266644</td>\n",
       "      <td>04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.979160</td>\n",
       "      <td>1.559571</td>\n",
       "      <td>0.383187</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>0.262079</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to take advantage of huggingface's `PreTrainedModel.generate` model, which can be used to easily implement beam search, top-k/nucleous sampling, etc... so that we get more human sounding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = \"\"\"\n",
    "About 10 men armed with pistols and small machine guns raided a casino in Switzerland and made off \n",
    "into France with several hundred thousand Swiss francs in the early hours of Sunday morning, police said. \n",
    "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino \n",
    "Basel, Chief Inspector Peter Gill told CNN. One group tried to break into the casino's vault on the lower level \n",
    "but could not get in, but they did rob the cashier of the money that was not secured, he said. The second group \n",
    "of armed robbers entered the upper level where the roulette and blackjack tables are located and robbed the \n",
    "cashier there, he said. As the thieves were leaving the casino, a woman driving by and unaware of what was \n",
    "occurring unknowingly blocked the armed robbers' vehicles. A gunman pulled the woman from her vehicle, beat \n",
    "her, and took off for the French border. The other gunmen followed into France, which is only about 100 \n",
    "meters (yards) from the casino, Gill said. There were about 600 people in the casino at the time of the robbery. \n",
    "There were no serious injuries, although one guest on the Casino floor was kicked in the head by one of the \n",
    "robbers when he moved, the police officer said. Swiss authorities are working closely with French authorities, \n",
    "Gill said. The robbers spoke French and drove vehicles with French license plates. CNN's Andreena Narayan \n",
    "contributed to this report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 10About 10 men a\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(test_article)\n",
    "print(res[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look much like a human-generated summary.  Let's use huggingface's `PreTrainedModel.generate` method to create something more human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Target ===\n",
      " Researcher says he can spot a good leader just by scanning their brain .\n",
      "Pierre Balthazard is a business professor who uses neuroscientific methods .\n",
      "There is no correlation between intelligence and leadership, he says .\n",
      "He hopes to improve people's leadership by working on brain function .\n",
      "\n",
      "=== Prediction ===\n",
      " U.S. professor Pierre Balthazard says he can use neuroscientific techniques to help people improve leadership skills .\n",
      "He uses electroencephalography (EEG) to produce a \"brain map\" of his subjects' brains .\n",
      "Balthazard: \"From someone's brain map I can tell if someone would rank high, medium or low on a psychometric assessment\"\n",
      "He is working with the military to develop a model that will scan soldiers' brains for complexity .\n"
     ]
    }
   ],
   "source": [
    "test_input_ids = dls.train_ds[0][0]['input_ids'].unsqueeze(0).to(learn.model.hf_model.device)\n",
    "gen_text = learn.model.hf_model.generate(test_input_ids, num_beams=4, max_length=130, min_length=30)\n",
    "\n",
    "print('=== Target ===')\n",
    "print(f'{hf_tokenizer.decode(dls.train_ds[0][1][\"input_ids\"], skip_special_tokens=True, clean_up_tokenization_spaces=False)}\\n')\n",
    "\n",
    "print('=== Prediction ===')\n",
    "print(hf_tokenizer.decode(gen_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add a `blurr_summarize` method to `Learner` that uses huggingface's `PreTrainedModel.generate` to create our predictions.  For the full list of arguments you can pass in see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate). You can also check out their [\"How To Generate\"](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb) notebook for more information about how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summarize(self:Learner, inp, **kwargs):\n",
    "    \"\"\"Uses the built-in `generate` method to generate the text \n",
    "    (see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)\n",
    "    for a list of arguments you can pass in)\n",
    "    \"\"\"\n",
    "    # grab the text generation kwargs\n",
    "    text_gen_kwargs = self.cbs.filter(lambda el: isinstance(el, HF_SummarizationModelCallback) )[0].text_gen_kwargs\n",
    "    text_gen_kwargs = { **text_gen_kwargs, **kwargs}\n",
    "    \n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.tfms[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_textblock_tfm.kwargs\n",
    "\n",
    "    if (isinstance(inp, str)):\n",
    "        input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors='pt', **tok_kwargs)\n",
    "    else:\n",
    "        input_ids = inp\n",
    "        \n",
    "    input_ids = input_ids.to(self.model.hf_model.device)\n",
    "    \n",
    "    gen_texts = self.model.hf_model.generate(input_ids, **text_gen_kwargs)\n",
    "    outputs = [ hf_tokenizer.decode(txt, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n",
    "               for txt in gen_texts ]\n",
    "    \n",
    "    if hf_textblock_tfm.hf_arch == 'pegasus':\n",
    "        outputs = [o.replace('<n>', ' ') for o in outputs]\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_summarize\" class=\"doc_header\"><code>Learner.blurr_summarize</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_summarize</code>(**`inp`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Uses the built-in `generate` method to generate the text \n",
       "(see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)\n",
       "for a list of arguments you can pass in)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction 1 ===\n",
      " About 10 men armed with pistols and small machine guns raid a casino in Switzerland and make off with several hundred thousand Swiss francs .\n",
      "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\n",
      "One group tried to break into the casino's vault on the lower level but could not get in .\n",
      "Another group entered the upper level where the roulette and blackjack tables are located and robbed the cashier there .\n",
      "There were about 600 people in the casino at the time of the robbery .\n",
      "\n",
      "=== Prediction 2 ===\n",
      " About 10 men armed with pistols and small machine guns raid a casino in Switzerland and make off with several hundred thousand Swiss francs .\n",
      "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\n",
      "One group tried to break into the casino's vault on the lower level but could not get in .\n",
      "Another group entered the upper level where the roulette and blackjack tables are located and robbed the cashier there .\n",
      "\n",
      "=== Prediction 3 ===\n",
      " About 10 men armed with pistols and small machine guns raid a casino in Switzerland and make off with several hundred thousand Swiss francs .\n",
      "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\n",
      "One group tried to break into the casino's vault on the lower level but could not get in .\n",
      "Another group entered the upper level where the roulette and blackjack tables are located and robbed the cashier .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = learn.blurr_summarize(test_article, num_return_sequences=3)\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much nicer!!! Now, we can update our @typedispatched `show_results` to use this new method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_SummarizationInput, y, samples, outs, learner, ctxs=None, max_n=6, **kwargs):  \n",
    "    gen_text_txts = learner.blurr_summarize(x[0])\n",
    "    res = L([ (sample[0], sample[1], gen_txt) for sample, gen_txt in zip(samples, gen_text_txts) ])          \n",
    "    \n",
    "    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London (CNN) -- Computer hacker Gary McKinnon will face no charges in the United Kingdom over what the U.S. government says is the biggest military computer hacking of all time, British prosecutors said Friday. The Crown Prosecution Service announcement follows a decision two months ago by Home Secretary Theresa May to block his extradition to the United States to face trial on health grounds. She said then that UK prosecutors must decide if a new criminal case should be opened in England. Read more: UK blocks hacker McKinnon's extradition to U.S. The prosecution service and London's Metropolitan Police said in a joint statement that their decision not to do so reflected the difficulty of trying the case in Britain when most of the evidence and witnesses are in the United States. \"The prospects of a conviction against Mr. McKinnon, which reflects the full extent of his alleged criminality, are not high,\" the statement said. UK authorities believe the United States is the appropriate place for the case to be tried, it said. McKinnon fought a decade-long battle against extradition before May's move to block it in October because of his health. The 46-year-old has Asperger syndrome and depressive illness, as well as schizophrenia and other mental health issues. He has admitted breaking into computers at NASA and the Pentagon, but says he did so to find out if the U.S. government was covering up the existence of UFOs. The U.S. government says McKinnon accessed 97 computers from his home in London for a year starting in March 2001, costing the government about $1 million. He is accused of breaking into military, NASA and civilian networks, and accessing computers at the Pentagon; Fort Benning, Georgia; Fort Meade, Maryland; the Earle Naval Weapons Station in Colts Neck, New Jersey; and the Johnson Space Center in Houston, among others. In one case, McKinnon allegedly crashed computers belonging to the Military District of Washington.</td>\n",
       "      <td>Gary McKinnon will not face a new criminal investigation in England, prosecutors say.\\nHe hacked into U.S. military and civilian computers over a decade ago.\\nMcKinnon's extradition was blocked in October on health grounds.\\nThe U.S. government said his actions cost it about $1 million.</td>\n",
       "      <td>NEW: UK authorities believe the United States is the appropriate place for the case to be tried, prosecutors say .\\nGary McKinnon accessed 97 computers from his home in London for a year starting in March 2001, the U.S. government says .\\nHe is accused of breaking into military, NASA and civilian networks, and accessing computers at the Pentagon .\\nThe 46-year-old has Asperger syndrome and depression, as well as schizophrenia and other mental health issues .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- The steep rock walls of Thailand's Hellfire Pass symbolize the slavery, starvation, torture and lost lives of thousands of POWs and Asian civilians during World War II, when Japan forced them to build the infamous Death Railway to boost its invasion of Burma. At today's Hellfire Pass Memorial Museum, visitors can share the misery and memories of those years, and learn from survivors who tell what happened and why. Recently named the best museum in Thailand and ranked among the top five in Asia by TripAdvisor, the haunting site and museum are called a \"must\" by the travel website. Fifty miles away, in the town of Kanchanaburi (about 75 miles west of Bangkok, near Thailand's border with Myanmar), evocative ceremonies take place annually from November 28 to December 7 during River Kwai Bridge Week, commemorating the Allied bombing of the area, which began on November 28, 1944. Events include cultural performances, plus a sound and light show portraying the history of the Death Railway, which crossed the so-called Bridge on the River Kwai on its route through Hellfire Pass. More than 16,000 enslaved British, Dutch, Australian and American POWs perished at these sites. More than 90,000 Asians also died from starvation and disease during their forced labor, according to the United Kingdom's Forces War Records. Together, the sites make this corner of jungle in Thailand one of the most sobering and evocative places on the planet for travelers with an interest in the horrifying events of World War II. Fatal railway. \"Hellfire Pass\" and \"Death Railway\" are terms coined by POWs whose fate was sealed in a sinister, deadly agreement. In August 1942, Thailand's prime minister Field Marshal Phibun Songkhram signed an agreement with Japan to allow the laying of a single-meter-gauge railway track toward British colonial Burma (now Myanmar). In October 1943, Japan began transporting troops and supplies on that finished railway through newly carved Hellfire Pass, to support its 1942 Burma invasion. For slave labor, Tokyo used more than 60,000 POWs -- 30,000 British, 18,000 Dutch, 13,000 Australians and 700 Americans. Most were captured when Britain surrendered its colonial hold on Singapore in February 1942, while other POWs were seized in Britain's Malaysia and Dutch-held Indonesia. About one-fifth of the POWs died from untreated diseases and starvation -- they were given watery gruel rations and sadistic punishments while building Hellfire Pass and the Death</td>\n",
       "      <td>Named the best museum in Thailand, Hellfire Pass Memorial Museum commemorates the notorious Death Railway.\\nDuring World War II, thousands of POWs and workers died building a railway for Japanese military.\\nVisitors can walk along the narrow trench of Hellfire Pass and other WWII monuments nearby.\\nAudio headset includes voices of survivors who describe atrocities they endured.</td>\n",
       "      <td>Japan used more than 60,000 POWs for slave labor to build the infamous Death Railway to boost its invasion of Burma .\\nMore than 90,000 Asians also died from starvation and disease during their forced labor, according to UK's Forces War Records .\\nThe Hellfire Pass Memorial Museum in Thailand was recently named the best museum in Thailand and ranked among the top five in Asia by TripAdvisor .\\nCeremonial ceremonies take place annually from November 28 to December 7 in Kanchanaburi commemorating the Allied bombing of the area .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname='summarize_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" About 10 men armed with pistols and small machine guns raid a casino in Switzerland and make off with several hundred thousand Swiss francs .\\nThe men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\\nOne group tried to break into the casino's vault on the lower level but could not get in .\\nAnother group entered the upper level where the roulette and blackjack tables are located and robbed the cashier there .\\nThere were about 600 people in the casino at the time of the robbery .\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn = load_learner(fname='summarize_export.pkl')\n",
    "inf_learn.blurr_summarize(test_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
