{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.summarization\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "from rouge_score import rouge_scorer, scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "The objective of summarization is to generate a concise and accurate representation of a much larger body of text.  For example, we may want to summarize an article in a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./')\n",
    "cnndm_df = pd.read_csv(path/'cnndm_sample.csv'); len(cnndm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN)  -- Globalization washes like a flood over the world's cultures and economies. Floods can be destructive; however, they can also bring blessings, as the annual floods of the Nile did for ancient Egypt. The world's great universities can be crucial instruments in shaping, in a positive way, humankind's reaction to globalization and the development of humankind itself. Traditionally, universities have been defined and limited by location, creating an academic community and drawing students and scholars to that place. Eventually, some universities began to encourage students to study el...</td>\n",
       "      <td>John Sexton: Traditionally, universities have been defined and limited by location .\\nGlobal campuses form a network of thought, innovation, he writes .\\nFaculty can teach, Sexton says, students can team up in many cities at once .\\nSexton: Research, scholarship can be shared and cultural ties made in \"century of knowledge\"</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Armenian President Robert Kocharian declared a state of emergency Saturday night after a day of clashes between police and protesters, a spokeswoman for the Armenian Foreign Ministry said. Opposition supporters wave an Armenian flag during a protest rally in Yerevan, Armenia, on Saturday. The protesters claim last month's presidential election was rigged. The state of emergency will \"hopefully bring some order\" to the capital, Yerevan, said Salpi Ghazarian, assistant to the Armenian foreign minister, who spoke to CNN early Sunday. The state of emergency could last until March 20, ...</td>\n",
       "      <td>NEW: Protest moves after crackdown at Freedom Square .\\nOrder sought after protests over last month's election turn violent .\\nDemonstrators say the election was fraudulent .\\nState of emergency could last until March 20, official says .</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   article  \\\n",
       "0  (CNN)  -- Globalization washes like a flood over the world's cultures and economies. Floods can be destructive; however, they can also bring blessings, as the annual floods of the Nile did for ancient Egypt. The world's great universities can be crucial instruments in shaping, in a positive way, humankind's reaction to globalization and the development of humankind itself. Traditionally, universities have been defined and limited by location, creating an academic community and drawing students and scholars to that place. Eventually, some universities began to encourage students to study el...   \n",
       "1  (CNN) -- Armenian President Robert Kocharian declared a state of emergency Saturday night after a day of clashes between police and protesters, a spokeswoman for the Armenian Foreign Ministry said. Opposition supporters wave an Armenian flag during a protest rally in Yerevan, Armenia, on Saturday. The protesters claim last month's presidential election was rigged. The state of emergency will \"hopefully bring some order\" to the capital, Yerevan, said Salpi Ghazarian, assistant to the Armenian foreign minister, who spoke to CNN early Sunday. The state of emergency could last until March 20, ...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                              highlights  \\\n",
       "0  John Sexton: Traditionally, universities have been defined and limited by location .\\nGlobal campuses form a network of thought, innovation, he writes .\\nFaculty can teach, Sexton says, students can team up in many cities at once .\\nSexton: Research, scholarship can be shared and cultural ties made in \"century of knowledge\"   \n",
       "1                                                                                          NEW: Protest moves after crackdown at Freedom Square .\\nOrder sought after protests over last month's election turn violent .\\nDemonstrators say the election was fraudulent .\\nState of emergency could last until March 20, official says .   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnndm_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# pretrained_model_name = \"t5-small\"\n",
    "# hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "#                                                                                model_cls=T5ForConditionalGeneration)\n",
    "\n",
    "# pretrained_model_name = \"google/pegasus-cnn_dailymail\"\n",
    "# hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "#                                                                                model_cls=PegasusForConditionalGeneration)\n",
    "\n",
    "# pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "# hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "#                                                                                model_cls=BartForConditionalGeneration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bart',\n",
       " transformers.configuration_bart.BartConfig,\n",
       " transformers.tokenization_bart.BartTokenizer,\n",
       " transformers.modeling_bart.BartForConditionalGeneration)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               model_cls=BartForConditionalGeneration)\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_batch_tfm = HF_SummarizationBatchTransform(hf_arch, hf_tokenizer, max_length=[256, 130])\n",
    "\n",
    "blocks = (HF_TextBlock(hf_batch_tfm=hf_batch_tfm), noop)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('article'), \n",
    "                   get_y=ColReader('highlights'), \n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(cnndm_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([2, 256]), torch.Size([2, 84]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN) -- Home to up to 10 percent of all known species, Mexico is recognized as one of the most biodiverse regions on the planet. The twin threats of climate change and human encroachment on natural environments are, however, threatening the existence of the country's rich wildlife. And there is a great deal to lose. In the United Nations Environment Program (UNEP) World Conservation Monitoring Centre's list of megadiverse countries Mexico ranks 11th. The list represents a group of 17 countries that harbor the majority of the Earth's species and are therefore considered extremely biodiverse. From its coral reefs in the Caribbean Sea to its tropical jungles in Chiapas and the Yucatan peninsula and its deserts and prairies in the north, Mexico boasts an incredibly rich variety of flora and fauna. Some 574 out of 717 reptile species found in Mexico -- the most in any country -- can only be encountered within its borders. It is home to 502 types of mammals, 290 species of birds, 1,150 varieties of birds and 26,000 classifications of plants. Pronatura, a non-profit organization that works to promote conservation and sustainable development in Mexico, has selected six species which it says symbolize the problems faced by the</td>\n",
       "      <td>Mexico hosts to up to 10 percent of all known species on Earth.\\nIt is home to 502 types of mammals, 290 bird species and 26,000 types of plants.\\nHuman development and climate change is placing a big strain on its biodiversity.\\nThe Golden Eagle is under threat in spite of being the country's national symbol.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael Zehaf-Bibeau, the 32-year-old gunman who attacked the Canadian Parliament and killed a soldier last week, had a familiar profile. It is that of a young man alienated from mainstream society, with few friends, without a steady job, drifting from one place to another -- often with a history of petty crime and drug abuse. Then comes the conversion to or rediscovery of Islam, and the adoption of a jihadist mindset, fed by media and online coverage of the West's involvement in wars in Iraq and Afghanistan, and by the well-oiled propaganda machine of groups like ISIS. Whether these young men acting alone (and they are almost always men) should be better described as \"lone-wolf\" terrorists or deranged criminals is debatable. \"There is no single, universally accepted definition of terrorism,\" says the FBI. In many cases, their conversion to militant Islam is about seeking identity and purpose, or even a sense of adventure. Few of these men have a deep understanding of Salafism, the deeply conservative brand of Islam that's the philosophical underpinning of groups like al Qaeda, and jihad; their writings are often incoherent.  Frequently they see radical Islam as a form of redemption from past misdeeds</td>\n",
       "      <td>Like many \"lone wolf\" terrorists, Ottawa gunman was alienated drifter who converted to Islam.\\nConversion to militant Islam is often about seeking identity, purpose or adventure.\\nSome countries have tried \"de-radicalization\" programs to help prevent violence.\\nBut with resources stretched thin, the focus is often on increased law enforcement.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for summarization tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_rouge(predicted_txts, reference_txts, rouge_keys=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True):\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "    for ref_text, pred_txt in zip(reference_txts, predicted_txts):\n",
    "        scores = scorer.score(ref_text, pred_txt)\n",
    "        aggregator.add_scores(scores)\n",
    "\n",
    "    result = aggregator.aggregate()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here we create a summarization specific subclass of `HF_BaseModelCallback` in order to include custom, summarization specific, metrics, and also handle the pre-calculated loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_SummarizationModelCallback(HF_BaseModelCallback):  \n",
    "    def __init__(self, rouge_metrics=[\"rouge1\", \"rouge2\", \"rougeL\"], text_gen_kwargs={}, **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self=self, names='rouge_metrics, text_gen_kwargs, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in rouge_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the target's HF_TokenizerTransform (used for rouge metrics)\n",
    "        hf_textblock_tfm = self.learn.dls.before_batch[0]\n",
    "        self.hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "        self.tok_kwargs = hf_textblock_tfm.tok_kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "    def before_fit(self): self.setup()\n",
    "        \n",
    "        \n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self): self.hf_loss = None\n",
    "        \n",
    "    def after_pred(self): \n",
    "        # the \"labels\" key will only be included in the input dictionary *IF* we are training with target labels, \n",
    "        # in which case the first output of the model will be the loss\n",
    "        if ('labels' in self.xb[0]):\n",
    "            self.hf_loss, self.learn.pred = self.pred[0], self.pred[1]\n",
    "        else:\n",
    "            self.learn.pred = self.pred[0]\n",
    "            \n",
    "    def after_loss(self): \n",
    "        # if we already have the loss from the model, update the Learner's loss to be it\n",
    "        if (self.hf_loss is not None): self.learn.loss = self.hf_loss\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if (self.training or self.learn.y is None): return\n",
    "        \n",
    "        # grab predicted and reference ids for any metrics that need them\n",
    "        input_ids, attention_mask = self.xb[0]['input_ids'], self.xb[0]['attention_mask']\n",
    "        gen_ids = self.learn.model.hf_model.generate(input_ids=input_ids, \n",
    "                                                     attention_mask=attention_mask, \n",
    "                                                     use_cache=True,\n",
    "                                                     **self.text_gen_kwargs)\n",
    "        \n",
    "        self.generated_ids += gen_ids.tolist()\n",
    "        self.refernce_ids += self.yb[0].tolist()\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self): self.generated_ids, self.refernce_ids = [], []\n",
    "        \n",
    "    def after_validate(self):\n",
    "        # are there rouge metrics to calculate?\n",
    "        if (self.rouge_metrics is not None and len(self.rouge_metrics) > 0):\n",
    "            gen_texts = self.hf_tokenizer.batch_decode(self.generated_ids, \n",
    "                                                       skip_special_tokens=True, \n",
    "                                                       clean_up_tokenization_spaces=True)\n",
    "\n",
    "            ref_texts = self.hf_tokenizer.batch_decode(self.refernce_ids, \n",
    "                                                       skip_special_tokens=True, \n",
    "                                                       clean_up_tokenization_spaces=True)\n",
    "\n",
    "            rouge_results = calculate_rouge(gen_texts, ref_texts, rouge_keys=self.rouge_metrics)\n",
    "            \n",
    "            for rouge_key, scores in rouge_results.items(): \n",
    "                self.custom_metrics_dict[rouge_key] = scores.mid.fmeasure\n",
    "                \n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a custom param splitter to give us a bit more depth in applying discriminative learning rates for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarization_splitter(m, arch):\n",
    "    \"\"\"Custom param splitter for summarization models\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, 'hf_model')) else m\n",
    "    \n",
    "    if arch in ['bart', 'pegasus']:     \n",
    "        embeds = nn.Sequential(\n",
    "            model.model.shared, \n",
    "            model.model.encoder.embed_positions, \n",
    "            model.model.encoder.embed_tokens,\n",
    "            model.model.decoder.embed_positions, \n",
    "            model.model.decoder.embed_tokens\n",
    "        )\n",
    "        \n",
    "        groups = L(embeds, model.model.encoder, model.model.decoder)\n",
    "        return groups.map(params).filter(lambda el: len(el) > 0)\n",
    "    \n",
    "    if arch in['t5']:\n",
    "        embeds = nn.Sequential(\n",
    "            model.shared, \n",
    "            model.encoder.embed_tokens,\n",
    "            model.decoder.embed_tokens\n",
    "        )\n",
    "        \n",
    "        groups = L(embeds, model.encoder, model.decoder)\n",
    "        return groups.map(params).filter(lambda el: len(el) > 0)\n",
    "    \n",
    "    raise ValueError('Invalid architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"summarization_splitter\" class=\"doc_header\"><code>summarization_splitter</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>summarization_splitter</code>(**`m`**, **`arch`**)\n",
       "\n",
       "Custom param splitter for summarization models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(summarization_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we don't really need a loss function, we have to provide a custom loss class/function for fastai to function properly (e.g. one with a `decodes` and `activation` methods).  Why?  Because these methods will get called in methods like `show_results` to get the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_MaskedLMLoss():\n",
    "    def __call__(self, inp, targ, **kwargs): return\n",
    "    def decodes(self, x): return x.argmax(dim=-1)\n",
    "    def activation(self, x): return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': True,\n",
       " 'length_penalty': 2.0,\n",
       " 'max_length': 130,\n",
       " 'min_length': 30,\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'num_beams': 4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_kwargs = { **hf_config.task_specific_params['summarization'], **{'max_length': 130, 'min_length': 30} }\n",
    "text_gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "model_cb = HF_SummarizationModelCallback(text_gen_kwargs=text_gen_kwargs)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=ranger,\n",
    "                loss_func=HF_MaskedLMLoss(),\n",
    "                cbs=[model_cb],\n",
    "                splitter=partial(summarization_splitter, arch=hf_arch))#.to_fp16()\n",
    "\n",
    "learn.create_opt() \n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " tensor(4.0109, device='cuda:1', grad_fn=<NllLossBackward>),\n",
       " torch.Size([2, 73, 50264]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0], preds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([2, 256]), 2, torch.Size([2, 74]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.00020892962347716094, lr_steep=0.00015848931798245758)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.690670</td>\n",
       "      <td>1.899561</td>\n",
       "      <td>0.357975</td>\n",
       "      <td>0.144255</td>\n",
       "      <td>0.234915</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(1, lr_max=4e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to take advantage of huggingface's `PreTrainedModel.generate` model, which can be used to easily implement beam search, top-k/nucleous sampling, etc... so that we get more human sounding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = \"\"\"\n",
    "About 10 men armed with pistols and small machine guns raided a casino in Switzerland and made off \n",
    "into France with several hundred thousand Swiss francs in the early hours of Sunday morning, police said. \n",
    "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino \n",
    "Basel, Chief Inspector Peter Gill told CNN. One group tried to break into the casino's vault on the lower level \n",
    "but could not get in, but they did rob the cashier of the money that was not secured, he said. The second group \n",
    "of armed robbers entered the upper level where the roulette and blackjack tables are located and robbed the \n",
    "cashier there, he said. As the thieves were leaving the casino, a woman driving by and unaware of what was \n",
    "occurring unknowingly blocked the armed robbers' vehicles. A gunman pulled the woman from her vehicle, beat \n",
    "her, and took off for the French border. The other gunmen followed into France, which is only about 100 \n",
    "meters (yards) from the casino, Gill said. There were about 600 people in the casino at the time of the robbery. \n",
    "There were no serious injuries, although one guest on the Casino floor was kicked in the head by one of the \n",
    "robbers when he moved, the police officer said. Swiss authorities are working closely with French authorities, \n",
    "Gill said. The robbers spoke French and drove vehicles with French license plates. CNN's Andreena Narayan \n",
    "contributed to this report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 10About 10 men armed with pistols and machine machine guns raided a casino in Switzerland and made off\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(test_article)\n",
    "print(hf_tokenizer.decode(res[0][:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look much like a human-generated summary.  Let's use huggingface's `PreTrainedModel.generate` method to create something more human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Target ===\n",
      " NEW: Planes depart Australia to resume their search for airplane debris.\n",
      "NEW: Official: Passengers' relatives are moved to a different Kuala Lumpur hotel.\n",
      "Objects seen on satellite spark intensive search in southern Indian Ocean.\n",
      "U.S. officials: Files were deleted from flight simulator's hard drive after February 3.\n",
      "\n",
      "=== Prediction ===\n",
      " Malaysia Airlines Flight 370 has been missing for two weeks, with no sign of the Boeing 777.\n",
      "Two large objects detected by satellite Sunday floating on waters over 1,400 miles off Australia's west coast.\n",
      "Australian military planes resume their search for the objects amid skepticism they will turn up anything.\n",
      "Malaysia Airlines flight 370 was carrying 227 passengers and 12 crew members, bound for Beijing from Kuala Lumpur.\n",
      "Search has covered 2.97 million square miles, nearly the size of the continental United States.\n"
     ]
    }
   ],
   "source": [
    "b = dls.valid.one_batch()\n",
    "\n",
    "test_input_ids = b[0]['input_ids'][0].unsqueeze(0).to(learn.model.hf_model.device)\n",
    "test_trg_ids = b[1][0].unsqueeze(0).to(learn.model.hf_model.device)\n",
    "\n",
    "gen_text = learn.model.hf_model.generate(test_input_ids, num_beams=4, max_length=130, min_length=30)\n",
    "\n",
    "print('=== Target ===')\n",
    "print(f'{hf_tokenizer.decode(test_trg_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)}\\n')\n",
    "\n",
    "print('=== Prediction ===')\n",
    "print(hf_tokenizer.decode(gen_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add a `blurr_summarize` method to `Learner` that uses huggingface's `PreTrainedModel.generate` to create our predictions.  For the full list of arguments you can pass in see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate). You can also check out their [\"How To Generate\"](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb) notebook for more information about how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summarize(self:Learner, inp, **kwargs):\n",
    "    \"\"\"Uses the built-in `generate` method to generate the text \n",
    "    (see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)\n",
    "    for a list of arguments you can pass in)\n",
    "    \"\"\"\n",
    "    # grab the text generation kwargs\n",
    "    text_gen_kwargs = self.cbs.filter(lambda el: isinstance(el, HF_SummarizationModelCallback) )[0].text_gen_kwargs\n",
    "    text_gen_kwargs = { **text_gen_kwargs, **kwargs}\n",
    "    \n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.before_batch[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_textblock_tfm.tok_kwargs\n",
    "\n",
    "    if (isinstance(inp, str)):\n",
    "        input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors='pt', **tok_kwargs)\n",
    "    else:\n",
    "        input_ids = inp\n",
    "        \n",
    "    input_ids = input_ids.to(self.model.hf_model.device)\n",
    "    \n",
    "    gen_texts = self.model.hf_model.generate(input_ids, **text_gen_kwargs)\n",
    "    outputs = [ hf_tokenizer.decode(txt, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n",
    "               for txt in gen_texts ]\n",
    "    \n",
    "    if hf_textblock_tfm.hf_arch == 'pegasus':\n",
    "        outputs = [o.replace('<n>', ' ') for o in outputs]\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_summarize\" class=\"doc_header\"><code>Learner.blurr_summarize</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_summarize</code>(**`inp`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Uses the built-in `generate` method to generate the text \n",
       "(see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)\n",
       "for a list of arguments you can pass in)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction 1 ===\n",
      " About 10 men armed with pistols and small machine guns raided a casino in Switzerland and made off with several hundred thousand Swiss francs .\n",
      "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\n",
      "A woman driving by unknowingly blocked the armed robbers' vehicles and was beaten to death .\n",
      "There were no serious injuries, although one guest on the Casino floor was kicked in the head by one of the robbers .\n",
      "\n",
      "=== Prediction 2 ===\n",
      " About 10 men armed with pistols and small machine guns raided a casino in Switzerland and made off with several hundred thousand Swiss francs .\n",
      "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\n",
      "A woman driving by unknowingly blocked the armed robbers' vehicles and was beaten to death .\n",
      "There were no serious injuries, although one guest on the Casino floor was kicked in the head by one of the robbers when he moved .\n",
      "\n",
      "=== Prediction 3 ===\n",
      " About 10 men armed with pistols and small machine guns raided a casino in Switzerland and made off with several hundred thousand Swiss francs .\n",
      "The men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\n",
      "A woman driving by unknowingly blocked the armed robbers' vehicles and was beaten to death .\n",
      "There were no serious injuries, although one guest was kicked in the head by one of the robbers when he moved .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = learn.blurr_summarize(test_article, num_return_sequences=3)\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much nicer!!! Now, we can update our @typedispatched `show_results` to use this new method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_SummarizationInput, y, samples, outs, learner, ctxs=None, max_n=6, **kwargs):  \n",
    "    hf_tokenizer = learner.dls.before_batch[0].hf_tokenizer\n",
    "    \n",
    "    gen_text_txts = learner.blurr_summarize(x)\n",
    "    res = L([ \n",
    "        (hf_tokenizer.decode(s[0], skip_special_tokens=True), \n",
    "         hf_tokenizer.decode(s[1], skip_special_tokens=True), \n",
    "         gen_txt) for s, gen_txt in zip(samples, gen_text_txts) ])          \n",
    "    \n",
    "    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN) -- Two weeks. Two gut-wrenching, frustrating, mysterious weeks. That's how long it's been since 227 passengers and 12 crew members boarded Malaysia Airlines Flight 370, destined for Beijing. A routine trip, it seemed, to catch up relatives in time for the weekend, start on a work assignment or just get away. Where they got to, still unknown. An exhaustive search -- covering a mind-boggling 2.97 million square miles, which is nearly the size of the continental United States -- has yielded some clues, but no proof of where the Boeing 777 is or definitively what happened to it. The latest, most notable lead revolved around two large objects detected by satellite Sunday floating on waters over 1,400 miles off of Australia's west coast. The first of several Australian military planes, as well as two long-range commercial jets, resumed their search Saturday morning to find any trace of the objects, amid some skepticism that they or ships in the area ever will and, if they do, that whatever they find will be related to the missing aircraft. Australian Prime Minister Tony Abbott on Friday defended the decision to announce the find, saying Australia owes it to families of those missing \"to give them information as soon as it's</td>\n",
       "      <td>NEW: Planes depart Australia to resume their search for airplane debris.\\nNEW: Official: Passengers' relatives are moved to a different Kuala Lumpur hotel.\\nObjects seen on satellite spark intensive search in southern Indian Ocean.\\nU.S. officials: Files were deleted from flight simulator's hard drive after February 3.</td>\n",
       "      <td>Malaysia Airlines Flight 370 has been missing for two weeks, with no sign of the Boeing 777 .\\nTwo large objects detected by satellite Sunday floating on waters over 1,400 miles off Australia's west coast .\\nAustralian military planes resume their search for the objects amid skepticism they will turn up anything .\\nMalaysia Airlines flight 370 was carrying 227 passengers and 12 crew members, bound for Beijing from Kuala Lumpur .\\nSearch has covered 2.97 million square miles, nearly the size of the continental United States .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As a growing number of airplanes scoured the southern Indian Ocean in the search for Malaysia Airlines Flight 370, authorities released new details that paint a different picture of what may have happened in the plane's cockpit. Military radar tracking shows that the aircraft changed altitude after making a sharp turn over the South China Sea as it headed toward the Strait of Malacca, a source close to the investigation into the missing flight told CNN. The plane flew as low as 12,000 feet at some point before it disappeared from radar, according to the source. The sharp turn seemed to be intentional, the source said, because executing it would have taken the Boeing 777 two minutes -- a time period during which the pilot or co-pilot could have sent an emergency signal if there had been a fire or other emergency onboard. Authorities say the plane didn't send any emergency signals, though some analysts say it's still unclear whether the pilots tried but weren't able to communicate because of a catastrophic failure. The official, who is not authorized to speak to the media, told CNN that the area the plane flew in after the turn is a heavily trafficked air corridor and that flying at 12,000 feet would have kept the jet well out of the way of that traffic. Earlier Sunday, Malaysian authorities</td>\n",
       "      <td>U.S. Navy sending listening device to help find voice and data recorders if wreckage is found.\\nSource: Plane changed altitude, flying as low as 12,000 feet after making short turn.\\nSchiavo: Altitude information \"explains so many pieces that didn't fit together\"\\n10 aircraft set to comb southern region for missing plane as search resumes Monday.</td>\n",
       "      <td>Malaysia Airlines Flight 370 changed altitude after making a sharp turn over the South China Sea, a source says .\\nThe plane flew as low as 12,000 feet at some point before it disappeared from radar, the source tells CNN .\\nAuthorities say the plane didn't send any emergency signals, though some analysts say it's still unclear .\\nIt's unclear whether the pilots tried to communicate because of a catastrophic failure, analysts say .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname='summarize_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" About 10 men armed with pistols and small machine guns raided a casino in Switzerland and made off with several hundred thousand Swiss francs .\\nThe men, dressed in black clothes and black ski masks, split into two groups during the raid on the Grand Casino Basel .\\nA woman driving by unknowingly blocked the armed robbers' vehicles and was beaten to death .\\nThere were no serious injuries, although one guest on the Casino floor was kicked in the head by one of the robbers .\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn = load_learner(fname='summarize_export.pkl')\n",
    "inf_learn.blurr_summarize(test_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core training code above works for **all** pretrained summarization models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained summarization models you are working with ... and if any of your pretrained summarization models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_bart.BartForConditionalGeneration,\n",
       " transformers.modeling_mbart.MBartForConditionalGeneration,\n",
       " transformers.modeling_pegasus.PegasusForConditionalGeneration,\n",
       " transformers.modeling_t5.T5ForConditionalGeneration]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='ConditionalGeneration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    ('facebook/bart-large-cnn',BartForConditionalGeneration),\n",
    "    ('t5-small', T5ForConditionalGeneration),\n",
    "    #('google/pegasus-cnn_dailymail', PegasusForConditionalGeneration), ... don't fit on my 1080TI :(\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./')\n",
    "cnndm_df = pd.read_csv(path/'cnndm_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== facebook/bart-large-cnn ===\n",
      "\n",
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizer\n",
      "model:\t\tBartForConditionalGeneration\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.322155</td>\n",
       "      <td>2.899196</td>\n",
       "      <td>0.281546</td>\n",
       "      <td>0.106553</td>\n",
       "      <td>0.206675</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN) -- Home to up to 10 percent of all known species, Mexico is recognized as one of the most biodiverse regions on the planet. The twin threats of climate change and human encroachment on natural environments are, however, threatening the existence of the country's rich wildlife. And there is a great deal to lose. In the United Nations Environment Program (UNEP) World Conservation Monitoring Centre's list of megadiverse countries Mexico ranks 11th. The list represents a group of 17 countries that harbor the majority of the Earth's species and are therefore considered extremely biodiverse. From its coral reefs in the Caribbean Sea to</td>\n",
       "      <td>Mexico hosts to up to 10 percent of all known species on Earth.\\nIt is home to 502 types of mammals, 290 bird species and 26,000 types of plants.\\nHuman development and climate change is placing a big strain on its biodiversity.\\nThe Golden Eagle is under threat in spite of being the country's national symbol.</td>\n",
       "      <td>Climate change and human encroachment on natural environments threaten Mexico's wildlife .\\nThe country is home to up to 10 percent of all known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Two weeks. Two gut-wrenching, frustrating, mysterious weeks. That's how long it's been since 227 passengers and 12 crew members boarded Malaysia Airlines Flight 370, destined for Beijing. A routine trip, it seemed, to catch up relatives in time for the weekend, start on a work assignment or just get away. Where they got to, still unknown. An exhaustive search -- covering a mind-boggling 2.97 million square miles, which is nearly the size of the continental United States -- has yielded some clues, but no proof of where the Boeing 777 is or definitively what happened to</td>\n",
       "      <td>NEW: Planes depart Australia to resume their search for airplane debris.\\nNEW: Official: Passengers' relatives are moved to a different Kuala Lumpur hotel.\\nObjects seen on satellite spark intensive search in southern Indian Ocean.\\nU.S. officials: Files were deleted from flight simulator's hard drive after February 3.</td>\n",
       "      <td>It's been two weeks since 227 passengers and 12 crew members boarded Malaysia Airlines Flight 370, destined for Beijing .\\nAn exhaustive search has</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== t5-small ===\n",
      "\n",
      "architecture:\tt5\n",
      "tokenizer:\tT5Tokenizer\n",
      "model:\t\tT5ForConditionalGeneration\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.000176</td>\n",
       "      <td>2.746533</td>\n",
       "      <td>0.277890</td>\n",
       "      <td>0.112718</td>\n",
       "      <td>0.209830</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: (CNN) -- Wondering where to go for your next holiday? Experts explain which destinations we should be checking out in 2014. Brazil: The World Cup. The modern game of football, or soccer, may have been born in England's public schools, but many will claim its soul has settled in Brazil. It has the world's most successful international team, winning the World Cup five times. It calls what many claim to be the world's greatest player, Pele, one of its own. And company managers and bosses are known to demand their employees skip work to watch the big games</td>\n",
       "      <td>New Zealand government threw $50 million into the construction of the Nga Haerenga cycle trails. Nosara in Costa Rica recently awarded a Blue Flag -- a certification awarded to world's best beaches. First few months of 2014 best period for Northern Lights for years, as NASA scientists predict a peak in solar activity.</td>\n",
       "      <td>experts explain which destinations we should be checking out in 2014 . Brazil has the world's most successful international team, winning the World Cup five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: I have an uncle who has always been a robust and healthy guy. He drank a glass of skim milk every day, bragged about how many pull-ups he was doing and fit into pants he was wearing 20 years before. He didn't take a single medication and retired early. Given that he had no medical problems and ran his own business, he opted to go several years without health insurance. Eventually, when he turned 65, he picked up Medicare. What happened next was a little strange. He fell off the wagon. He</td>\n",
       "      <td>Sanjay Gupta: Moral hazard causes some to neglect health when they get health insurance. He says Obamacare alone won't guarantee good health; personal habits must do that. He says research shows 30 minutes of daily exercise cuts heart attack, stroke risk by a third. Gupta: It's time to stop playing defense on your health; instead, start optimizing it yourself.</td>\n",
       "      <td>uncle has always been a robust and healthy guy . he drank skim milk every day, bragged about how many</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "bsz = 2\n",
    "inp_seq_sz = 128; trg_seq_sz = 130\n",
    "\n",
    "test_results = []\n",
    "for model_name, model_cls in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   model_cls=model_cls)\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\nmodel:\\t\\t{type(hf_model).__name__}\\n')\n",
    "    \n",
    "    # 1. build your DataBlock\n",
    "    def add_t5_prefix(inp): return f'summarize: {inp}' if (hf_arch == 't5') else inp\n",
    "    \n",
    "    hf_batch_tfm = HF_SummarizationBatchTransform(hf_arch, hf_tokenizer, max_length=[inp_seq_sz, trg_seq_sz])\n",
    "    blocks = (HF_TextBlock(hf_batch_tfm=hf_batch_tfm), noop)\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=Pipeline([ColReader('article'), add_t5_prefix]), \n",
    "                       get_y=ColReader('highlights'), \n",
    "                       splitter=RandomSplitter())\n",
    "\n",
    "    dls = dblock.dataloaders(cnndm_df, bs=bsz)\n",
    "\n",
    "    # 2. build your Learner\n",
    "    text_gen_kwargs = {}\n",
    "    if (hf_arch in ['bart', 't5']):\n",
    "        text_gen_kwargs = { \n",
    "            **hf_config.task_specific_params['summarization'], \n",
    "            **{'max_length': 30, 'min_length': 10} \n",
    "        }\n",
    "    \n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    model_cb = HF_SummarizationModelCallback(text_gen_kwargs=text_gen_kwargs)\n",
    "\n",
    "    learn = Learner(dls, \n",
    "                    model,\n",
    "                    opt_func=ranger,\n",
    "                    loss_func=HF_MaskedLMLoss(),\n",
    "                    cbs=[model_cb],\n",
    "                    splitter=partial(summarization_splitter, arch=hf_arch))#.to_fp16()\n",
    "\n",
    "    learn.create_opt() \n",
    "    learn.freeze()\n",
    "    \n",
    "    # 3. Run your tests\n",
    "    b = dls.one_batch()\n",
    "\n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***\\n')\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, inp_seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(preds[1].shape[0], bsz)\n",
    "        test_eq(preds[1].shape[2], hf_config.vocab_size)\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max=1e-3)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(learner=learn, max_n=2)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizer</td>\n",
       "      <td>BartForConditionalGeneration</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5</td>\n",
       "      <td>T5Tokenizer</td>\n",
       "      <td>T5ForConditionalGeneration</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model_name', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
