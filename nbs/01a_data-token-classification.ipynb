{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.token_classification\n",
    "\n",
    "> This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for token classification tasks (e.g., NER or named entity recognition, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Using GPU #1: GeForce GTX 1080 Ti\n"
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification tokenization, batch transform, and DataBlock methods\n",
    "\n",
    "Token classification tasks attempt to predict a class for each token.  The idea is similar to that in image segmentation models where the objective is to predict a class for each pixel.  Such models are common in building named entity recognition (NER) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "path = Path('./')\n",
    "germ_eval_df = pd.read_csv(path/'germeval2014_sample.csv', converters=df_converters); len(germ_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# for idx, el in germ_eval_df.iterrows():\n",
    "#     print (el['tokens'])\n",
    "#     print (el['labels'])\n",
    "#     print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    },
    {
     "data": {
      "text/plain": "('bert',\n transformers.configuration_bert.BertConfig,\n transformers.tokenization_bert.BertTokenizer,\n transformers.modeling_bert.BertForTokenClassification)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task,\n",
    "                                                                               config_kwargs={'num_labels': n_labels})\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a new class and transform for token classification targets/predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenTensorCategory(TensorBase): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenCategorize(Transform):\n",
    "    \"Reversible transform of a list of category string to `vocab` id\"\n",
    "    \n",
    "    def __init__(self, vocab=None, ignore_token=None, ignore_token_id=None):  \n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab)\n",
    "        self.ignore_token = '[xIGNx]' if ignore_token is None else ignore_token\n",
    "        self.ignore_token_id = CrossEntropyLossFlat().ignore_index if ignore_token_id is None else ignore_token_id\n",
    "        \n",
    "        self.loss_func, self.order = CrossEntropyLossFlat(ignore_index=self.ignore_token_id), 1\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, labels):\n",
    "        ids = [[self.vocab.o2i[lbl]] + [self.ignore_token_id]*(n_subtoks-1) for lbl, n_subtoks in labels] \n",
    "        return HF_TokenTensorCategory(reduce(operator.concat, ids))\n",
    "    \n",
    "    def decodes(self, encoded_labels): \n",
    "        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_TokenCategorize` modifies the fastai `Categorize` transform in a couple of ways.  First, it allows your targets to consist of a `Category` ***per*** token, and second, it uses the idea of an `ignore_token` to mask subtokens that don't need a prediction.  For example, the target of special tokens (e.g., pad, cls, sep) are set to `ignore_token` as are subsequent sub-tokens of a given token should more than 1 sub-token make it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def HF_TokenCategoryBlock(vocab=None, ignore_token=None, ignore_token_id=None):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    return TransformBlock(type_tfms=HF_TokenCategorize(vocab=vocab, \n",
    "                                                       ignore_token=ignore_token,\n",
    "                                                       ignore_token_id=ignore_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"HF_TokenCategoryBlock\" class=\"doc_header\"><code>HF_TokenCategoryBlock</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>HF_TokenCategoryBlock</code>(**`vocab`**=*`None`*, **`ignore_token`**=*`None`*, **`ignore_token_id`**=*`None`*)\n\n`TransformBlock` for single-label categorical targets",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HF_TokenCategoryBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a custom class, `HF_TokenClassInput`, for the @typedispatched methods to use so that we can override how token classification inputs/targets are assembled, as well as, how the data is shown via methods like `show_batch` and `show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenClassInput(HF_BaseInput): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenClassBatchTransform(HF_BatchTransform):\n",
    "    def __init__(self, hf_arch, hf_tokenizer, \n",
    "                 ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "                 max_length=None, padding=True, truncation=True, is_pretokenized=True,\n",
    "                 n_tok_inps=1, hf_input_return_type=HF_TokenClassInput, tok_kwargs={}, **kwargs):\n",
    "                 \n",
    "        super().__init__(hf_arch, hf_tokenizer,\n",
    "                         max_length=max_length, padding=padding, truncation=truncation, is_pretokenized=is_pretokenized,\n",
    "                         n_tok_inps=n_tok_inps, hf_input_return_type=hf_input_return_type, tok_kwargs=tok_kwargs, **kwargs)\n",
    "\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        \n",
    "    def encodes(self, samples):  \n",
    "        samples = super().encodes(samples)\n",
    "        if (len(samples[0]) == 1): return samples\n",
    "        \n",
    "        target_cls = type(samples[0][1])\n",
    "        updated_samples = []\n",
    "        \n",
    "        # we assume that first target = the categories we want to predict for each token\n",
    "        for s in samples:\n",
    "            targ_len = len(s[1])\n",
    "            idx_first_input_id = s[0]['special_tokens_mask'].tolist().index(0)\n",
    "            targ_ids = target_cls([ self.ignore_token_id if (el == 1 or idx > targ_len) \n",
    "                                   else s[1][idx-idx_first_input_id].item() \n",
    "                                   for idx, el in enumerate(s[0]['special_tokens_mask']) ])\n",
    "\n",
    "            updated_samples.append((s[0], targ_ids))\n",
    "        \n",
    "        return updated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_TokenClassBatchTransform` is used to turn any targets we don't want to include in the loss calcuation (e.g. padding, cls, sep, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_batch_tfm = HF_TokenClassBatchTransform(hf_arch, hf_tokenizer, is_pretokenized=True, tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_batch_tfm=hf_batch_tfm), \n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the example above we had to define a `get_y` in order to return both the entity we want to predict a category for, as well as, how many subtokens are used by the `hf_tokenizer` to represent it.  This is necessary for the input/target alignment discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2, torch.Size([4, 76]), torch.Size([4, 76]))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:HF_TokenClassInput, y, samples, dataloaders, ctxs=None, max_n=6, **kwargs):  \n",
    "    hf_tokenizer = dataloaders.before_batch[0].hf_tokenizer\n",
    "    \n",
    "    res = L()\n",
    "    for inp, trg, sample in zip(x, y, samples):\n",
    "        # recontstruct the string and split on space to get back your pre-tokenized list of tokens\n",
    "        toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=True)\n",
    "        pretokenized_toks =  hf_tokenizer.convert_tokens_to_string(toks).split()\n",
    "\n",
    "        res.append([f'{[ (tok, lbl) for tok, lbl in zip(pretokenized_toks, ast.literal_eval(sample[1])) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S', 'O'), ('.', 'O'), ('593', 'B-OTH'), ('.', 'I-OTH'), ('Wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'O'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S', 'O'), ('.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'B-LOCderiv'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'O'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzählen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('Scenes', 'B-OTH'), ('of', 'I-OTH'), ('a', 'I-OTH'), ('Sexual', 'I-OTH'), ('Nature', 'I-OTH'), ('(', 'O'), ('GB', 'O'), ('2006', 'O'), (')', 'O'), ('-', 'O'), ('Regie', 'O'), (':', 'O'), ('Ed', 'B-PER'), ('Blum', 'I-PER'), ('Shortbus', 'B-OTH'), ('(', 'O'), ('USA', 'B-LOC'), ('2006', 'O'), (')', 'O'), ('-', 'O'), ('Regie', 'O'), (':', 'O'), ('John', 'B-PER'), ('Cameron', 'I-PER'), ('Mitchell', 'I-PER'), (':', 'O'), ('Film', 'O'), ('über', 'O'), ('den', 'O'), ('gleichnamigen', 'B-LOCderiv'), ('New', 'I-LOCderiv'), ('Yorker', 'O'), ('Club', 'O'), (',', 'O'), ('der', 'O'), ('verschiedensten', 'O'), ('Paaren', 'O'), ('eine', 'O'), ('Plattform', 'O'), ('zur', 'O'), ('Aufarbeitung', 'O'), ('ihrer', 'O'), ('Probleme', 'O'), ('bietet', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained token classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[transformers.modeling_albert.AlbertForTokenClassification,\n transformers.modeling_auto.AutoModelForTokenClassification,\n transformers.modeling_bert.BertForTokenClassification,\n transformers.modeling_camembert.CamembertForTokenClassification,\n transformers.modeling_distilbert.DistilBertForTokenClassification,\n transformers.modeling_electra.ElectraForTokenClassification,\n transformers.modeling_flaubert.FlaubertForTokenClassification,\n transformers.modeling_longformer.LongformerForTokenClassification,\n transformers.modeling_mobilebert.MobileBertForTokenClassification,\n transformers.modeling_roberta.RobertaForTokenClassification,\n transformers.modeling_xlm.XLMForTokenClassification,\n transformers.modeling_xlm_roberta.XLMRobertaForTokenClassification,\n transformers.modeling_xlnet.XLNetForTokenClassification]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='TokenClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'monologg/electra-small-finetuned-imdb',\n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== albert-base-v1 ===\n\nSome weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForTokenClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\talbert\ntokenizer:\tAlbertTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('s.', 'O'), ('593.', 'O'), ('wink', 'B-OTH'), ('&amp;', 'I-OTH'), ('seibold', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('s.', 'O'), ('32', 'O'), ('inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('falken', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('afrikanische', 'B-LOCderiv'), ('baumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('malaienbaumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('gruppe', 'O'), ('zuzuzahlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('gegenstand', 'O'), ('der', 'O'), ('forschung', 'O'), ('.', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('auerdem', 'O'), ('befindet', 'O'), ('sich', 'O'), ('im', 'O'), ('nordwesten', 'O'), ('der', 'O'), ('stadt', 'O'), ('(', 'O'), ('auf', 'O'), ('dem', 'O'), ('gelande', 'O'), ('des', 'O'), ('ehemaligen', 'O'), ('militarflughafens', 'O'), ('butzweilerhof', 'B-LOC'), (')', 'O'), ('das', 'O'), ('coloneum', 'B-LOC'), (',', 'O'), ('europas', 'B-LOC'), ('groter', 'O'), ('studiokomplex', 'O'), ('mit', 'O'), ('einer', 'O'), ('flache', 'O'), ('von', 'O'), ('35', 'O'), ('ha', 'O'), ('und', 'O'), ('20', 'O'), ('studios', 'O'), ('(', 'O'), ('25.000', 'O'), ('m2', 'O'), (')', 'O'), ('mit', 'O'), ('bis', 'O'), ('zu', 'O'), ('30', 'O'), ('meter', 'O'), ('deckenhohe', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== bert-base-multilingual-cased ===\n\nSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\tbert\ntokenizer:\tBertTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S', 'O'), ('.', 'O'), ('593', 'B-OTH'), ('.', 'I-OTH'), ('Wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'O'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S', 'O'), ('.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'B-LOCderiv'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'O'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzählen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('Mit', 'O'), ('der', 'O'), ('Servicefrau', 'O'), ('verband', 'O'), ('Bianca', 'B-PER'), ('offenbar', 'O'), ('eine', 'O'), ('Art', 'O'), ('Freundschaft', 'O'), ('oder', 'O'), ('wenigstens', 'O'), ('wünschte', 'O'), ('sich', 'O'), ('das', 'O'), ('Bianca', 'O'), ('B', 'B-PER'), ('.', 'I-PER'), ('«', 'O'), ('Sie', 'O'), ('gab', 'O'), ('mir', 'O'), ('immer', 'O'), ('wieder', 'O'), ('Geld', 'O'), ('oder', 'O'), ('kleine', 'O'), ('Geschenke', 'O'), ('»', 'O'), (',', 'O'), ('so', 'O'), ('L', 'B-PER'), ('.', 'O'), ('Doch', 'O'), ('gleichzeitig', 'O'), ('sei', 'B-PER'), ('Bianca', 'I-PER'), ('B', 'O'), ('.', 'O'), ('ihr', 'O'), ('gegenüber', 'O'), ('nicht', 'O'), ('ehrlich', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== camembert-base ===\n\nSome weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\tcamembert\ntokenizer:\tCamembertTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S.', 'O'), ('593.', 'O'), ('Wink', 'B-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'B-LOCderiv'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzhlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O'), ('der', 'O'), ('Forschung', 'O'), ('.', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('(', 'O'), ('Standard', 'B-ORG'), ('Oil', 'I-ORG'), ('of', 'I-ORG'), ('New', 'I-ORG'), ('Jersey', 'I-ORG'), (')', 'O'), (',', 'O'), ('die', 'O'), ('ausgesprochen', 'O'), ('Esso', 'O'), ('“', 'B-ORG'), ('ergeben', 'O'), ('(', 'O'), ('heute', 'O'), ('ExxonMobil', 'O'), (')', 'B-ORG'), ('.', 'O'), (';', 'O'), ('Exxon', 'O'), (':', 'B-ORG'), ('Ein', 'O'), ('Name', 'O'), (',', 'O'), ('der', 'O'), ('in', 'O'), ('den', 'O'), ('frhen', 'O'), ('1970ern', 'O'), ('von', 'O'), ('Esso', 'O'), ('erfunden', 'B-ORG'), ('wurde', 'O'), (',', 'O'), ('um', 'O'), ('ein', 'O'), ('neutrales', 'O'), ('aber', 'O'), ('eindeutiges', 'O'), ('Markenzeichen', 'O'), ('fr', 'O'), ('das', 'O'), ('Unternehmen', 'O'), ('zu', 'O'), ('haben', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== distilbert-base-uncased ===\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\tdistilbert\ntokenizer:\tDistilBertTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('s', 'O'), ('.', 'O'), ('593', 'B-OTH'), ('.', 'I-OTH'), ('wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('seibold', 'I-OTH'), ('et', 'O'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('s', 'O'), ('.', 'O'), ('32', 'O'), ('inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('falken', 'O'), (',', 'B-LOCderiv'), ('wie', 'O'), ('der', 'O'), ('afrikanische', 'O'), ('baumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('malaienbaumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('gruppe', 'O'), ('zuzuzahlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('gegenstand', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('(', 'O'), ('standard', 'B-ORG'), ('oil', 'I-ORG'), ('of', 'I-ORG'), ('new', 'I-ORG'), ('jersey', 'I-ORG'), (')', 'O'), (',', 'O'), ('die', 'O'), ('ausgesprochen', 'O'), ('„', 'O'), ('esso', 'B-ORG'), ('“', 'O'), ('ergeben', 'O'), ('(', 'O'), ('heute', 'O'), ('exxonmobil', 'B-ORG'), (')', 'O'), ('.', 'O'), (';', 'O'), ('exxon', 'B-ORG'), (':', 'O'), ('ein', 'O'), ('name', 'O'), (',', 'O'), ('der', 'O'), ('in', 'O'), ('den', 'O'), ('fruhen', 'O'), ('1970ern', 'O'), ('von', 'O'), ('esso', 'B-ORG'), ('erfunden', 'O'), ('wurde', 'O'), (',', 'O'), ('um', 'O'), ('ein', 'O'), ('neutrales', 'O'), ('aber', 'O'), ('eindeutiges', 'O'), ('markenzeichen', 'O'), ('fur', 'O'), ('das', 'O'), ('unternehmen', 'O'), ('zu', 'O'), ('haben', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== monologg/electra-small-finetuned-imdb ===\n\narchitecture:\telectra\ntokenizer:\tElectraTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('s', 'O'), ('.', 'O'), ('593', 'B-OTH'), ('.', 'I-OTH'), ('wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('seibold', 'I-OTH'), ('et', 'O'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('s', 'O'), ('.', 'O'), ('32', 'O'), ('inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('falken', 'O'), (',', 'B-LOCderiv'), ('wie', 'O'), ('der', 'O'), ('afrikanische', 'O'), ('baumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('malaienbaumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('gruppe', 'O'), ('zuzuzahlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('gegenstand', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('nach', 'O'), ('seiner', 'O'), ('ruckkehr', 'O'), ('hielt', 'O'), ('strummer', 'B-PER'), ('ein', 'O'), ('bandmeeting', 'O'), ('ab', 'O'), (',', 'O'), ('in', 'O'), ('dem', 'O'), ('er', 'O'), ('sheppard', 'B-PER'), (',', 'O'), ('white', 'B-PER'), ('und', 'O'), ('howard', 'B-PER'), ('mitteilte', 'O'), (',', 'O'), ('dass', 'O'), ('er', 'O'), ('nicht', 'O'), ('mehr', 'O'), ('mit', 'O'), ('ihnen', 'O'), ('arbeiten', 'O'), ('werde', 'O'), (',', 'O'), ('da', 'O'), ('er', 'O'), ('weiter', 'O'), ('versuchen', 'O'), ('wolle', 'O'), (',', 'O'), ('mick', 'B-PER'), ('jones', 'I-PER'), ('wieder', 'O'), ('zuruck', 'O'), ('in', 'O'), ('die', 'O'), ('band', 'O'), ('zu', 'O'), ('holen', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== allenai/longformer-base-4096 ===\n\nSome weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\tlongformer\ntokenizer:\tLongformerTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S.', 'O'), ('593.', 'O'), ('Wink', 'B-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'B-LOCderiv'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzählen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O'), ('der', 'O'), ('Forschung', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('Da', 'O'), ('ich', 'O'), ('mir', 'O'), ('als', 'O'), ('kleine', 'O'), ('Rentnerin', 'O'), ('nicht', 'O'), ('sehr', 'O'), ('viel', 'O'), ('leisten', 'O'), ('kann', 'O'), (',', 'O'), ('muss', 'O'), ('ich', 'O'), ('daher', 'O'), ('auch', 'O'), ('vorzeitig', 'O'), ('anfragen', 'O'), (',', 'O'), ('zu', 'O'), ('welchen', 'O'), ('Zeiten', 'O'), ('Sie', 'O'), ('für', 'O'), ('den', 'O'), ('Verkauf', 'O'), ('von', 'O'), ('Särgen', 'O'), ('Rabattaktionen', 'O'), ('haben', 'O'), ('werden', 'O'), (',', 'O'), ('so', 'O'), ('dass', 'O'), ('ich', 'O'), ('dann', 'O'), ('richtig', 'O'), ('zuschlagen', 'O'), ('kann', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== google/mobilebert-uncased ===\n\nSome weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing MobileBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing MobileBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MobileBertForTokenClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['mobilebert.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\tmobilebert\ntokenizer:\tMobileBertTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('s', 'O'), ('.', 'O'), ('593', 'B-OTH'), ('.', 'I-OTH'), ('wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('seibold', 'I-OTH'), ('et', 'O'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('s', 'O'), ('.', 'O'), ('32', 'O'), ('inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('falken', 'O'), (',', 'B-LOCderiv'), ('wie', 'O'), ('der', 'O'), ('afrikanische', 'O'), ('baumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('malaienbaumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('gruppe', 'O'), ('zuzuzahlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('gegenstand', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('newsru', 'B-OTH'), ('.', 'O'), ('ua', 'O'), ('/', 'B-OTH'), (':', 'I-OTH'), ('политисполком', 'I-OTH'), ('спу', 'I-OTH'), ('отказал', 'I-OTH'), ('морозу', 'I-OTH'), ('в', 'O'), ('отставке', 'B-ORG'), ('die', 'O'), ('spu', 'O'), ('legte', 'O'), (',', 'O'), ('wie', 'O'), ('auch', 'O'), ('vier', 'O'), ('weitere', 'O'), ('parteien', 'O'), (',', 'O'), ('beim', 'O'), ('obersten', 'O'), ('gericht', 'B-LOC'), ('der', 'O'), ('ukraine', 'O'), ('beschwerde', 'O'), ('gegen', 'O'), ('den', 'O'), ('ablauf', 'O'), ('der', 'O'), ('wahl', 'O'), ('ein', 'O'), ('und', 'O'), ('behauptet', 'O'), (',', 'O'), ('es', 'O'), ('sei', 'O'), ('bei', 'O'), ('der', 'O'), ('auszahlung', 'O'), ('zu', 'O'), ('unregelmaßigkeiten', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== roberta-base ===\n\nSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\troberta\ntokenizer:\tRobertaTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S.', 'O'), ('593.', 'O'), ('Wink', 'B-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'B-LOCderiv'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzählen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O'), ('der', 'O'), ('Forschung', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('Der', 'O'), ('28-Jährige', 'O'), ('und', 'O'), ('sein', 'O'), ('Team', 'O'), (',', 'O'), ('zu', 'O'), ('dem', 'O'), ('auch', 'O'), ('Karla', 'B-PER'), ('Barquero', 'I-PER'), ('gehört', 'O'), (',', 'O'), ('demonstrieren', 'O'), ('im', 'O'), ('gemeinnützigen', 'O'), ('\"', 'O'), ('La', 'B-ORG'), ('Tirimbina', 'I-ORG'), ('Regenwald', 'I-ORG'), ('Zentrum', 'I-ORG'), ('\"', 'O'), ('westlich', 'O'), ('von', 'O'), ('Puerto', 'B-LOC'), ('Viejo', 'I-LOC'), ('de', 'I-LOC'), ('Sarapiquí', 'I-LOC'), (',', 'O'), ('wie', 'O'), ('die', 'O'), ('Ureinwohner', 'O'), ('vor', 'O'), ('500', 'O'), ('Jahren', 'O'), ('das', 'O'), ('Getränk', 'O'), ('und', 'O'), ('Rohschokolade', 'O'), ('produzierten', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== xlm-mlm-en-2048 ===\n\nSome weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForTokenClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n- This IS expected if you are initializing XLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing XLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMForTokenClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['transformer.position_ids', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\txlm\ntokenizer:\tXLMTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('s.', 'O'), ('593', 'O'), ('.', 'B-OTH'), ('wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('seibold', 'I-OTH'), ('et', 'I-OTH'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('s.', 'O'), ('32', 'O'), ('inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('falken', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'B-LOCderiv'), ('afrikanische', 'O'), ('baumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('malaienbaumfalke', 'O'), ('(', 'O'), ('falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('gruppe', 'O'), ('zuzuzahlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('gegenstand', 'O'), ('der', 'O'), ('forschung', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('(', 'O'), ('standard', 'B-ORG'), ('oil', 'I-ORG'), ('of', 'I-ORG'), ('new', 'I-ORG'), ('jersey', 'I-ORG'), (')', 'O'), (',', 'O'), ('die', 'O'), ('ausgesprochen', 'O'), ('\"', 'O'), ('esso', 'B-ORG'), ('\"', 'O'), ('ergeben', 'O'), ('(', 'O'), ('heute', 'O'), ('exxonmobil', 'B-ORG'), (')', 'O'), ('.', 'O'), (';', 'O'), ('exxon', 'B-ORG'), (':', 'O'), ('ein', 'O'), ('name', 'O'), (',', 'O'), ('der', 'O'), ('in', 'O'), ('den', 'O'), ('fruhen', 'O'), ('1970ern', 'O'), ('von', 'O'), ('esso', 'B-ORG'), ('erfunden', 'O'), ('wurde', 'O'), (',', 'O'), ('um', 'O'), ('ein', 'O'), ('neutrales', 'O'), ('aber', 'O'), ('eindeutiges', 'O'), ('markenzeichen', 'O'), ('fur', 'O'), ('das', 'O'), ('unternehmen', 'O'), ('zu', 'O'), ('haben', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== xlm-roberta-base ===\n\nSome weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\txlm_roberta\ntokenizer:\tXLMRobertaTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('(', 'O'), ('Standard', 'B-ORG'), ('Oil', 'I-ORG'), ('of', 'I-ORG'), ('New', 'I-ORG'), ('Jersey', 'I-ORG'), (')', 'O'), (',', 'O'), ('die', 'O'), ('ausgesprochen', 'O'), ('„', 'O'), ('Esso', 'B-ORG'), ('“', 'O'), ('ergeben', 'O'), ('(', 'O'), ('heute', 'O'), ('ExxonMobil', 'B-ORG'), (')', 'O'), ('.', 'O'), (';', 'O'), ('Exxon', 'B-ORG'), (':', 'O'), ('Ein', 'O'), ('Name', 'O'), (',', 'O'), ('der', 'O'), ('in', 'O'), ('den', 'O'), ('frühen', 'O'), ('1970ern', 'O'), ('von', 'O'), ('Esso', 'B-ORG'), ('erfunden', 'O'), ('wurde', 'O'), (',', 'O'), ('um', 'O'), ('ein', 'O'), ('neutrales', 'O'), ('aber', 'O'), ('eindeutiges', 'O'), ('Markenzeichen', 'O'), ('für', 'O'), ('das', 'O'), ('Unternehmen', 'O'), ('zu', 'O'), ('haben', 'O'), ('.', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('Die', 'O'), ('Flügel', 'O'), ('Die', 'O'), ('geöffneten', 'O'), ('Flügel', 'O'), ('zeigen', 'O'), ('in', 'O'), ('vier', 'O'), ('Szenen', 'O'), ('Höhepunkte', 'O'), ('aus', 'O'), ('dem', 'O'), ('Leben', 'O'), ('von', 'O'), ('Maria', 'B-PER'), ('und', 'O'), ('Jesus', 'B-PER'), (',', 'O'), ('passend', 'O'), ('zu', 'O'), ('den', 'O'), ('christlichen', 'O'), ('Festen', 'O'), ('Weihnachten', 'O'), (',', 'O'), ('Ostern', 'O'), ('und', 'O'), ('Pfingsten', 'O'), (':', 'O'), ('Der', 'O'), ('Hauptschrein', 'O'), ('Im', 'O'), ('Hauptschrein', 'O'), ('ist', 'O'), ('die', 'O'), ('Krönung', 'O'), ('Mariens', 'B-PER'), ('durch', 'O'), ('Christus', 'B-PER'), ('dargestellt', 'O'), ('.', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=== xlnet-base-cased ===\n\nSome weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForTokenClassification: ['lm_loss.weight', 'lm_loss.bias']\n- This IS expected if you are initializing XLNetForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing XLNetForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\narchitecture:\txlnet\ntokenizer:\tXLNetTokenizer\n\n*** TESTING DataLoaders ***\n\n"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token / target label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S.', 'O'), ('593.', 'O'), ('Wink', 'B-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'O'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'B-LOCderiv'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzahlen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[('NEWSru.ua', 'B-OTH'), ('/', 'O'), (':', 'O'), ('оииоо', 'B-OTH'), ('оаа', 'I-OTH'), ('оро', 'I-OTH'), ('в', 'I-OTH'), ('оаве', 'I-OTH'), ('Die', 'I-OTH'), ('SPU', 'O'), ('legte', 'B-ORG'), (',', 'O'), ('wie', 'O'), ('auch', 'O'), ('vier', 'O'), ('weitere', 'O'), ('Parteien', 'O'), (',', 'O'), ('beim', 'O'), ('Obersten', 'O'), ('Gericht', 'O'), ('der', 'O'), ('Ukraine', 'O'), ('Beschwerde', 'B-LOC'), ('gegen', 'O'), ('den', 'O'), ('Ablauf', 'O'), ('der', 'O'), ('Wahl', 'O'), ('ein', 'O'), ('und', 'O'), ('behauptet', 'O'), (',', 'O'), ('es', 'O'), ('sei', 'O'), ('bei', 'O'), ('der', 'O'), ('Auszahlung', 'O'), ('zu', 'O'), ('Unregelmaßigkeit', 'O')]</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, task=task)\n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n')\n",
    "    \n",
    "    hf_batch_tfm = HF_TokenClassBatchTransform(hf_arch, hf_tokenizer, padding='max_length', max_length=seq_sz, \n",
    "                                               is_pretokenized=True, tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "    blocks = (\n",
    "        HF_TextBlock(hf_arch, hf_tokenizer, hf_batch_tfm=hf_batch_tfm), \n",
    "        HF_TokenCategoryBlock(vocab=labels)\n",
    "    )\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('tokens'),\n",
    "                       get_y= lambda inp: [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ],\n",
    "                       splitter=RandomSplitter())\n",
    "    \n",
    "    dls = dblock.dataloaders(germ_eval_df, bs=bsz)\n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***\\n')\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        if (hasattr(hf_tokenizer, 'add_prefix_space')):\n",
    "            test_eq(dls.before_batch[0].tok_kwargs['add_prefix_space'], True)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, 'PASSED', ''))\n",
    "        dls.show_batch(dataloaders=dls, max_n=2)\n",
    "        \n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, 'FAILED', err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>arch</th>\n      <th>tokenizer</th>\n      <th>model_name</th>\n      <th>result</th>\n      <th>error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>albert</td>\n      <td>AlbertTokenizer</td>\n      <td>albert-base-v1</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert</td>\n      <td>BertTokenizer</td>\n      <td>bert-base-multilingual-cased</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>camembert</td>\n      <td>CamembertTokenizer</td>\n      <td>camembert-base</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>distilbert</td>\n      <td>DistilBertTokenizer</td>\n      <td>distilbert-base-uncased</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>electra</td>\n      <td>ElectraTokenizer</td>\n      <td>monologg/electra-small-finetuned-imdb</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>longformer</td>\n      <td>LongformerTokenizer</td>\n      <td>allenai/longformer-base-4096</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>mobilebert</td>\n      <td>MobileBertTokenizer</td>\n      <td>google/mobilebert-uncased</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>roberta</td>\n      <td>RobertaTokenizer</td>\n      <td>roberta-base</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>xlm</td>\n      <td>XLMTokenizer</td>\n      <td>xlm-mlm-en-2048</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>xlm_roberta</td>\n      <td>XLMRobertaTokenizer</td>\n      <td>xlm-roberta-base</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>xlnet</td>\n      <td>XLNetTokenizer</td>\n      <td>xlnet-base-cased</td>\n      <td>PASSED</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model_name', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Converted 00_utils.ipynb.\nConverted 01_data-core.ipynb.\nConverted 01a_data-token-classification.ipynb.\nConverted 01b_data-question-answering.ipynb.\nConverted 01e_data-summarization.ipynb.\nConverted 01z_data-language-modeling.ipynb.\nConverted 02_modeling-core.ipynb.\nConverted 02a_modeling-token-classification.ipynb.\nConverted 02b_modeling-question-answering.ipynb.\nConverted 02e_modeling-summarization.ipynb.\nConverted 02z_modeling-language-modeling.ipynb.\nConverted index.ipynb.\n"
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
